

[[Disentangling Long and Short-Term Interests for Recommendation.pdf]]
[[Zettelkasten/PermanentNote/研究/論文/Disentangling Long and Short-Term Interests for Recommendation/日本語訳+補足|日本語訳+補足]]

---
#### 目次

- **研究背景と課題設定**
    - 解決しようとしている問題
    - 既存研究・従来手法の限界
    - 著者らの研究動機

- **イントロダクション**
    - 論文の概要（タイトル・著者・学会/ジャーナル・発表年）
    - この研究を読む意義（共同研究に関連する文脈）
    
- **提案手法の概要**
    - 手法の全体像（フレームワーク、数式・図解）    
    - キーアイデア（従来との差分を明示）
    - 理論的貢献（もしあれば）
    
- **実験設定**
    - 使用データセット        
    - 評価指標
    - 比較対象（ベースライン）
    
- **実験結果と分析**
    - 定量的結果（表・グラフでの比較）
    - 定性的分析（ケーススタディや可視化）
    - 著者らの主張（なぜ改善したか）
        
- **考察**    
    - 強み（何が有効だったか）
    - 限界（適用範囲や弱点）
    - 我々の研究との接点（ここが重要）
    
- **今後の展望と共同研究の可能性**
    - 応用可能性（自分たちのテーマへの適用余地）
    - 実験/モデル拡張のアイデア
    - コラボレーションポイント（タスク分担など）
    
- **まとめ**
    - 論文の要点再確認
    - 我々の研究にとってのインパクト

---
#### 研究背景と課題設定

**研究テーマ：Lifelong User Action Sequence Modeling**

![[Pasted image 20251001180713.png]]

- 解決しようとしている問題
	- ログの集約によって、行動の順序情報が失われてしまう
	- すべてのアクションを同じ重みで扱う

↓

研究テーマ：**短期と長期でログデータを動的に扱うことによって解決し、推薦の精度を上げたい**

- 既存研究・従来手法の限界
	- DV365(Meta)：
		- 研究動機
			- ユーザ履歴の長期化が精度改善に効く一方、オンラインで超長系列を扱うと**GPU/機能基盤コストが急増**し現実的でない
			- → コスト・新鮮性・一般化性（多面サーフェス・異種モデル間）という**ROIの制約**を満たしつつ、長期履歴の価値（特に**安定的嗜好**）を取り出す。
		- pros
			- 各ユーザー最大70,000件もの超長期の履歴を扱うことが出来る
			- 長期の埋め込みを扱った後、短期的関心を扱う下流モデルに組み合わせて学習
				- **Q. 短期的関心との組み合わせ方が重要なのでは？**
		- cons
			- 「安定的関心仮説」に基づいて、すべてのアクションを同じ重みで扱っている
			- 集約しているので順序情報が失われている
	- CLSR：
		- 研究動機
			- 推薦におけるユーザー関心は **安定した長期的嗜好** と **変動する短期的嗜好** の両方を含む ← 従来は両者を **統合した1つの表現** として扱ってきたが、これだと **精度と解釈性が低下** する。
		- 従来手法の限界
			- **CF系 (Matrix Factorization, LightGCNなど)**：長期嗜好には強いが、逐次性を無視。
			- **シーケンス系 (GRU4Rec, DIEN, SASRec, SURGEなど)**：短期的パターンは捉えるが、長期嗜好を忘れやすい。
			- **ハイブリッド (SLi-Recなど)**：長短期を同時に扱うが、**明示的な監督がないため「絡み合った」表現**になり、実際には disentangle されない。
		- pros
			- 短期と長期の関心の性質が異なることを指摘し、分離させている
			- 自己教師あり学習で監督を付けている
			- ユーザー履歴の順序情報は短期的関心でのみ、GRU+Attentionである程度保持される
		- cons
			- データセットのインタラクションは1億件ほどだが、期間は1, 2週間ほどで我々の長期と感覚がズレている
	- LiGR(LinkedIn)
		- **このあたりの関連論文も読み進めていきたい**

- 論点
	1. **長期と短期をどこで分けるか**
	2. **長期と短期の関心をどう動的に扱うか**
	3. **動的に扱った長期と短期の関心を、どう統合して推薦に活かすか**

---
#### イントロダクション

**論文の詳細**
- DV365: Extremely Long User History Modeling at Instagram
	- [[DV365 Extremely Long User History Modeling at Instagram.pdf]]
	- [[Zettelkasten/PermanentNote/研究/論文/DV365 Extremely Long User History Modeling at Instagram/日本語訳+補足|日本語訳+補足]]
	- Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’25)
	- Author
		-  Wenhan Lyu (Meta Platforms, Inc.)
		- Devashish Tyagi† (OpenAI)
		- Yihang Yang (Meta Platforms, Inc.)
		- Ziwei Li (Meta Platforms, Inc.)
		- Ajay Somani† (Independent, New York, USA)
		- Karthikeyan Shanmugasundaram (Meta Platforms, Inc.)
		- Nikola Andrejevic (Meta Platforms, Inc.)
		- Ferdi Adeputra (Meta Platforms, Inc.)
		- Curtis Zeng (Meta Platforms, Inc.)
		- Arun K. Singh (Meta Platforms, Inc.)
		- Maxime Ransan (Meta Platforms, Inc.)
		- Sagar Jain†∗ (Independent, Menlo Park, USA)
	
- Disentangling Long and Short-Term Interests for Recommendation
	- [[Disentangling Long and Short-Term Interests for Recommendation.pdf]]
	- [[Zettelkasten/PermanentNote/研究/論文/Disentangling Long and Short-Term Interests for Recommendation/日本語訳+補足|日本語訳+補足]]
	- The ACM Web Conference (WWW ’22)
	- Author
		- - Yu Zheng（清華大学・Kuaishouインターン時の仕事）
		- Chen Gao（清華大学、corresponding author: chgao96@gmail.com）
		- Jianxin Chang（Kuaishou Technology）
		- Yanan Niu（Kuaishou Technology）
		- Yang Song（Kuaishou Technology）
		- Depeng Jin（清華大学）
		- Yong Li（清華大学）

---
#### 提案手法の概要

##### 手法の全体像（フレームワーク、数式・図解)
【DV365】
![[Pasted image 20250827105508.png]]

- **上流（基盤）モデル**：UniTi（統一タイムライン）の長期履歴を**multi-slicing**（明示/暗黙、行動種別・時間バケット・視聴時間/比率等の規則ベース派生）→**プーリング**で約**200個のサブ埋め込み**を作成→**Funnel Summarization Arch (FSA)** で**58×256**に圧縮→**INT4量子化**→KVSへ配信（3Bユーザ）。
    
- **下流統合**：
    1. DLRMランキング：線形射影→**他のスパース埋め込みとConcat**
    2. HSTU：**入力シーケンス先頭にDV365トークンをprepend**
    3. Retrieval：**GateNet**で動的融合

【CLSR】
![[Pasted image 20250928112827.png]]
$$
\zeta = 
\begin{cases} 
	U_l = f_1(U), \\ U_s^{(t)} = f_2(U_s^{(t-1)}, V^{(t-1)}, Y^{(t-1)}, U), \\ Y^{(t)} = f_3(U_l, U_s^{(t)}, V^{(t)}, U), 
\end{cases}
$$
- $f_1$​: 長期的興味の表現    
- $f_2$​: 短期的興味の進化
- $f_3$​: インタラクション予測

![[Pasted image 20250913122904.png]]

- A：Contrastive Learning のためのラベルベクトル生成
	- 長期：全履歴平均プーリング
	- 短期：直近k件平均プーリング
- B：長期的興味のエンコーダ
- C：短期的興味のエンコーダ
- D：長期的興味と短期的興味の統合
	- 長期的興味と短期的興味を重視する割合をMLPで学習する
- E：インタラクション予測

- **2つのエンコーダ**
    - 長期エンコーダ (ϕ)：履歴全体を attention pooling で要約
    - 短期エンコーダ (ψ)：GRU + attention で直近の動きを強調
- **プロキシの生成**
    - 長期 proxy = 履歴全体の平均
    - 短期 proxy = 直近 k 件の平均  
        → これを「擬似ラベル」にして contrastive learning を行う
- **対比学習 (Contrastive Loss)**
    - 長期表現は長期 proxy に近く、短期 proxy から遠くなるよう学習
    - 短期表現も同様に監督
- **適応的融合 (Attention-based Fusion)**
    - ユーザー履歴・ターゲットアイテムを入力にして、長期 vs 短期の重み $\alpha$ を学習
    - $\hat{u}_t = \alpha u_l + (1-\alpha) u_s$


##### キーアイデア（従来との差分を明示）
【DV365】
- **超長系列をE2Eで扱わない**：
	- 長期は**オフライン要約**で“安定嗜好”にフォーカスすることで新鮮性の要件をクリア。
- **1つの上流表現を全下流に供給**してROI最大化。

【CLSR】
- アイデア
	- 長期的興味はユーザーの全体的な嗜好を反映しており、直近のインタラクションにあまり影響されない
	- 短期的興味は継続的にインタラクションするにつれて急速に進化する時間依存の変数
	- 将来のインタラクション予測で長期的と短期的興味のどちらが重要な役割を果たすかは、その時点でのアイテム・ユーザー履歴など様々な要因に依存する
	- → 「長期＝安定」「短期＝動的」と仮定し、**proxyを利用した自己教師あり学習**で両者を disentangle
	- LS-termの興味を学習するためのラベル付きデータを得るのは難しい → Contrastive Learning
- 従来との差分
	- LS-term の興味を明示的に区別する
		- 従来のマルコフ連鎖ベースの手法および深層学習モデルではLS-termの興味を区別することが出来ない → 統一された表現ではユーザーの興味を完全にとらえるには不十分
		- 長期的興味

---
#### 実験設定

##### 使用データセット

【DV365】
- Instagram内部ログ（Reels/Feed/Explore等）
- **生産系学習テーブル**は平均~1.5k（最大2k）の履歴
	- DV365用に**オフラインで長期履歴（平均4万/最大7万）**を追加集約して学習。


【CLSR】
- **Taobao (e-commerce)**：ユーザー数 36,915、平均履歴長 39.9
	- 100万インタラクション規模
- **Kuaishou (short-video)**：ユーザー数 60,813、平均履歴長 245.9  
	- 1,500万インタラクション規模

##### 評価指標

【DV365】
- ランキング：**Normalized Entropy (NE)** の相対差分
- 検索：**HitRate@1/@10**

【CLSR】
- **AUC, GAUC**（分類精度）
- **MRR, NDCG@2**（ランキング性能）


##### 比較対象(ベースライン)

【DV365】
- 実運用の**DLRM+HSTU**（ランキング）および**MoL系+HSTU**（Retrieval）
- DV365は**特徴として追加**し、上乗せ効果を測定

【CLSR】
- 長期系：NCF, DIN, LightGCN
- 短期系：Caser, GRU4Rec, DIEN, SASRec, SURGE
- 長短期統合：SLi-Rec (SOTA)

---
#### 実験結果と分析

##### 定量的結果
【DV365】
- **ランキング（Reels）**：全タスクでNE改善、**平均+0.4%程度**
	- 採用モジュールは**線形射影+Concat**が良好（Table 3）。
- **Retrieval**：**HR@1で+2〜8%**の改善。**GateNet融合**が概ね優位（Table 4）。
- プロダクト展開：**15モデルにローンチ**し、アプリ滞在時間**+0.7%**などの事業影響。

【CLSR】
- **CLSRは全ての指標でSOTAを超過**。
    - Taobao: AUC +0.03、NDCG@2 +5%
    - Kuaishou: GAUC +0.01、NDCG@2 +2〜3%
- 特に短期モデル > 長期モデルという傾向を超え、長短期分離の効果を実証。
##### 定性的分析
【DV365】
- 長期は**安定嗜好の抽出**に有効（“24hギャップ学習”で短期依存を抑制）
	- 短期はHSTU等が担当。**役割分担**で一貫して効く。

【CLSR】
Counterfactual評価：
- **クリック**（低コスト行動） → 短期重視
- **購入/Like**（高コスト行動） → 長期重視  
    → CLSRの融合重み α\alphaα がこの挙動を自動で学習できていることを確認。

##### なぜ改善したか(要因)
【DV365】
- **Multi-slicing**により明示/暗黙・時間・行動強度の**軸で要約** → FSAで**パラメタ共有（token-wise視点）**しつつ圧縮。
- **量子化**で配信容易。E2E長系列では難しい**コスト寄与**を回避しつつ、長期の**カウント的情報**を保持。

【CLSR】
- 長短期を分けないSLi-Recは「混ざった表現」になり情報が冗長化。
- CLSRは自己教師ありで分離することで、**役割が純化された表現**になり精度が向上。

---
#### 考察

##### 強み
【DV365】
- **ROI最適**：オンライン特徴抽出/保存/推論の大幅コスト削減（例：KVストレージ換算でPB級→TB級へ）。
- **一般化性**：単一上流から**異種下流（Retrieval/Ranking, Reels/Threads等）**へ横展開。
- **鮮度遅延への頑健性**：**staleness増加でもNEゲインが概ね一定**（付録B）。

【CLSR】
- **分離による解釈性**：長期/短期のどちらが効いているか追跡可能。
- **自己教師あり学習**：明示ラベルが不要で産業応用に向く。
- **一貫した精度改善**：2つの大規模データセットで安定。

##### 限界
【DV365】
- 長期側は**順序情報を基本捨象**（平均プーリング中心）。Emerging Interestの捕捉は**下流短期側に依存**
- 汎用埋め込みは**知識転送の損失**があり、下流側に**適応モジュール**が必要（例：GateNet）。

【CLSR】
- Proxy定義が単純（平均プーリング）。長期の順序性や周期性は無視。
- 短期の定義が固定k件に依存。ユーザーごとの柔軟性がない。

##### 我々の研究との接点
【DV365】
- **長短期の分担**を明確化しつつ、当方テーマ（短期×長期の動的使い分け）に直結。
- 「長期にも**順序/周期性**を導入」や、「融合機構の**最適化（学習α→MoE/Gating強化）**」は発展余地。

【CLSR】
- 「長期にも順序性を持たせる」「短期ウィンドウを動的にする」などの発展が可能。
- DV365と比較すると、CLSRはより**学習時の disentanglement**に注力しており、我々の「融合戦略の最適化」とも相性が良い。

##### やりたいこと
- 行動の順序情報を活かすこと
	- DV365 → ✕
	- CLSR → △ ？
- アクションに重みづけを動的に変えること
	- DV365 → ✕
	- CLSR → 〇
		- 長期と短期の分離
- 90日以上の長期的な履歴で活用すること
	- DV365 → 〇
	- CLSR → △？
		- この論文における「長期」は1~2週間程度

---
#### 今後の方針

- CLSRの超長期的データ検証
	- 本当に長期と短期の関心を分離できていて、精度が向上しているのかを検証
- CLSRの拡張
	- 短期的興味が進化するのはt-1からなのかは疑問 ← 実務的には押し間違え等のノイズがありそうなので
- 他の論文のお気持ちリサーチ
	- LinkedIn, Pinterestなど
	- CLSRを引用している論文のリサーチ
