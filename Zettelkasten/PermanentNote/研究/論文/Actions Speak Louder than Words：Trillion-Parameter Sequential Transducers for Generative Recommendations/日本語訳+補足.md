
pj-リクルート共同研究チャンネル で紹介されていた、Instagramと関係があるかもしれない論文

---
## 論文詳細

|項目|内容|
|---|---|
|発表形式|国際会議（conference paper）|
|採択先|ICML 2024（第41回 International Conference on Machine Learning）|
|論文掲載|PMLR Vol.235, pp.58484–58509|
|arXiv公開日|2024年2月27日|
|著者|Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Jiayuan He, Yinghai Lu, Yu Shi|
|所属情報|論文に明記はなく、GitHubのリポジトリ（Meta Recsys）に関連することから、**Metaのレコメンド研究チームが関与**している可能性が高いと推察されます。([Proceedings of Machine Learning Research](https://proceedings.mlr.press/v235/zhai24a.html?utm_source=chatgpt.com "Actions Speak Louder than Words: Trillion-Parameter ..."), [ICML](https://icml.cc/media/icml-2024/Slides/32684.pdf?utm_source=chatgpt.com "Actions Speak Louder than Words"), [GitHub](https://github.com/meta-recsys/generative-recommenders?utm_source=chatgpt.com "GitHub - meta-recsys/generative-recommenders: ..."))|

---
## Abstract

大規模な推薦システムは、**高いカーディナリティ（多種多様で巨大な集合）を持つ異種混合の特徴量**に依存し、**毎日数百億規模のユーザー行動を処理**しなければならないという特徴があります。何千もの特徴量を使い、膨大なデータで学習しているにもかかわらず、業界で用いられる多くの **深層学習型推薦モデル（DLRM）** は、計算資源に比例して性能をスケールさせることに失敗しています。

言語や画像分野で Transformer が成功を収めたことに着想を得て、私たちは推薦システムにおける根本的な設計選択を見直しました。そして、推薦問題を **生成モデリングの枠組みにおける逐次変換タスク（sequential transduction tasks）** として再定式化し、**高カーディナリティかつ非定常なストリーミング推薦データ**に適した新しいアーキテクチャ **HSTU** を提案します。

HSTU は、合成データや公開データセットにおいてベースラインを **最大65.8%（NDCG指標）** 上回り、長さ8192の系列において **FlashAttention2ベースのTransformerよりも5.3倍から15.2倍高速**です。さらに、**1.5兆パラメータ規模のHSTUベースの生成型レコメンダ**は、オンラインA/Bテストで **12.4%の指標改善**を達成し、**数十億ユーザーを抱える大規模インターネットプラットフォーム上の複数のサービスにすでに導入**されています。

さらに重要なのは、**生成型レコメンダのモデル品質が、学習計算量に対して累乗則（power-law）に従って経験的にスケール**することを確認した点です。この関係は、GPT-3 や LLaMA-2 の規模に至るまで **3桁のオーダー**で成立しており、今後のモデル開発に必要なカーボンフットプリントを削減し、推薦分野における **初のファウンデーションモデル** への道を切り開くものです。


> [!NOTE] 逐次変換タスク (sequential transduction tasks)
> - **Sequential** = 時系列の並び（ユーザーの行動ログなど）
> - **Transduction** = 「ある系列を別の系列に変換する」処理
> 
> つまり、**「入力の時系列を、出力の時系列に変換する問題設定」**のことです。
