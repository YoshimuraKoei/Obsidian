
第2回リクルートMTGから提案されたネタ
https://nakata-lab-2020.slack.com/archives/C03NSQVLL8N/p1755494508873879?thread_ts=1752491588.962969&cid=C03NSQVLL8N
---
[[【LinkedIn】From Features to Transformers：Redefining Ranking for Scalable Impact.pdf]]

**CCS**
- Computing methodologies → Neural networks; 
- Information systems → Recommender systems; Learning to rank.

**🔑 Keywords (論文指定)**
- **Large Scale Ranking**
- **Deep Neural Networks**

**👥 著者・所属**
- **Fedor Borisyuk***（LinkedIn, Mountain View, CA, USA）
- **Lars Hertel***（LinkedIn, Mountain View, CA, USA）
- **Ganesh Parameswaran***（LinkedIn, Mountain View, CA, USA）
- **Gaurav Srivastava***（LinkedIn, Mountain View, CA, USA）
- **Sudarshan Ramanujam***（LinkedIn, Mountain View, CA, USA）
- **Borja Ocejo***（LinkedIn, Mountain View, CA, USA）

（以上の6名が _equal contribution_）

その他 LinkedIn 所属の著者：  
Peng Du, Andrei Akterskii, Neil Daftary, Shao Tang, Daqi Sun, Charles Xiao, Deepesh Nathani, Mohit Kothari, Yun Dai, Guoyao Li, Aman Gupta

**📖 ジャーナル・出版情報**
- 出版者: **ACM (Association for Computing Machinery)**
- 書誌情報:
    - 論文タイトル: _From Features to Transformers: Redefining Ranking for Scalable Impact_
    - ACM 権利表記あり（正式な DOI は "XXXXXXX.XXXXXXX" の placeholder のまま）
    - 「Conference acronym ’XX, June 03–05, 2018, Woodstock, NY」と記載（おそらくテンプレートのまま、本来は ACM 系国際会議に採択予定の原稿）

---
## Abstract

私たちは LinkedIn において開発された大規模ランキングフレームワーク **LiGR** を紹介する。本フレームワークは、最先端の Transformer ベースのモデリングアーキテクチャを実運用環境に導入するものである。ここでは、学習型正規化（learned normalization）と、ユーザ履歴およびランキング対象アイテムへの同時集合単位（set-wise）アテンションを組み込んだ修正版 Transformer アーキテクチャを提案する。このアーキテクチャは、以下のような画期的成果を可能にした。

1. **大部分の手作業による特徴量エンジニアリングを廃止**し、従来の最先端システムを数百の特徴量に依存せず、わずかな特徴量だけで上回る性能を達成。
2. ランキングシステムにおけるスケーリング則を実証し、モデルを大規模化し、学習データを増やし、**文脈シーケンスを長くすることで性能が向上する**ことを示した。
3. アイテム集合を同時にスコアリングする方式を導入し、多様性が自動的に改善されることを実現。

さらに、大規模ランキングモデルの効率的な提供を可能にするために、ユーザ履歴の一括処理や集合単位アテンションを活用した推論のスケーリング手法を述べる。また、多数のアブレーション研究や A/B テストから得られた重要な知見をまとめ、最も効果的な技術的アプローチを明らかにする。

---
## Introduction

現代のプロフェッショナル向けソーシャルネットワークは、ユーザーが幅広いコンテンツ――求人情報や業界ニュースから、ソーシャルな交流やキャリア更新に至るまで――に関わる**デジタル・エコシステム**として機能している。世界中で10億人を超えるユーザーが数百の地域にまたがって利用しており、この大規模環境で関連性とエンゲージメントを維持することは極めて複雑な課題である。

効果的なパーソナライズには、
- **ユーザーの詳細な嗜好を把握**し、
- **急速に変化する関心に適応**し、
- **多様なユースケースに一般化**できるシステム

が必要となる。


> [!NOTE] 効果的なパーソナライズ
> - **ユーザーの詳細な嗜好を把握**し、
> - **急速に変化する関心に適応**し、
> - **多様なユースケースに一般化**できるシステム

本研究は、従来の**特徴量エンジニアリングに依存したランキング基盤**を、プラットフォームの複雑化とユーザー需要の拡大に合わせてスケール可能な**データ駆動型アーキテクチャ**へと進化させる必要性に動機づけられている。従来型システムはしばしば数百もの手作業による特徴量に依存しており、それらは保守に手間がかかり、拡張も困難である。さらに既存の多くのランキングモデルは、データ量の増加、文脈の深さ、モデル容量の拡大にうまく対応できず、リアルタイムでのパーソナライズ能力を制限している。

本論文では、こうした課題を解決するために、**アーキテクチャ上の革新と実用的なスケーラビリティを兼ね備えた次世代の生成型ランキングフレームワーク**を提案する。我々の主な貢献は以下の通りである：

- **Transformerベースのランキングアーキテクチャ**: 学習型ゲーティッド正規化と、ユーザー履歴および候補アイテム全体に対する集合単位（set-wise）アテンションを組み込んだ修正版Transformerを導入。この設計により、アイテム集合を同時にスコアリングでき、手作業のルール設定なしにランキング出力の多様性が促進される。
    
- **最小限の特徴量依存**: 従来のように多数の手作業特徴や反事実シグナルに依存するのではなく、わずか7つの特徴量だけで高いランキング品質を達成。数百特徴を用いる強力なベースラインを大幅に上回り、モデルがインタラクションデータから直接有用な表現を学習できることを示す。
    
- **スケーリング則の実証**: 本アーキテクチャは、モデル規模・学習データ量・履歴シーケンス長の拡大に伴い性能が向上するというランキング・検索システムにおけるスケーリング則を確認。さらに HSTU や Wukong などのベースラインを上回る結果を達成。
    

加えて、大規模ランキングモデルを効率的に本番運用に展開するため、**ユーザー履歴の単一パス処理や集合単位アテンション**といった推論時最適化を導入。これらはスケーラビリティと低レイテンシを両立し、実際のフィードランキングシステムに完全に実装されている。

第5章では、包括的なアブレーション研究とオンラインA/Bテストから得られた知見を提示し、その有効性を実証する。実運用のデプロイでは、**プロフェッショナルコンテンツにエンゲージするDAU（Daily Active Users）が0.27%増加**するという定量的成果を得た。我々は、この貢献が大規模ランキングおよび検索システムを発展させるエンジニアや研究者にとって、実践的かつ信頼性のある戦略を提供すると考える。

---
## 2. Related Work

**ディープラーニングを用いた推薦システム:**  
推薦モデルを改良するために様々なアーキテクチャが提案されてきた。最初期の例として **Wide & Deep モデル [10]** があり、線形モデルとMLPを組み合わせて記憶能力と汎化能力の双方を実現した。  
その後、**DeepFM** は因子分解機械（Factorization Machines）[15] を取り入れ、**DCN / DCNv2** はクロスレイヤーを追加して特徴間の明示的な相互作用を捉えた [33, 34]。  
さらに **xDeepFM** は CIN（Compressed Interaction Network）を導入してベクトル単位の特徴モデリングを行い [21]、**AutoInt** は自己注意機構（Self-Attention）[30] を用いた特徴相互作用学習を行った。  
**AFN** は対数変換を組み込んだ [11]。  
**FinalMLP** は二流（dual-MLP）構造を導入してCTR予測を改善した [22]。  
**InterFormer [36]** は DLRM 型モデルとトランスフォーマ型モデルを統合し、手作業で作られた特徴と時系列特徴の双方を同時にモデリングした。

しかしながら、これらの手法の多くはいまだに**手作業で作成された特徴量に強く依存**している。近年の取り組み [6] ではプロダクション環境で統一アーキテクチャを模索してきたが、多くは試行錯誤的であった。  
これに対し本研究では、**統一されたトランスフォーマモデルが複雑なハイブリッドモデル [6] を上回り、カウンタ特徴（counter features）への依存を減らし、開発を簡素化できる**ことを示す。さらに、我々の手法は **LiNR GPU 検索システム [5]** と統合され、埋め込みベースの検索品質も向上させている。

**セットワイズランキング（Set-wise Ranking）:**  
従来のランキング手法は **ポイントワイズ（point-wise）** であり、ユーザセッション内のアイテム間依存性を無視していた。  
これに対し **セットワイズ手法**は、同時に提示されるアイテム同士の相互作用をモデル化し、より良い性能を実現する [7, 13, 20, 24, 26, 28]。  
特に **トランスフォーマベースのリランカー**は、コンテキストに依存した効果を捉える上で有効であることが示されている。


> [!NOTE] Title
> - 「同時に提示されるアイテム同士の相互作用をモデル化し、より良い性能を実現する」
> 	- セットワイズ手法は取り入れるべきかもね

**レコメンダシステムのスケーリング則:**  
スケーリング則とは、モデルサイズ・データ量・計算資源を増やすにつれてモデル性能がどのように向上するかを記述するもので、LLM において広く研究されてきた [19]。  
一方、推薦システムではこれを産業規模で検討した研究は少なく [9, 14, 37, 38]、LLM を用いたレコメンダはスケーリングの傾向を示すものの [9, 14]、大規模展開が難しく、主に特徴量強化の役割にとどまっている。

本研究では、わずか **7つの特徴量のみを使用しながら従来手法を上回る**トランスフォーマ修正版を提案し、**スケーラビリティとシンプルさの両立**を実現している。


> [!NOTE] Title
> - 推薦システムでは大規模展開が難しいという問題がある

---
## 3. Model Architecture

本節では、LinkedIn フィードランキングにおいてどのようにランキングモデルを実現したか、そのモデリング手法を紹介する。  
LiGR（**LinkedIn Generative Recommender**）という大規模ランキングフレームワークの中で、最先端の **Transformer ベースのモデリングアーキテクチャ** を導入した。

我々は、**学習済みゲーテッド正規化（learned gated normalization）** と **ユーザ履歴およびランキング対象アイテムへの同時セットワイズ注意機構（simultaneous set-wise attention）** を組み込んだ改良版 Transformer アーキテクチャを提案する。このアーキテクチャにより、以下の複数の画期的成果が得られた。

1. **手作業による特徴量設計の大部分を不要化**し、ベースラインで数百の特徴量を用いていた従来の最先端システムを、わずか **7 つの特徴量**のみで上回った。
    
2. **ランキングシステムにおけるスケーリング則（scaling law）** を検証し、モデルの大型化・学習データ量の増加・コンテキスト系列の長大化により性能が向上することを示した。
    
3. アイテムを**セット単位で同時にスコアリング（joint scoring）**することで、**多様性（diversity）の自動的改善**を実現した。
    


> [!NOTE] Title
> - 手作業による特徴量設計の大部分を不要化
> - スケーリング則の検証
> - セット単位でのスコアリングによる多様性の改善

さらに、大規模ランキングモデルを効率的に本番環境で提供するため、**ユーザ履歴を一度だけ処理するシングルパス方式とセットワイズ注意機構を活用した推論のスケーリング手法**を記述する。

---
### 3.1 Baseline model architecture

ベースラインとなるフィードランキングモデル [6] は、**ポイントワイズ（point-wise）ランキング**を用い、  
各 `<会員, 投稿>` ペアに対して **「いいね」「コメント」「シェア」「投票」「長時間閲覧（long dwell）」「クリック」** の複数の行動確率を予測し、それらを線形結合して最終スコアを算出する。

このモデルは**2 つのタワーから成るマルチタスクニューラルネットワーク**を採用している。

- **クリックタワー（Click Tower）**：クリックおよび長時間閲覧 [39] を予測
    
- **コントリビューションタワー（Contributions Tower）**：いいね、コメント、シェア、投票、さらにこれらのいずれかが発生した場合に 1.0 となる **「Contribution」ラベル** を予測
    

評価指標としては、**Contribution・Click・Long Dwell の AUC** を報告する。

両タワーは同一の**正規化済み dense / sparse 特徴量 [16]** を利用し、**Actor および Hashtag テーブル [23]** から ID 埋め込み [6] を参照している。  
再現性のため、このモデルのアーキテクチャ図は [6] に含まれている。

---
### 3.2 Generative Ranking with Setwise attention and Learnt normalization

我々が提案する大規模ランキングフレームワーク **LiGR** は、[37] で述べられた **Generative Recommender（GR）方式** に類似した**完全逐次型（fully sequential）** のアーキテクチャである。

具体的には、モデルの入力はメンバーの**インタラクション履歴**

$X_{0},\,X_{1},\,\dots,\,X_{n}$

であり、各インタラクション $X_{i}$​ は1つの**トークン**を表す。  
入力トークン $X_{i}$​ 自体は、**アイテム・アイテムの作成者・コンテンツ埋め込みなどのエンティティを表す複数の埋め込み特徴量を連結したもの**である。

モデルの出力は入力に対応する**アクション系列**

$y_{0},\,y_{1},\,\dots,\,y_{n}$

である。例えば $y_{i}$​ は、メンバーがアイテム $i$ に対して行ったすべてのアクション（クリック・いいね・コメント・シェア等）を表す**マルチホットベクトル**となる。

[37] の**アクションインターリーブ手法（action interleaving scheme）** を用い、入力と対応する出力を交互に並べる（図1参照）。  

![[Pasted image 20251006142912.png]]

これによりモデルは、**過去アイテムの特徴とメンバーの行動を結合して内部的に豊かな特徴表現を学習**できる。  
因果的注意（causal attention）により、位置 $i$ は将来の出力を先読みすることができないよう制約されている。  
インターリーブされたアクションに対応するモデル出力はマスクされる。

モデルは **Pre-Norm Transformer ブロック [31]（前正規化 [2, 12, 32]）** から構成され、各マルチヘッド注意層およびフィードフォワード層には線形射影とシグモイド活性化による**ゲーティング**が施される：

$h_{j+1}=h_{j}+F(h_{j})\times\sigma(h_{j}W)$

ここで $F$ は **マルチヘッド注意＋フィードフォワード層** を表し（図2参照）、提案アーキテクチャは **Highway Transformer [8]** に類似しているが、スキップ接続上ではなく **MHA および FF 層の出力に対してゲーティングを適用**する点が異なる。

![[Pasted image 20251006150106.png]]

---
#### セットワイズランキング層（Setwise Ranking Layer）

従来の LinkedIn Feed ランキングモデルは**ポイントワイズ方式**に依存しており、  
**ルールベースの多様性リランカー**によってセッションレベルのポリシーを適用してきた。  
例として次のようなルールがある：

1. ネットワーク外コンテンツ同士の間隔を**最低2アイテム**空ける
2. 同一投稿者のポスト同士の間隔を**最低2アイテム**空ける

しかしこれらのルールは**一律のテンプレート（one-size-fits-all）** を前提としており、柔軟性に欠ける。

そこで我々は、各アイテムを独立に扱うのではなく、**セッション全体にわたるユーザのインタラクションとセッションレベルのデータを活用し、同一セッション内アイテムを再ランキングするセットワイズモデル層**を導入することを提案する。  
この改良により**セッション体験を向上させ、LinkedIn Feed におけるユーザエンゲージメントを増加させる**ことを目的とする。

さらに、[24] の **SetRank アーキテクチャ** に倣い、**セッションレベルの情報フロー**を LiGR モデルに拡張した。  
SetRank は**セッション内のアイテム同士に自己注意（self-attention）を導入し、リストワイズランキングを可能にする**。  
我々は LiGR に図1のような**インセッション注意ブロック（in-session attention blocks）**を追加した。

歴史的なセッションは長さが様々であるため、インセッション注意には**セッションIDに応じて可変のアテンションマスク**が必要となる。  
これを効率的に実現するため、**FlexAttention [17]** を用いた。

最後に、学習時の**バイナリクロスエントロピー損失に Attention Rank loss [1] を追加**した。  
我々のケースでは、この Attention Rank loss の集約もセッションIDによって決定される**セッションレベル**で行われる。

> [!NOTE] セッションの定義
> **“セッション全体”** とは「ユーザーがフィードを開いて、  閉じるまでのひと続きの閲覧行動」を指し、その中には複数のポスト（候補アイテム）がある。

> [!NOTE] セットワイズ
> - 「セットワイズ」とは、**セッション内で表示されるポスト群を “ひとまとまり（セット）” としてモデルが扱う**という意味です。   
> - モデルがあるポストをスコアリングするとき、**同じセッション内に並んでいる他のポストの特徴やユーザの直近のインタラクションも同時に参照**します。

---
### 3.3 LiGR with Semantic IDs

**IDベースのモデル**では、大きな課題の1つとして**疎なID特徴量（sparse ID features）の語彙数が非常に大きくなる**点が挙げられる。  
このためモデルサイズが増大し、推論サーバの運用が複雑化するとともに、**IDを常に最新の状態に保つため継続的な更新**が必要になる。

例えば、LinkedInではメンバーによる新しい投稿が**毎秒のように作成される**。  
パラメータが数兆規模におよぶ大規模モデルでは、しばしば [37] のように**複数のマシンにモデルを分割配置する高度な分散推論基盤**が必要となる。

近年の研究 [29] は、**Semantic ID（意味ベースのID）** を用いることで、**同等の性能を維持したままモデルサイズを大幅に縮小できる**ことを示している。  
本論文の §5.1 では、この手法により **モデルサイズを 5.4 B（54億）パラメータから 1.3 B（13億）パラメータへ削減しつつ、モデル品質を損なわない**ことを示す。

我々は、[27] および [29] で提案された **RQ-VAE（Residual-Quantized Variational Auto-Encoder）モデル** を活用し、**教師なし学習で離散的かつ階層的な Semantic ID を生成し、追加特徴量として利用する**ことを検討した。

RQ-VAE モデルは、まず**高次元特徴ベクトルのバッチ $x$** を入力として受け取り、各アイテムに対して**Semantic ID の組（tuple）**を割り当てる。

1. **エンコーダによる次元圧縮**  
    エンコーダネットワーク $f_{\phi}$​ が入力埋め込み $x$ を**潜在的な低次元表現 $z$** に写像する：
    $z = f_{\phi}(x) \quad(\,z \in \mathbb{R}^{d})$
    ここで $\phi$ はエンコーダのパラメータ。
    
2. **階層型残差量子化（Residual Quantization; RQ）**  
    潜在表現 $z$ は、一連のコードブックを用いて段階的に量子化される。
    - **Step 1：初期量子化**  
        第1コードブック $C_{1}$​ から $z$ に最も近いベクトルを探し、
        $c_{1} = Quantize(z,\,C_{1}) ,\; c_{1} \in C_{1}$
        とする。残差 $r_{1}$​ は
        $r_{1} = z - c_{1}$        
        で計算される。
        
    - **Step 2：残差の逐次量子化（Residual Quantization）**  
        残差 $r_{1}$​ を次のコードブック $C_{2}, C_{3}, \dots$ で繰り返し量子化する。  
        各ステップ $i$ では：
        $c_{i} = Quantize(r_{i-1},\,C_{i}),\; c_{i} \in C_{i}$
    
    - **最終的な量子化表現**  
        全ての量子化コードの総和を最終潜在表現 $z_{q}$​ とする：
        $z_{q} = c_{1} + c_{2} + \cdots + c_{N} = \sum_{i=1}^{N} c_{i}$
        
3. **デコーダによる復元**  
    デコーダネットワーク $g_{\theta}$​ が量子化済み潜在表現 $z_{q}$​ を元の埋め込み空間に写像し、
    $\hat{x} = g_{\theta}(z_{q})$
    と再構成する。ここで $\theta$ はデコーダのパラメータであり、$\hat{x}$ は復元された埋め込みである。

RQ-VAE は以下の損失を最小化することで学習される：

1. **再構成損失（Reconstruction Loss）**  
    　入力埋め込み $x$ と復元された埋め込み $\hat{x}$ の差を測る：

$$
L_{\text{reconstruction}}=\|\,x-\hat{x}\,\|^{2} \tag{2}
$$

2. **量子化損失（Quantization Loss）**  
    　潜在表現 $z$ と、その量子化後の表現 $z_{q}$​ との間の差を罰則項として加える：
$$
L_{\text{quantization}}=\|\,z-z_{q}\,\|^{2}
$$

**最終損失関数**はこれらの加重和として定義される：

$$
L= L_{\text{reconstruction}}+\beta\,L_{\text{quantization}}​
$$

ここで $\beta$ は量子化損失の重要度を調整するハイパーパラメータであり、  
本研究では $\beta = 0.25$ を使用した。

学習を安定させるため、[29, 35] で提案された **ベクトルリセット手法（vector reset approach）** を採用した。  
各コードブックベクトルに対して**割り当て回数の指数移動平均**を追跡し、  
もしあるコードブックベクトルの利用率が閾値を下回った場合は、  
**現在のバッチからランダムにサンプリングしたコンテンツ埋め込み要素でそのベクトルをリセット**する。

最も性能の良かった RQ-VAE モデルは次の仕様を持つ：

- **コードブック：3つ**
- 各コードブックの**コードベクトル数：1,000**
- 潜在ベクトルの次元数：**8**    

さらに、RQ-VAE モデルにおいて**クラスタが均等に分布するよう学習させるため**、  
[25] の **FLOPs Regularizer** を適用した。

学習後は、各アイテムに対してその**コンテンツ特徴量に基づき Semantic ID の組（tuple）を割り当てる**。  
その後、それぞれの Semantic ID の埋め込みベクトルは、LiGR モデルの一部として **embedding bag** 内で学習される。

---
### 3.4 Retrieval with LiGR

我々はLiGRを用いて動画検索タスクの実験も行った。メンバータワー（図3）では、メンバーの疎な特徴量を埋め込みベクトルに変換し、それを密な特徴量と連結してフィードフォワードネットワーク（FFN）に通す。FFNの出力はTransformerモデルにおける仮想トークンとして扱われる。Transformerにはさらに、メンバーの履歴アクティビティの埋め込みも追加のトークンとして入力される。ベースラインモデルには「緑色のブロック」は含まれず、これらは生成型推薦（GR）が有効な場合にのみ使用される。GRの下では、共有されたアクションヘッドがメンバープロフィール、過去のアクティビティおよびそれらに対応する行動に基づいて、アクティビティに対する次の行動を予測する。

![[Pasted image 20251006150218.png]]

アイテムタワーでは、アイテムの特徴量を個別のトークンとして扱い、Transformerモデルがこれらのトークンを処理する。その後、LiNRモデル[5]を用いてメンバー埋め込みとアイテム埋め込みの類似度を計算する。

ベースラインの損失関数は、各学習データ点につき1つの正例と2つのマイニングされた負例を用いたsoftmax損失である。LiGRでは、この2タワー型のsoftmax損失に加えて、アクション予測損失を組み合わせる。このアクション予測損失は、各アクティビティに対して可能なすべての行動の中から正しい行動をsoftmaxで予測する損失である。

---
## 4. System Architecture

LiGRはシステムの大幅な単純化に貢献している。**カウンター特徴量への依存を減らし、数百あった特徴量を10未満にまで削減できたから**である。Transformerモデルの提供には、図4に示すように、ニアライン（near-line）のメンバーおよびアイテムのアクティビティを保持する必要がある。システム内では、GNN[4] や LLM[3] によって生成された多様なアイテムおよびメンバーの埋め込みに加え、モデル内に保持・提供されるID埋め込みも利用している。ユーザーからのリクエストを受けると、システムはまずモデルベースの検索システム[5]を用いて上位候補を取得し、その後、上位K件の候補を第2層のLiGRスコアリングモデルで評価する。LiGRは入力としてアイテムの特徴量とメンバーのコンテキストを受け取る。

**4.0.1 LiGRの学習**
LiGRモデルの学習はGPUメモリを多く消費するため、いくつかの最適化が必要であった。まず、ユーザーのアイテム履歴とそれに対応するアクションを交互に組み込むことで、最大のモデルでは系列長が2048に達した。このため、メモリ消費を抑えるためにFlash Attentionと混合精度（mixed precision）を用いた。次に、LiGRは表2に示すように複数のID埋め込み特徴量を活用しており、他のハイパーパラメータと併せてID埋め込みの次元数を拡張することが重要である。単一GPUのメモリを超える大きさの埋め込みテーブルを扱うために、埋め込みテーブルを列方向に分割し複数デバイスに分散配置する手法を用いた。この方法により、埋め込みテーブルを任意の大きさまで拡張することが可能になった。

**4.0.2 推論の最適化**
LiGRモデルでは、多数のTransformer層を長いユーザー行動履歴と多数の候補アイテムに適用する必要があるため、単純な推論は計算コストが高い。[18, 37] では、すべての候補アイテムに対して履歴の計算を共通化（アモータイズ）する方法が提案されている。我々の場合、履歴に基づく注意機構の実装により、アイテムは過去のセッションに属する他のアイテムにのみ注意を向けることができる。このため、すべての候補を同時に推論でき、履歴計算を自動的にすべての候補に対して共通化できる。

フィードの指標はレイテンシに敏感であるため、制約内に収めるため慎重な最適化が求められる。(1) **Split Scoring**：ポイントワイズスコアリングの後に上位10件の投稿にのみセットワイズスコアリングを適用し、ポイントワイズとセットワイズのモデル重みを分離した。(2) **Score Combination**：上位1〜10件にはセットワイズスコアをポイントワイズスコアに加算し、11件目以降は順序を維持した。(3) **Simplification**：ルールベースの多様性リランカーを無効化した。(4) **Unified Inference**：ポイントワイズ部分とセットワイズ部分のモデルをリライターで結合し、単一の推論でモデル全体をスコアリングできるようにした。オンラインスコアリングのスケーリングにあたっては、すべてのアイテムをバッチサイズ次元を用いて渡し、リストワイズデータセットとして再パッケージ化し、学習済みモデルで一括スコアリングした。これらの最適化によって、追加のレイテンシコストはp90で10msに抑えられ、メンバーの体験への影響を最小限にできた。



---
## 🚀 LiGR（Transformerベース）の特徴

- **特徴量エンジニアリングは最小限（7種類程度）**
- それ以外は
    - **ID埋め込み**（ユーザーID、アイテムID、作者ID など）
    - **コンテンツ埋め込み**（テキストや画像は事前学習モデルから埋め込み）
- それを **トークン列として Transformer に渡す**
- Transformer が履歴やアイテム間の関係性を学習 → 人手設計よりも強力な表現が得られる

- 最大で1024件の履歴イベントを扱える

- **Transformer 自体は順序を持たないモデル**
- でも **位置情報（positional encoding）を与えると、順序依存のパターンを学習できる**
- LiGR はこれを利用して、**従来の集約特徴量では失われていた履歴順序**をフル活用している


```
履歴シーケンス = [
  {PostID=101, ActorID=A, Type=ニュース, Action=クリック},
  {PostID=202, ActorID=B, Type=動画,   Action=いいね},
  {PostID=303, ActorID=C, Type=求人,   Action=シェア}
]
```


> [!NOTE] Title
> - 「最大で1024件の履歴イベントを扱える」 ← 少なすぎる
> - 「従来の集約特徴量では失われていた履歴順序をフル活用している」 ← よさげ

### 1. **LiNR (LinkedIn Neural Retrieval)**

- 役割：候補生成（retrieval）
- ユーザー埋め込みとアイテム埋め込みを学習して、  
    → 類似度計算で「数百万アイテムから上位1万件くらい」を引っ張ってくる。
- **二塔モデル (two-tower)** に近い設計。

---
### 2. **LiGR (LinkedIn Generative Recommender)**

- 役割：ランキング（ranking）
- LiNRが持ってきた候補アイテムに対して、ユーザー履歴シーケンスと一緒に入力。
- **Transformer が set-wise attention でスコア付け**し、最終的に「どの順番で表示するか」を決める。
- ここで初めて「多様性」や「順序情報」を効かせられる。


