---
title: 確率的勾配降下法
AI: "false"
published: 2025-07-06
description: 【初心者必見】確率的勾配降下法（SGD）とは？仕組みやメリット・デメリットをわかりやすく解説
tags:
  - permanent-note
  - sgd
  - stochastic-gradient-descent
  - math
---
https://jitera.com/ja/insights/43068
---
#### 確率的勾配降下法(SGD)とは

> [!NOTE] 確率的勾配降下法
確率的勾配降下法(SGD: Stochastic Gradient Descent)は、**機械学習や深層学習でよく使われる最適化アルゴリズムの一つ。** このアルゴリズムは、大規模なデータセットに対しても効率的に問題を解くことが出来るという特長がある。

```table-of-contents
```
##### 確率的勾配降下法の動作原理
このアルゴリズムはデータセット全体を一度に使用するのではなく、**ランダムに選ばれたデータサンプルを使ってステップごとに目的関数の勾配を推定する。**
確率的勾配降下法の特徴は、ランダム性があること。局所的最適解に陥るリスクを低減し、より一般的な最適解へと導く。
##### 確率的勾配降下法(SGD)と通常の勾配降下法(GD)の違い
通常の勾配降下法(GD)は、データセット全体の平均勾配を計算してからパラメータを更新するのに対し、**SGDはランダムに選ばれたサンプルをもとに勾配を更新し、頻繁にパラメータを更新する**という違いがある。
SGDは計算効率が高く、収束速度が速いという点にメリットがある。SGDでは最適解の質やハイパーパラメータの設定に注意が必要であり、更新のたびにノイズが多く含まれるため、収束の過程が不安定になることがある。
そのため、**大規模なデータセットや逐次的にデータが到着するオンライン学習の場面ではSGDが優れた選択肢**である。通常の勾配降下法(GD)はSGDより安定しているが、計算コストが高いことが欠点。


> [!NOTE] 確率的勾配降下法のメリット
> - 大規模なデータセットでも対応できる
> - 収束速度が速い
> - オンライン学習に適している
> - 局所最適解から脱出できる可能性がある


> [!NOTE] 確率的勾配降下法のデメリット
> - 最適解の質が低くなる恐れがある
> - ハイパーパラメータの設定に敏感
> 	- 学習率が大きすぎると、アルゴリズムが最適解を飛び越えてしまう
> - アルゴリズムの収束判定が難しい
> 	- ノイズの影響を受けやすく、損失関数の値が上下に振動することに起因する
> - 勾配の分散が大きい場合に不安定になる


> [!NOTE] 確率的勾配降下法の適用事例
> - 画像分類タスク
> - 自然言語処理
> - 推薦システム

