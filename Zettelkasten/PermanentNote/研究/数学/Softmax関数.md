---
title: ［活性化関数］ソフトマックス関数（Softmax function）とは？
AI:
published: 2025-10-06
description:
tags:
  - permanent-note
  - math
  - softmax
  - sigmoid
  - deep-learning
---

https://atmarkit.itmedia.co.jp/ait/articles/2004/08/news016.html
---

AI／機械学習の[ニューラルネットワーク](https://atmarkit.itmedia.co.jp/ait/articles/1901/06/news046.html)などにおける**ソフトマックス関数**（**Softmax function**、もしくは**正規化指数関数**： **Normalized exponential function**）とは、入力データ（＝ベクトル）内の複数の値（＝ベクトルの各成分）を**0.0**～**1.0**の範囲の確率値に変換する関数である。この関数によって出力される複数の値（＝ベクトルの各成分）の合計は常に**1.0**（＝100％）になる。

ソフトマックス関数の出力値をグラフにすると、滑らかな（＝ソフトな）曲線が得られる（図1）。この滑らかさと、1つの成分だけが最大値を取る特性から「ソフトマックス関数」と呼ばれる。

![[Pasted image 20251006205214.png]]

**図1　「ソフトマックス関数」のグラフのイメージ**  
このグラフは、3つの成分を持つベクトルの1つの成分x0を変化させた際の、ソフトマックス関数の出力を示している。具体的には、x0（横軸）を変化させ、x1を**1.0**、x2を**0.0**と固定してソフトマックス関数に入力。出力は「猫」「虎」「ライオン」という3つのクラスの確率値として曲線で表現されている。また、参考のためにシグモイド関数のグラフも示している。


冒頭では文章により説明したが、厳密に数式で表現すると次のようになる。xやyはベクトル、nはクラスの数、iやkは特定のクラスを示すインデックスを意味する。

![](https://image.itmedia.co.jp/ait/articles/2004/08/di-capture01.png)

e（**オイラー数**）や、それに対応するnp.exp(x)という後述のコードについては、「[シグモイド関数](https://atmarkit.itmedia.co.jp/ait/articles/2003/04/news021.html)」で説明しているので、詳しくはそちらを参照してほしい。オイラー数は、微分計算がしやすいというメリットがある。具体的に上記の数式の**導関数**（**Derivative function**：微分係数の関数）を求めると、次のように非常にシンプルな式になる。

![](https://image.itmedia.co.jp/ait/articles/2004/08/di-capture02.png)


---
https://qiita.com/SabanoMizuni/items/ab4b73cd9b8e733da11a

## シグモイド関数との違い

シグモイド関数（sigmoid function）は、機械学習において多く用いられる関数です。  

$$
S(x) = \frac{1}{1 + \text{exp}(-x)}
$$
のような関数で表現されます。下図のとおりの単調増加関数です。

![[Pasted image 20251006205700.png]]

### シグモイド関数の性質

数学的には似た性質を持つ関数も多く、非常に興味深いのですが、機械学習への応用に限定して重要な性質を挙げます。  
シグモイド関数は、$(\infty, -\infty) \rightarrow (0, 1)$ となるよう、 $y=0$ 及び $y=1$ を持つ単調増加関数です。これが意味するのは、 $x$ がどのような値を取ろうとも $S(x)$ の関数値は $0$ から $1$ の間に押し込められることを意味します。確率論に出てくる累積分布関数（cumulative distribution function）の性質と同様であることから、当該関数は**ニューラルネットワークにおいて入力値 $x$ を確率値に相当する出力値 $S(x)$ に変換**する活性化関数（activation function）としてよく用いられます。


### ソフトマックス関数

シグモイド関数がある一つの値（実数値）の入力に対して出力を行った関数であるのに対し、ソフトマックス関数は**複数値からなるベクトル**を入力し、それを正規化したベクトルを出力します。

ソフトマックス関数は、複数値からなる入力値ベクトル $x$ がどのような値を取ろうとも、出力値ベクトルの各要素を $0$ から $1$ の間に押し込め、全要素の合計が $1$ となるように正規化する性質を持ちます。当該関数は**ニューラルネットワークにおいて入力値ベクトルの各要素を確率値に相当する出力値ベクトルに変換**する活性化関数（activation function）としてよく用いられます。

### 機械学習における使い分け

以上のことから、シグモイド関数とソフトマックス関数には、共通する性質が読み取れます。  
$n=2$の場合のソフトマックス関数は、以下のとおり表現されます。

$$
y_1 = \frac{\exp(x_1)}{\exp(x_1) + \exp(x_2)} = \frac{1}{1 + \frac{\exp{x_2}}{\exp{x_1}}} = \frac{1}{1 + \exp{(-(x_1 - x_2))}}
$$

このように、$n=2$ のソフトマックス関数はシグモイド関数であり、シグモイド関数を一般化したものがソフトマックス関数です。
こうした両者の関数の関係性と、共に確率値を出力する関数であるという性質から、機械学習の分類問題においては**2値分類ではシグモイド関数**、**多値分類（3値以上）ではソフトマックス関数**が、出力層の活性化関数として用いられます。  
（なお、回帰問題では出力層の活性化関数として恒等関数（identity function）を用います。）

