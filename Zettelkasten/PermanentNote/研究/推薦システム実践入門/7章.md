---
title: 推薦システムの評価
AI: "false"
published: 2025-06-16
description: 
tags:
  - permanent-note
  - recommend
  - oreilly
---
---
## 3つの評価方法の概要

#### オフライン評価
オフライン評価では、**実際のサービス上での閲覧や購買などのユーザーの行動履歴から得られた過去のログ(サービスログ)** を用いてモデルの予測精度などを評価する。サービスログを用いたオフライン評価のメリットは、評価のコストが低いことや、データ量が豊富なため評価結果のばらつきが小さいことにある。
#### オンライン評価
オンライン評価は、新しいテスト対象の推薦モデルや新しいユーザーインタフェースを**一部のユーザーへ実際に掲出する**ことを通して評価を行う。売上などのビジネス目標にどのくらい貢献したかを直接知ることができ、オンライン評価よりも正確な評価が行いやすい。
#### ユーザースタディ
**ユーザーにインタビューやアンケートを行う**ことで、推薦モデルやユーザーインタフェイスの定性的な性質を調査する。ユーザー体験に関するフィードバックを直接得ることが出来る一方、個人の趣向による回答のばらつきが大きく、データ量を十分に得ることが難しいために再現性に欠けるという問題もある。
![[Pasted image 20250616193442.png]]

## オフライン評価

#### モデルの精度評価
ウェブ上に公開されている推薦システムのサービスログは、ユーザーの行動ログのみが1つ与えられるケースと、あらかじめ学習データセットとテストデータセットが別々に与えられるケースがある。
推薦システムのサービス形態によっては、**アイテムの消費に関した時系列の情報が推薦の精度に大きく寄与する**場合がある。このような場合、実サービスの運用の際にはモデルの学習時において得ることのできないはずの**未来の情報を用いた学習(リーク)を避ける必要がある。** 
#### モデルのチューニング
モデルのチューニングとは、**予測性能が高くなるようにモデルが持つパラメータを調整すること。** このチューニングは、学習データ内のバリデーション(valid)データに対するバリデーションを通して行い、決してテストデータでチューニングを行わないように注意。テストデータでチューニングを行ってしまうと、本来の目的である **「未知のデータに対する」汎化性能が評価できなくなってしまうため。**
パラメータのチューニングには、**手動で行う方法**/**グリッドサーチ**/**ベイズ最適化**で行う選択肢がある。**手動で行う方法**は、モデルの作成者がパラメータを手動で動かしながらチューニングを行う方法。**グリッドサーチ**は各パラメータの候補をあらかじめ絞ったうえで、各パラメータのすべての組み合わせに対して評価を行う方法。**ベイズ最適化**はパラメータチューニングの仮定において、以前の検証結果を用いて、以降のパラメータをベイズ確率の枠組みから選択する方法。
#### 評価指標
###### 予測誤差指標
学習モデルがどれほどテストデータの評価値に近い予測ができるかを測る。
- MAE
- MSE, RMSE
###### 集合の評価指標
モデルが出力したスコアの高いk個のアイテム集合に関する抽出能力を測る。クリックや購買の有無などの二値分類の精度の評価や、推薦の幅広さを図るために利用される。
- Precision
- Recall
- F1-measure
###### ランキング評価指標
アイテムの順序を考慮したランキングの評価に用いられる。モデルが出力したスコアの高いk個のアイテムがどれほど正しく並んでいるかを測る。
- nDCG
- MAP
- MRR
###### その他の評価指標
クリックの有無のような予測精度以外で、ユーザーの満足度を間接的に測る。
- カバレッジ
- 多様性
- 新規性
- セレンディピティ
#### 予測誤差指標
###### MAE ... Mean Absolute Error
- 予測値と実測値の差の絶対値の平均で表現される
###### MSE ... Mean Squared Error
- 予測値と実測値の差の2乗の平均で表される
###### RMSE ... Root Measn Squared Error
- MSEに平方根をとり、予測値や実測値との次元を合わせた指標
#### 集合の評価指標
以下では、モデルがユーザーにクリックされると予測したアイテム集合と(予測アイテム集合)と、ユーザーが実際にクリックしていたアイテム集合(適合アイテム集合)を入力として、Precision, Recall, F1-measureの指標を扱う。
###### Precision(適合率)
**Precision@K = |C \cap R_K| / K**
Precisionは、予測アイテム集合の中に存在する適合アイテムの割合。
###### Recall(再現率)
**Recall@K = |C \cap R_K| / C**
Recallは、予測アイテム集合の要素がどれくらい適合アイテム集合の要素をカバーできているかの割合。

PrecisionとRecallは、どちらかの数値が上がると一方の数値が下がるトレードオフの関係が良く見られる。そこで、PrecisionとRecallの両方を加味して評価する指標にF1-measureがある。

###### F1-measure
**F1 = 2 * Recall * Precision / (Recall + Precision)**
F1-measureは、PrecisionとRecallの調和平均として表現される。
#### ランキング評価指標
###### PR曲線
Top@KをTop@1、Top@2、...、Top@Nと変えていくと、対応するRecallとPrecisionの組み合わせが複数得られる。これらの点について、Recallを横軸に、Precisionを縦軸にプロットし、各点を結んだものが**PR曲線**である。この曲線が右上に位置するほど精度が高い推薦であると言える。このPR曲線とRecallとPrecisionの各軸で囲まれる面積が**PR曲線のAUC(Area Under Curve)** であり、1に近いほど精度が高い。
###### MRR@K(Mean Recipirocal Rank)
**MRR@K = 1/|U| * \sum_{u \in U} 1/k_u**
ここで、Uはユーザー全体の集合、k_uはランキングK番目以内の最初の適合位置を表す。**MRRは、ユーザーのランキングにおける最初の適合アイテムが、どれほどランキングの上位に位置しているかを評価する指標**である。
###### AP@K(Average Precision)
AP@Kは、ランキングのK番目までにおける各適合アイテムまでのPrecisionを平均した値となる。
###### MAP@K(Mean Average Precision)
MAPはAPを各ユーザーに対して平均を取った値になる。
###### nDCG(normalized Discounted Cumulative Gain)
今まで紹介してきたランキング指標は、クリックの有無のような二値に対する指標だった。しかしサービスによっては、各アイテムがクリックされた後の購買の有無など、クリック以外の行動まで含めて重みを付け、多値の評価を行いたい状況もある。そこで、nDCGが登場。詳しくは省略。
#### その他の指標
###### カタログカバレッジ(Catalogue Coverage)
カタログカバレッジの分子は、実際に推薦した商品の数で、分母が全商品の数。このように、カバレッジは推薦の幅広さを測る指標である。
###### ユーザーカバレッジ(User Coverage)
ユーザーカバレッジは、どれくらいのユーザーに対して推薦が行えたかを測る指標。分子は、実際に推薦が行えたユーザーの数で、分母が全ユーザーである。初期ユーザーに対するユーザーカバレッジが低い場合、初期ユーザーに対して推薦が行えていないコールドスタート問題が起きていることを意味する。
###### 新規性(Novelty)
Noveltyは、ランキングにおける推薦アイテムの真新しさを表す。
###### 多様性(Diversity)
ランキングRにおける多様性は、ランキングの各アイテム間の類似度の距離の平均値によって定義される。
###### セレンディピティ(Serendipity)
セレンディピティはランキングにおける意外であり、かつ有用なアイテムの割合を測定する。

## オンライン評価

主にビジネス関連の評価であるため割愛。

## ユーザースタディによる評価

主にビジネス関連の評価であるため割愛。