---
title: "Reparameterization Trickã«ã¤ã„ã¦"
source: "https://qiita.com/Ogin0pan/items/ce58b6da95a66c04fefe"
author:
  - "[[Ogin0pan]]"
published: 2024-12-02
created: 2025-11-03
description: "Reparameterization Trickã«ã¤ã„ã¦ IPFactory Advent Calender 2024 2æ—¥ç›®ã®è¨˜äº‹ã§ã™ã€‚ Reparameterization Trick ã¯ã€å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ (Variational Autoencoder..."
tags:
  - "clippings"
  - "vae"
  - "reparameterization-trick"
---
![](https://relay-dsp.ad-m.asia/dmp/sync/bizmatrix?pid=c3ed207b574cf11376&d=x18o8hduaj&uid=3999280)


> [!NOTE] Reparameterization Trick
> VAE ãªã©ã§æå¤±é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã«ã€å‹¾é…é™ä¸‹æ³•ã‚’ä½¿ã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
> ã ãŒã€
> $$
> 	z \approx N(\mu, \sigma^2)
> $$
> ã¨ã„ã†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ“ä½œã¯ç¢ºç‡çš„ã§ã€**ä¹±æ•°ã«ä¾å­˜ã™ã‚‹**éæ±ºå®šçš„ãªæ“ä½œã€‚ã ã‹ã‚‰ $\frac{\partial z}{\partial \mu}$, $\frac{\partial z}{\partial \sigma}$ ãŒè¨ˆç®—ã§ããªã„ã€‚
> å…·ä½“çš„ã«ã¯ã€ $\mu$ ã‚’å°‘ã—å¢—ã‚„ã—ã¦å†åº¦å®Ÿè¡Œã™ã‚‹ã¨ã€ä¹±æ•°ã«ä¾å­˜ã™ã‚‹ã®ã§ $z$ ãŒç¢ºç‡çš„ã«å¤‰ã‚ã£ã¦ã—ã¾ã„ã€ æ›´æ–°ã«ã‚ˆã£ã¦ $z$ ã®å€¤ãŒã©ã†ãªã£ãŸã®ã‹ä¸€è²«ã—ãªã„ã€‚
> ä¸€æ–¹ã€ $z = \mu + \sigma \cdot \epsilon, \; \epsilon \approx \mathcal{N}(0,1)$ ã¨ã™ã‚Œã°ã€ $\epsilon$ ã¯ç¢ºç‡çš„ã ãŒã€ $\mu$ ã¨ $\sigma$ ã«ä¾å­˜ã—ãªããªã‚Šã€ $\frac{\partial z}{\partial \mu} = 1, \frac{\partial z}{\partial \sigma} = \epsilon$ ã¨ãªã‚‹ã€‚

IPFactory Advent Calender 2024 2æ—¥ç›®ã®è¨˜äº‹ã§ã™ã€‚

Reparameterization Trick ã¯ã€å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ (Variational Autoencoder, VAE)  
ã®ã‚ˆã†ãªæ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã•ã‚Œã‚‹é‡è¦ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã§ã™ã€‚  
ã“ã®ãƒˆãƒªãƒƒã‚¯ã¯ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ“ä½œã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹ãŸã‚ã«è€ƒæ¡ˆã•ã‚Œã€å‹¾é…é™ä¸‹æ³•ã‚’ç”¨ã„ãŸæœ€é©åŒ–ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚  
æœ¬è¨˜äº‹ã§ã¯ã€ç†è«–çš„ãªèƒŒæ™¯ã‹ã‚‰å®Ÿè£…ä¾‹ã¾ã§ã‚’è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

## ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®èª²é¡Œ

VAE ã§ã¯ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ $x$ ã‹ã‚‰æ½œåœ¨å¤‰æ•° $z$ ã‚’æ¨å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚  
  
ã“ã®éš›ã€æ½œåœ¨å¤‰æ•°ã®åˆ†å¸ƒ $qÏ•(z|x)$ ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã¾ã™ã€‚

ã“ã“ã§ã€ $Î¼$ ã¯å¹³å‡ã€ $Ïƒ2$ ã¯åˆ†æ•£ã‚’è¡¨ã—ã¾ã™ã€‚ã“ã®åˆ†å¸ƒã‹ã‚‰ç›´æ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã¨ã€  
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ“ä½œãŒç¢ºç‡çš„ã§ã‚ã‚‹ãŸã‚ã€ãã®éç¨‹ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‹¾é…è¨ˆç®—ã«åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒå›°é›£ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã®æœ€é©åŒ–ãŒå¦¨ã’ã‚‰ã‚Œã¾ã™ã€‚

## è§£æ±ºç­–ã¨ã—ã¦ã® Reparameterization Trick

Reparameterization Trick ã¯ã€ç¢ºç‡çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ã€Œå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã€ã™ã‚‹ã“ã¨ã§å¾®åˆ†å¯èƒ½ãªæ“ä½œã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ“ä½œã‚’åˆ†è§£ã—ã¾ã™

- æ¨™æº–æ­£è¦åˆ†å¸ƒ $N(0,1)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
- ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã«ã‚¹ã‚±ãƒ¼ãƒ« ( $Ïƒ$ ) ã¨ã‚·ãƒ•ãƒˆ ( $Î¼$ ) ã‚’é©ç”¨ã—ã¦ç›®çš„ã®åˆ†å¸ƒã«å¤‰æ›ã™ã‚‹

å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®å¼ã§è¡¨ç¾ã•ã‚Œã¾ã™ã€‚

ã“ã“ã§ã€ $Ïµâˆ¼N(0,1)$ ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚ã“ã®å¤‰æ›ã¯ç·šå½¢æ“ä½œã®ã¿ã‚’å«ã‚€ãŸã‚ã€å¾®åˆ†å¯èƒ½ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å‹¾é…é™ä¸‹æ³•ã‚’ç”¨ã„ãŸæœ€é©åŒ–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

## Reparameterization Trick ã®ä»•çµ„ã¿

Reparameterization Trick ã‚’ç°¡å˜ã«è¨€ã†ã¨ã€  
ã€Œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ“ä½œã‚’æ•°å­¦çš„ã«åˆ†è§£ã—ã¦ã€å¾®åˆ†å¯èƒ½ãªéƒ¨åˆ†ã¨ç¢ºç‡çš„ãªéƒ¨åˆ†ã«åˆ†ã‘ã‚‹ã€  
ã¨ã„ã†è€ƒãˆæ–¹ã§ã™ã€‚

- æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: æ¨™æº–æ­£è¦åˆ†å¸ƒ $N(0,1)$ ã¯ç°¡å˜ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ã§ã‚ã‚Šã€å¾®åˆ†ã«ã¯å½±éŸ¿ã—ãªã„
- ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚·ãƒ•ãƒˆã®é©ç”¨: å¹³å‡ $Î¼$ ã¨åˆ†æ•£ $Ïƒ2$ ã‚’ç”¨ã„ã¦ã€æ¨™æº–æ­£è¦åˆ†å¸ƒã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å¤‰æ›ã™ã‚‹

ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è‡ªä½“ãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ $Î¼$ ã¨ $Ïƒ$ ï¼‰ã«ä¾å­˜ã™ã‚‹å½¢ã«ãªã‚Šã¾ã™ã€‚ã“ã‚ŒãŒæœ€é©åŒ–å¯èƒ½ãªå½¢ã«å¤‰æ›ã™ã‚‹éµã§ã™ã€‚

## å®Ÿè£…

### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ‡ãƒ¼ã‚¿import

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

transform = transforms.Compose([
    transforms.ToTensor()  # [0, 1] ã«ã‚¹ã‚±ãƒ¼ãƒ«
])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
```

- MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ28Ã—28ãƒ”ã‚¯ã‚»ãƒ«ã®æ‰‹æ›¸ãæ•°å­—ç”»åƒï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- ãƒ‡ãƒ¼ã‚¿ã‚’ \[0, 1\] ã®ç¯„å›²ã«æ­£è¦åŒ–
- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãã‚Œãã‚Œæº–å‚™

### VAEãƒ¢ãƒ‡ãƒ«ã®å®šç¾©

```python
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc_mu = nn.Linear(128, latent_dim)      # å¹³å‡
        self.fc_logvar = nn.Linear(128, latent_dim)  # å¯¾æ•°åˆ†æ•£
        #å…¥åŠ›ç”»åƒã‚’åœ§ç¸®ã—ã€æ½œåœ¨ç©ºé–“ã®å¹³å‡ï¼ˆmuï¼‰ã¨å¯¾æ•°åˆ†æ•£ï¼ˆlogvarï¼‰ã‚’å‡ºåŠ›

        
        self.fc2 = nn.Linear(latent_dim, 128)
        self.fc3 = nn.Linear(128, input_dim)
        # æ½œåœ¨å¤‰æ•°ï¼ˆzï¼‰ã‚’å…ƒã«ç”»åƒã‚’å†æ§‹æˆ

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        mu = self.fc_mu(h1)
        logvar = self.fc_logvar(h1)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)  # æ¨™æº–åå·®
        eps = torch.randn_like(std)    # æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        return mu + eps * std
        #æ½œåœ¨ç©ºé–“ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã®ãƒˆãƒªãƒƒã‚¯ã€‚
        #ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ“ä½œã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹ã€‚

    def decode(self, z):
        h2 = F.relu(self.fc2(z))
        return torch.sigmoid(self.fc3(h2))  # [0, 1] ã«ã‚¹ã‚±ãƒ¼ãƒ«

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```

### æå¤±é–¢æ•°

```python
def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD
```

- å†æ§‹æˆèª¤å·®ï¼ˆBCEï¼‰
	- å†æ§‹æˆç”»åƒï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€å‡ºåŠ›ï¼‰ã¨å…ƒã®ç”»åƒã®ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã®å·®ã‚’è¨ˆç®—
- KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆKLDï¼‰
	- æ½œåœ¨å¤‰æ•°ã®åˆ†å¸ƒã‚’æ¨™æº–æ­£è¦åˆ†å¸ƒï¼ˆå¹³å‡0ã€åˆ†æ•£1ï¼‰ã«è¿‘ã¥ã‘ã‚‹ãŸã‚ã®æ­£å‰‡åŒ–

### ãƒ¢ãƒ‡ãƒ«ã¨æœ€é©åŒ–ã®æº–å‚™

```python
input_dim = 28 * 28  # MNISTã®ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆ28x28ãƒ”ã‚¯ã‚»ãƒ«ï¼‰
latent_dim = 20
model = VAE(input_dim, latent_dim)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
```

- input\_dim
	- å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆMNISTç”»åƒï¼‰ã¯28Ã—28ãƒ”ã‚¯ã‚»ãƒ«
	- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«æ¸¡ã™éš›ã€2æ¬¡å…ƒé…åˆ—ã‚’1æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã™ã‚‹ãŸã‚ã€  
		æ¬¡å…ƒã¯ 28Ã—28=784
	- input\_dim = 784 ãŒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›å±¤ã®ã‚µã‚¤ã‚º
- latent\_dim
	- æ½œåœ¨ç©ºé–“ã®æ¬¡å…ƒæ•°ã‚’æŒ‡å®šï¼ˆã“ã“ã§ã¯20)
	- æ½œåœ¨ç©ºé–“ã¨ã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒé«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®ã™ã‚‹ä½æ¬¡å…ƒã®éš ã‚Œè¡¨ç¾ã®ã“ã¨
	- æ½œåœ¨ç©ºé–“ã®æ¬¡å…ƒæ•°ã¯ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾åŠ›ã¨è¨ˆç®—è² è·ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«å½±éŸ¿ã™ã‚‹
		- å°ã•ã™ãã‚‹ã¨è¡¨ç¾åŠ›ãŒä¸è¶³
		- å¤§ãã™ãã‚‹ã¨éå­¦ç¿’ã‚„è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—åŠ ã®ãƒªã‚¹ã‚¯
- VAE
	- å…ˆã»ã©å®šç¾©ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
	- ã“ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ model ã«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ãŒå«ã¾ã‚Œã‚‹
		- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
			- å…¥åŠ›æ¬¡å…ƒ input\_dim ã‚’éš ã‚Œè¡¨ç¾ latent\_dim ã«åœ§ç¸®
		- ãƒ‡ã‚³ãƒ¼ãƒ€
			- æ½œåœ¨ç©ºé–“ latent\_dim ã‹ã‚‰å…ƒã®ãƒ‡ãƒ¼ã‚¿æ¬¡å…ƒ input\_dim ã‚’å†æ§‹æˆ
- input\_dim ã¨ latent\_dim ã®ç”¨é€”
	- input\_dim: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›å±¤ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›å±¤ã®ã‚µã‚¤ã‚º
	- latent\_dim: æ½œåœ¨ç©ºé–“ã®æ¬¡å…ƒæ•°ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›å±¤ãŠã‚ˆã³ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›å±¤ã®ã‚µã‚¤ã‚º

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—

```python
model.train()
epochs = 10
for epoch in range(epochs):
    train_loss = 0
    for data, _ in train_loader:
        data = data.view(-1, 28 * 28)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
    print(f"Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset):.4f}")
```

```text
Epoch 1, Loss: 169.6315
Epoch 2, Loss: 127.2651
Epoch 3, Loss: 119.4524
Epoch 4, Loss: 115.9890
Epoch 5, Loss: 114.0870
Epoch 6, Loss: 112.8046
Epoch 7, Loss: 111.8957
Epoch 8, Loss: 111.2991
Epoch 9, Loss: 110.7355
Epoch 10, Loss: 110.3423
Test Accuracy: 0.8951
```

- ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰éƒ¨åˆ†ã§ Reparameterization Trick ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

```python
def reparameterize(self, mu, logvar):
    std = torch.exp(0.5 * logvar)  # æ¨™æº–åå·®ã‚’è¨ˆç®—
    eps = torch.randn_like(std)    # æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    return mu + eps * std          # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸæ½œåœ¨å¤‰æ•°
```

### ã©ã®ã‚ˆã†ã«Reparameterization Trickã‚’ä½¿ã£ã¦ã„ã‚‹ã‹

- å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆVAEï¼‰ã¯æ½œåœ¨ç©ºé–“ã‚’ç¢ºç‡åˆ†å¸ƒï¼ˆæ­£è¦åˆ†å¸ƒï¼‰ã¨ã—ã¦è¡¨ç¾ã™ã‚‹
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‹ã‚‰å‡ºåŠ›ã•ã‚Œã‚‹å¹³å‡ï¼ˆmuï¼‰ã¨åˆ†æ•£ï¼ˆlogvarï¼‰ã‚’ç”¨ã„ã¦æ½œåœ¨å¤‰æ•°ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹
- **å•é¡Œç‚¹**
	- é€šå¸¸ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ“ä½œï¼ˆä¹±æ•°ç”Ÿæˆï¼‰ã¯éå¾®åˆ†å¯èƒ½ã§ã‚ã‚‹ãŸã‚ã€é€†ä¼æ’­ï¼ˆbackpropagationï¼‰ã‚’é€šã˜ãŸå­¦ç¿’ãŒã§ããªã„
- **è§£æ±ºæ–¹æ³•**
	- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ã€Œå¾®åˆ†å¯èƒ½ãªå½¢ã€ã«åˆ†è§£ã™ã‚‹ã®ãŒ Reparameterization Trick

### åˆ†æ•£ï¼ˆæ¨™æº–åå·®ï¼‰ã‚’è¨ˆç®—

```python
std = torch.exp(0.5 * logvar)
```

- logvarï¼ˆå¯¾æ•°åˆ†æ•£ï¼‰ã‚’ç”¨ã„ã¦æ¨™æº–åå·®ï¼ˆstdï¼‰ã‚’è¨ˆç®—

$Ïƒ=expâ¡(logâ¡var2)$

ã“ã“ã§ã€  
ğœã¯æ¨™æº–åå·®

### æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ä¹±æ•°ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
eps = torch.randn_like(std)
```

å¹³å‡0ã€åˆ†æ•£1ã®æ¨™æº–æ­£è¦åˆ†å¸ƒï¼ˆ $N(0,1)$ ï¼‰ã‹ã‚‰ä¹±æ•° $Ïµ$ ã‚’ç”Ÿæˆ

### ç·šå½¢å¤‰æ›ã§æ½œåœ¨å¤‰æ•°ã‚’ç”Ÿæˆ

```python
return mu + eps * std
```

æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœã‚’ã€ä»¥ä¸‹ã®å¼ã§å¤‰æ›ã—ã¾ã™

$$
z=Î¼+Ïµâ‹…Ïƒ
$$

- $Î¼$ : ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§å‡ºåŠ›ã•ã‚ŒãŸå¹³å‡
- $Ïƒ$ : æ¨™æº–åå·®
- $Ïµâˆ¼N(0,1)$ : æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ä¹±æ•°

ã“ã®å½¢å¼ã«ã™ã‚‹ã“ã¨ã§ã€ $Î¼$ ã¨ $Ïƒ$ ã®é–¢æ•°ã¨ã—ã¦æ½œåœ¨å¤‰æ•° $z$ ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚

## ãªãœã“ã‚ŒãŒReparameterization Trickãªã®ã‹

é€šå¸¸ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆä¾‹: torch.randnï¼‰ã¯éå¾®åˆ†å¯èƒ½ã§ã™ãŒã€mu ã¨ logvar ã‚’ä½¿ã£ãŸã“ã®å¤‰æ›ã¯å¾®åˆ†å¯èƒ½ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å‹¾é…é™ä¸‹æ³•ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’çµ±ä¸€çš„ã«å­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚  

å…·ä½“çš„ã«ã¯ã€mu ã¨ logvar ã«å¯¾ã—ã¦å‹¾é…ã‚’è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ãŸã‚ã€æå¤±é–¢æ•°ãŒæ½œåœ¨å¤‰æ•°ğ‘§ã‚’é€šã˜ã¦ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã«é€†ä¼æ’­ã—ã¾ã™ã€‚

## ã¾ã¨ã‚

**Reparameterization Trick** ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ç¢ºç‡çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¾®åˆ†å¯èƒ½ãªå½¢å¼ã«å¤‰æ›ã—ã€VAE ãªã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«æœ€é©åŒ–ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚  
æœ¬è¨˜äº‹ã§ã¯ã€ç†è«–çš„èƒŒæ™¯ã¨å…±ã«å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ç¤ºã—ã¾ã—ãŸã€‚  
ã“ã®æ‰‹æ³•ã¯æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã¶ä¸Šã§é¿ã‘ã¦é€šã‚Œãªã„é‡è¦ãªæŠ€è¡“ã§ã™ã®ã§ã€ãœã²å®Ÿè£…ã—ã¦ç†è§£ã‚’æ·±ã‚ã¦ã¿ã¦ãã ã•ã„ã€‚

> å‚è€ƒæ–‡çŒ®  
> [https://qiita.com/pocokhc/items/d438a13d4c6ef861364a](https://qiita.com/pocokhc/items/d438a13d4c6ef861364a)  
> [https://gregorygundersen.com/blog/2018/04/29/reparameterization/](https://gregorygundersen.com/blog/2018/04/29/reparameterization/)  
> [https://leimao.github.io/blog/Reparameterization-Trick/](https://leimao.github.io/blog/Reparameterization-Trick/)  
> [https://sassafras13.github.io/ReparamTrick/](https://sassafras13.github.io/ReparamTrick/)

[0](https://qiita.com/Ogin0pan/items/#comments)

ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§ã¸ç§»å‹•

Xï¼ˆTwitterï¼‰ã§ã‚·ã‚§ã‚¢ã™ã‚‹

Facebookã§ã‚·ã‚§ã‚¢ã™ã‚‹

ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ ã™ã‚‹

## Qiita Conference 2025 Autumn 11æœˆ5æ—¥(æ°´)~7æ—¥(é‡‘)é–‹å‚¬ï¼

![](https://cdn.qiita.com/assets/public/official_campaigns/qiita_conference_2025_autumn/image-conference_2025_autumn_ogp-d35d2500cd0420d93f2ed69a8d162d74.png)

Qiita Conferenceã¯ã€AIæ™‚ä»£ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«è´ˆã‚‹Qiitaæœ€å¤§è¦æ¨¡ã®ãƒ†ãƒƒã‚¯ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§ã™ï¼

åŸºèª¿è¬›æ¼”ã‚²ã‚¹ãƒˆ(æ•¬ç§°ç•¥)

piacereã€ç‰›å°¾ å‰›ã€Esteban Suarezã€å’Œç”° å“äººã€seyaã€ãƒŸãƒé§†å‹•ã€å¸‚è°· è¡å•“ã€ã‹ã‚‰ã‚ã’ã€å²©ç€¬ ç¾©æ˜Œã€ã¾ã¤ã‚‚ã¨ã‚†ãã²ã‚ã€ã¿ã®ã‚‹ã‚“ã€ Null-Sensei

[ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ã‚’è¦‹ã‚‹](https://qiita.com/official-campaigns/conference/2025-autumn)

## Qiita Advent Calendar é–‹å‚¬ï¼

[5](https://qiita.com/Ogin0pan/items/ce58b6da95a66c04fefe/likers)

ã„ã„ã­ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§ã¸ç§»å‹•

0