
# 🧠 事前学習と表現学習の関係メモ

## 🧩 全体の流れ

``` markdown
生データ（例：「猫が好きです」）
　↓
① 事前学習モデル（BERTなど）
　→ 高次元の意味ベクトル（例：768次元）
　↓
② 表現学習（Autoencoder / Contrastive Learningなど）
　→ 低次元の潜在表現（例：64次元）
　↓
③ 下流タスク（分類・検索・推薦など）

```

---

## 🧠 ① 事前学習（Pre-training）

### 概要

- 大規模データ（文章・画像など）を用いて**一般的な意味構造を学ぶ段階**。
- 目的は「**ベクトル化（embedding）**」＝非数値データを数値表現に変換すること。

### 例

|モデル|入力|出力|次元数の目安|
|---|---|---|---|
|BERT / RoBERTa|文章|意味ベクトル|768〜1024|
|CLIP / ResNet|画像|特徴ベクトル|512〜2048|
|wav2vec2.0|音声|音響ベクトル|512〜1024|

### ポイント

- **「意味を抽出」**するが、汎用的で情報量が多い（＝高次元・冗長）。
- この段階で得られるベクトルは「素材」として使う。

---

## 🧩 ② 表現学習（Representation Learning）

### 概要

- 事前学習モデルの高次元ベクトルをもとに、  **下流タスクに適した“構造的・低次元な潜在表現”を学ぶ段階**

### 手法例

|手法|アプローチ|出力の意味|
|---|---|---|
|Autoencoder|入力ベクトルを圧縮→再構成|情報圧縮による潜在表現|
|Contrastive Learning|類似サンプルを近づけ、異なるものを遠ざける|概念距離を学習した表現|
|Projection Layer (MLP)|下流タスク向けに変換|分類・推薦に最適化|

### 出力

- 低次元ベクトル（例：32〜128次元）
- 意味的に整理された「潜在空間（latent space）」

---

## ⚙️ ③ 下流タスク（Downstream Tasks）

表現学習で得た潜在ベクトルを利用して：

- 分類（感情分析、話題推定など）
- 検索・類似度計算（文検索、画像検索）
- 推薦（ユーザー×アイテムの類似性計算）
    

---

## 🧭 まとめ

|段階|入力|出力|目的|
|---|---|---|---|
|事前学習|生データ|高次元embedding|意味を抽出・数値化|
|表現学習|高次元embedding|低次元潜在表現|構造を圧縮・整理|
|下流タスク|潜在表現|予測・分類結果|応用・意思決定|

---

> 📝 **要するに：**
> 
> - 「事前学習」＝意味を学ぶ（高次元の素材）
> - 「表現学習」＝構造を整える（低次元の理解）
> - 二段階で“人間が理解しやすく・モデルが使いやすい表現”を作り出す。