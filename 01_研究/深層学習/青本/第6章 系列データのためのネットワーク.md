---
title:
AI: "false"
published:
created: " 2025-11-07"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 6

## 6.1 系列データ

系列データを扱う問題の例
1. 入力として1つの文が与えられ、それをいくつかのクラスに分類する問題
2. 発話を記録した時間信号からその発話内容を推定する **音声認識 (speech recognition)** である。
いずれの場合も、推定すべきものも系列データであり、その系列長は入力系列の長さとは異なることに注意する。

---
## 6.2 リカレントニューラルネットワーク

### 6.2.1 概要

**リカレントニューラルネットワーク (recurrent neural network)** とは、内部に（有向）閉路を持つニューラルネットワークの総称で、系列データを扱う。
- Elman ネットワーク
- Jordan ネットワーク
- 時間遅れネットワーク
- エコー状態ネットワーク
など様々なものがある。

ここでは、順伝播型ネットワークと同様の構造を持ち、ただし中間層のユニットの出力が自分自身に戻される「帰還路」を持つシンプルなものを考える。
→ この構造により、**情報を一時的に記憶し、また振る舞いを動的に変化させることが出来る。**

RNN は各時刻 $t$ につき1つの入力 $\mathbf{x}^t$ を受け取り、また同時に1つの出力 $\mathbf{y}^t$ を返す。つまり入力と同じ長さの系列を出力する。
**順伝播型ネットワークが入力1つに対し1つの出力を与える写像を表すのに対し、RNN は（少なくとも理論上）過去のすべての入力から1つの出力への写像を表す。**


### 6.2.2 順伝播の計算

系列 $\mathbf{x}^1, \mathbf{x}^2$ を RNN に順番に入力すると、対応する出力は系列 $\mathbf{y}^1, \mathbf{y}^2, \cdots$ を与える。

RNN の帰還路は、中間層の出力を自らの入力に戻すが、この間の結合は全ユニット間で存在する。
時刻 $t-1$ における中間層の任意のユニット $j'$ から時刻 $t$ における中間層の任意のユニット $j$ へ、重み $w_{jj'}$ の結合が存在する。
したがって、時刻 $t$ における中間層の各ユニットへの入力は、同時刻 $t$ にて入力層から届くものと、時刻 $t-1$ の中間層の出力をフィードバックしたものとの和
$$
	u_j^t = \sum_i w_{ji}^{(in)} x_i^t + \sum_{j'} w_{jj'} z_{j'}^{t-1}
$$
となる。中間層のバイアスは $w_{j0}^{(in)}$ ($x_0^t=1$ と固定) が与えられる (バイアスを使わないこともあるが)。
ここから中間層の出力は、活性化関数 $f$ を経由して、
$$
	z_j^t = f(u_j^t)
$$
と計算される。**RNN の活性化関数には伝統的に tanh が多く使われてきたが、ReLU も使う。**
以上をまとめると、
$$
	\mathbf{z}^t = \mathbf{f}(\mathbf{W}^{(in)} \mathbf{x}^t + \mathbf{W} \mathbf{z}^{(t-1)})
$$
のように書ける。

![[PXL_20251107_101739895.jpg]]

RNN の出力 $\mathbf{y}^t$ は次のように計算する。まず出力層への入力は、中間層の出力 $\mathbf{z}^t$ から、
$$
	v_k^t = \sum_j w_{kj}^{(out)} z_j^t
$$
と決まる。活性化関数を $\mathbf{f}^{\text{out}}$ と書くと、以上をまとめて、
$$
	\mathbf{y}^t = \mathbf{f}^{(\text{out})} = \mathbf{f}(\mathbf{w}^{(\text{out})} \mathbf{z}^t)
$$
と書ける。深層ニューラルネットワークでは3.6.3項のバッチ正規化に代表される層出力の正規化が採用されるが、RNN の場合にはインスタンス正規化が採用される。
**→ RNN の場合、同じ層を何度も信号が循環するので、バッチ内のサンプル集合を対象に統計量を計算することが意味をなさないため。**

### 6.2.3 問題への適用

RNN の出力層の設計は、順伝播型ネットワークと同じ。例えばクラス分類であれば、クラス数 ($C$ とする) と同数のユニットを並べ、ソフトマックス関数を活性化関数に選ぶ。
出力系列 $\mathbf{y}_1,\cdots,\mathbf{y}^T$  の目標値 $\mathbf{d}^1,\cdots,\mathbf{d}^T$  が与えられたとき、訓練サンプルを $n=1,\cdots,N$ で表し、サンプル $n$ の系列長を $T_n$ と書くと、損失関数は
$$
	E(\mathbf{w}) = - \sum_{n=1}^N \sum_{t=1}^{T_n} \sum_{k=1}^C d_{nk}^t \log y_k^t (\mathbf{x}_n^t;\mathbf{w})
$$

| データ名 | 記号    | 形状（実装ベース）                  | 意味                                       |
| ---- | ----- | -------------------------- | ---------------------------------------- |
| 入力系列 | $(X)$ | $([N, T, d_{\text{in}}])$  | 各時刻 (t) における入力特徴ベクトル（次元 (d_{\text{in}})） |
| 出力系列 | $(Y)$ | $([N, T, d_{\text{out}}])$ | 各時刻 (t) における出力ベクトル（次元 (d_{\text{out}})）  |

> [!NOTE] RNN と深層ニューラルネットの次元
> RNN ... 3次元テンソル $X \in \mathbb{R}^{N \times T \times d^{\text{in}}}$ 
> 深層ニューラルネット ... 2次元テンソル $X \in \mathbb{R}^{N \times d^{\text{in}}}$


一方、入力は系列データであるが、出力は系列ではなく単一の要素である場合もある。
会話の発生信号やテキストから感情を読み取る **感情分析 (sentiment analysis)** はその一例。この場合、例えば RNN の最後の時刻の出力のみを利用する。

---
## 6.3 ゲート機構

### 6.3.1 RNN と勾配消失問題

RNN は、過去の入力の履歴を最新の出力の計算に反映させることが出来る。
**→ 「どれだけ遠い過去の入力を出力に反映させることが出来るのか？」**

**理論上は、過去の入力全てが関わるはずだが、何も工夫しなければせいぜい過去10時刻分程度であると言われている。**
この限界は、順伝播型ネットワークの勾配消失問題と同様の理由で生じる。

基本的な RNN では、入力を短期的に記憶しておくことはできても、**長期にわたって記憶し、出力の計算に利用することは難しい**と言える。

### 6.3.2 長・短期記憶 (LSTM)

長期にわたる記憶を可能にすべく考案されたのが **長・短期記憶 (LSTM: Long Short-Term Memory)** である。
LSTM は基本的な RNN の中間層のユニットをメモリユニットと呼ぶ要素で置き換えた構造を持つ。入出力層の構造や問題への適用方法は基本的に同じ。

![[PXL_20251107_124805578.jpg]]

**メモリセル** ... 状態 $s_j^t$ を保持し、これを1時刻を隔ててメモリセル自身に帰還することで記憶を実現する。
帰還路には途中に忘却ゲートが挿入されており、ユニット f の出力がゲートの値 $g_j^{F,t}$ となる。メモリセルには、 $s_j^t$ に $g_j^{F,t}$ を掛けたものが伝えられ、それが1に近ければ現状態がそのまま記憶され、0に近ければリセットされる。

このような仕組みで、これまでに扱った **RNN の限界「短期間の記憶しか実現できない」の緩和を図る。**
単純な場合、忘却ゲートを1にし、入力ゲートを0にし続ければ、メモリセルの状態は永遠に記憶される。
タイミングよくこれらのゲートを開閉することで、長い文脈を捉えたより高度な推定が可能になる。

＜数式で書く＞
$j$ 番目のメモリユニット内部のメモリセルは変数 $s_j^t$ を保持する。メモリセルの帰還路は変数 $s_j^t$ の中身を1時刻分後に引き継ぐ。
$$
	s_j^t = g_j^{F,t} s_j^{t-1} + g_j^{I,t} f(u_j^t)
$$
第1項 ... 引き継いだ前時刻の状態に関する項
第2項 ... このメモリユニット $j$ が受け取る入力に関する項

第2項の $u_j^t$ について、
$$
	u_j^t = \sum_i w_{ji}^{in}x_i^t + \sum_j' w_{jj'} z_{j'}^{t-1}
$$
また、
$$
	g_j^{F,t} = \sigma(u_j^{F,t}) = \sigma \left( \sum_i w_{ji}^{F, in} x_i^t + \sum_{j'} w_{jj'}^F z_{j'}^{t-1} + w_j^F s_j^{t-1}  \right)
$$
$$
	g_j^{I,t} = \sigma(u_j^{I,t}) = \sigma \left( \sum_i w_{ji}^{I, in} x_i^t + \sum_{j'} w_{jj'}^F z_{j'}^{t-1} + w_j^I s_j^{t-1}  \right)
$$

$\sigma$ ：ロジスティックシグモイド関数
$w_j^F$ ：メモリセルから忘却ゲートの値を決めるユニットへの結合重み
$w_j^I$ ：メモリセルから入力ゲートの値を決めるユニットへの結合重み
この二つの結合重みは、 **のぞき穴 (peephole) 結合**とも呼ばれるが、性能向上への貢献は大きくない場合もあり、その場合は省略される。

メモリユニットからの出力は、
$$
	z_j^t = g_j^{O,t} f(s_j^t)
$$
のように計算される。ただし、
$$
	g_j^{O,t} = \sigma (u_j^{O,t}) = \sigma \left( \sum_i w_{ji}^{O,in} x_i^t + \sum_{j'} w_{jj'}^O z_{j'}^{t-1} + w_j^O s_j^t \right)
$$
のぞき穴結合では、出力ゲートのみ $s_j^{t-1}$ ではなく $s_j^t$ を加算する。
ゲート以外の2箇所の活性化関数 f は、LSTM では通常 tanh が使われる。
メモリユニットの出力 $z_j^t$ は、次の事項の3種のゲートを制御するユニットの入力となる他、出力層のユニットへの入力にもなる。

以上をまとめると、
$$
	\mathbf{u}^t = \mathbf{W}^{in} \mathbf{x}^t + \mathbf{W} \mathbf{z}^{t-1}
$$
$$
	\mathbf{s}^t = \mathbf{g}^{F,t} \odot \mathbf{s}^{t-1} + \mathbf{g}^{I,t} \odot f(\mathbf{u}^t)
$$
$$
	\mathbf{z}^t = \mathbf{g}^{O,t} \odot f(\mathbf{s}^t)
$$
$$
	\mathbf{g}^{F,t} = \sigma(\mathbf{W}^{F,in} \mathbf{x}^t + \mathbf{W}^F \mathbf{z}^{t-1})
$$
$$
	\mathbf{g}^{I,t} = \sigma(\mathbf{W}^{I,in} \mathbf{x}^t + \mathbf{W}^I \mathbf{z}^{t-1})
$$
$$
	\mathbf{g}^{O,t} = \sigma(\mathbf{W}^{O,in} \mathbf{x}^t + \mathbf{W}^O \mathbf{z}^{t-1})
$$

**$\odot$ ：アダマール積 (Hadamard product)**


> [!NOTE] ？？？
> $w_j^O s_j^t$ の部分にあたるベクトル表記がどこにもない...


| 名称                 | 役割                        |
| ------------------ | ------------------------- |
| 隠れ状態（hidden state） | RNNと同様に、出力や次層への入力として使われる。 |
| セル状態（cell state）   | 長期的な情報を保持する「メモリ」。         |


### 6.3.3 その他のゲート付き構造

LSTM は、一般に基本的な RNN よりも高い性能を示すが、構造がかなり複雑であるのでその性能を維持したまま構造を簡素化する試みが行われている。
**LSTM の3つのゲートのうち、忘却ゲートが最も重要であると言われており、ゲート数を減らした構造が考案されている。**

その1つが **更新ゲートRNN (UGRNN: update gate RNN)** である。
UGRNN は次のような計算を行う。
$$
	\mathbf{u}^t = \mathbf{W}^{in} \mathbf{x}^t + \mathbf{W} \mathbf{z}^{t-1}
$$
$$
	\mathbf{s}^t = \mathbf{g}^{U,t} \odot \mathbf{s}^{t-1} + (\mathbf{1} - \mathbf{g}^{U,t} \odot f(\mathbf{u}^t))
$$
$$
	\mathbf{z}^t = \mathbf{s}^t
$$
$$
	\mathbf{g}^{U,t} = \sigma (\mathbf{W}^{U,in} \mathbf{x}^t + \mathbf{W}^U \mathbf{z}^{t-1})
$$
$\mathbf{g}^{U,t}$ は忘却ゲートに変わるもので、 **更新(update)ゲート** と呼ばれる。これが1日かければ現在の状態は過去の状態 $\mathbf{s}^{t-1}(=\mathbf{z}^{t-1})$ をそのままコピーしたものとなり、逆に-に近ければ入力 $\mathbf{x}^t$ と過去の状態を混ぜたものになる。

二つ目が、 **ゲート付きRNN (GRU: gated recurrent unit)** である。これは UGRNN を拡張した形を持ち、**初期化 (reset) ゲート** と呼ぶゲートが追加されている。
UGRNN では更新ゲート $\mathbf{g}^{U,t}$ が0であっても、過去の状態 $\mathbf{z}^{t-1}$ が常に同じ割合 $(\mathbf{Wz}^{t-1})$ で現在に伝播するが、これを初期化ゲートを加えたもので置き換えることで更なる制御が可能に。
$$
	\mathbf{u}^t = \mathbf{W}^{in}\mathbf{x}^t + \mathbf{Wg}^{R,t} \odot \mathbf{z}^{t-1}
$$
$$
	\mathbf{s}^t = \mathbf{g}^{U,t} \odot \mathbf{s}^{t-1} + (\mathbf{1} - \mathbf{g}^{U,t} \odot f(\mathbf{u}^t))
$$
$$
\mathbf{z}^t = \mathbf{s}^t
$$
$$
	\mathbf{g}^{U,t} = \sigma (\mathbf{W}^{U,in} \mathbf{x}^t + \mathbf{W}^U \mathbf{z}^{t-1})
$$
$$
	\mathbf{g}^{R,t} = \sigma (\mathbf{W}^{R,in} \mathbf{x}^t + \mathbf{W}^R \mathbf{z}^{t-1})
$$

活性化関数 f は LSTM と同様 tanh である。
このほかにも +RNN と表記される **交差RNN (+RNN: intersection RNN)** などさまざまなものが考案されている。
どれを使うべきかといった議論は文献[138]にある。

---
## 6.4 自己回帰モデル

### 6.4.1 概要

文や音声信号などの系列データ $\mathbf{x}^1,\cdots,\mathbf{x}^t$ において時刻 $t$ の値 $\mathbf{x}^t$ は、過去の値 $\mathbf{x}^{t-1},\mathbf{x}^{t-2}$ と強い結びつきを持っている。その結びつきを
$$
	\mathbf{x}^t = \phi_0 + \sum_{i=t-\rho}^{t-1} \phi_i \mathbf{x}^i + \epsilon^t
$$
のように、 $\mathbf{x}^t$ を過去の値の線形和 ($\phi_i$  は定数)とランダムなノイズ $\epsilon^t$ の和として表すことでモデル化したものを （線形）**自己回帰モデル (auto-regressive model)** と呼ぶ。

この数式のような線形和では表現力はたかが知れているが、「過去の系列から $\mathbf{x}^t$ が決まる仕組み」 を RNN を使って表すことが出来る。

### 6.4.2 RNN による自己回帰モデル

RNN に単語の並びの関係性を学習させ、表現させたものを **言語モデル (language model)** と呼ぶ。

### 6.4.4 Seq2Seq

系列データを入力に受け取って系列データを出力する問題で、入力と出力の間で系列の長さが違う場合がある。(ex. 機械翻訳)
このような問題に RNN を適用する方法はいくつか知られているが、その1つに **系列変換 (Seq2Seq: sequence to sequence)** がある。
Seq2Seq は入力系列を受け取ってそこから特徴を取り出す部分と、それを元に出力系列を生成する部分からなり、それらはともに RNN のエンコーダ・デコーダである。

Seq2Seq は7.2.2項で説明する注意機構と組み合わせて用いることで、一層高い性能を得ることが出来る。

---
## 6.5 1次元畳み込みネットワーク


