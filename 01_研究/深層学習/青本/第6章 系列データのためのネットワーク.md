---
title:
AI: "false"
published:
created: " 2025-11-07"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 6

## 6.1 系列データ

系列データを扱う問題の例
1. 入力として1つの文が与えられ、それをいくつかのクラスに分類する問題
2. 発話を記録した時間信号からその発話内容を推定する **音声認識 (speech recognition)** である。
いずれの場合も、推定すべきものも系列データであり、その系列長は入力系列の長さとは異なることに注意する。

---
## 6.2 リカレントニューラルネットワーク

### 6.2.1 概要

**リカレントニューラルネットワーク (recurrent neural network)** とは、内部に（有向）閉路を持つニューラルネットワークの総称で、系列データを扱う。
- Elman ネットワーク
- Jordan ネットワーク
- 時間遅れネットワーク
- エコー状態ネットワーク
など様々なものがある。

ここでは、順伝播型ネットワークと同様の構造を持ち、ただし中間層のユニットの出力が自分自身に戻される「帰還路」を持つシンプルなものを考える。
→ この構造により、**情報を一時的に記憶し、また振る舞いを動的に変化させることが出来る。**

RNN は各時刻 $t$ につき1つの入力 $\mathbf{x}^t$ を受け取り、また同時に1つの出力 $\mathbf{y}^t$ を返す。つまり入力と同じ長さの系列を出力する。
**順伝播型ネットワークが入力1つに対し1つの出力を与える写像を表すのに対し、RNN は（少なくとも理論上）過去のすべての入力から1つの出力への写像を表す。**


### 6.2.2 順伝播の計算

系列 $\mathbf{x}^1, \mathbf{x}^2$ を RNN に順番に入力すると、対応する出力は系列 $\mathbf{y}^1, \mathbf{y}^2, \cdots$ を与える。

RNN の帰還路は、中間層の出力を自らの入力に戻すが、この間の結合は全ユニット間で存在する。
時刻 $t-1$ における中間層の任意のユニット $j'$ から時刻 $t$ における中間層の任意のユニット $j$ へ、重み $w_{jj'}$ の結合が存在する。
したがって、時刻 $t$ における中間層の各ユニットへの入力は、同時刻 $t$ にて入力層から届くものと、時刻 $t-1$ の中間層の出力をフィードバックしたものとの和
$$
	u_j^t = \sum_i w_{ji}^{(in)} x_i^t + \sum_{j'} w_{jj'} z_{j'}^{t-1}
$$
となる。中間層のバイアスは $w_{j0}^{(in)}$ ($x_0^t=1$ と固定) が与えられる (バイアスを使わないこともあるが)。
ここから中間層の出力は、活性化関数 $f$ を経由して、
$$
	z_j^t = f(u_j^t)
$$
と計算される。**RNN の活性化関数には伝統的に tanh が多く使われてきたが、ReLU も使う。**
以上をまとめると、
$$
	\mathbf{z}^t = \mathbf{f}(\mathbf{W}^{(in)} \mathbf{x}^t + \mathbf{W} \mathbf{z}^{(t-1)})
$$
のように書ける。

![[PXL_20251107_101739895.jpg]]

RNN の出力 $\mathbf{y}^t$ は次のように計算する。まず出力層への入力は、中間層の出力 $\mathbf{z}^t$ から、
$$
	v_k^t = \sum_j w_{kj}^{(out)} z_j^t
$$
と決まる。活性化関数を $\mathbf{f}^{\text{out}}$ と書くと、以上をまとめて、
$$
	\mathbf{y}^t = \mathbf{f}^{(\text{out})} = \mathbf{f}(\mathbf{w}^{(\text{out})} \mathbf{z}^t)
$$
と書ける。深層ニューラルネットワークでは3.6.3項のバッチ正規化に代表される層出力の正規化が採用されるが、RNN の場合にはインスタンス正規化が採用される。
**→ RNN の場合、同じ層を何度も信号が循環するので、バッチ内のサンプル集合を対象に統計量を計算することが意味をなさないため。**

### 6.2.3 問題への適用

RNN の出力層の設計は、順伝播型ネットワークと同じ。例えばクラス分類であれば、クラス数 ($C$ とする) と同数のユニットを並べ、ソフトマックス関数を活性化関数に選ぶ。
出力系列 $\mathbf{y}_1,\cdots,\mathbf{y}^T$  の目標値 $\mathbf{d}^1,\cdots,\mathbf{d}^T$  が与えられたとき、訓練サンプルを $n=1,\cdots,N$ で表し、サンプル $n$ の系列長を $T_n$ と書くと、損失関数は
$$
	E(\mathbf{w}) = - \sum_{n=1}^N \sum_{t=1}^{T_n} \sum_{k=1}^C d_{nk}^t \log y_k^t (\mathbf{x}_n^t;\mathbf{w})
$$

| データ名 | 記号    | 形状（実装ベース）                  | 意味                                       |
| ---- | ----- | -------------------------- | ---------------------------------------- |
| 入力系列 | $(X)$ | $([N, T, d_{\text{in}}])$  | 各時刻 (t) における入力特徴ベクトル（次元 (d_{\text{in}})） |
| 出力系列 | $(Y)$ | $([N, T, d_{\text{out}}])$ | 各時刻 (t) における出力ベクトル（次元 (d_{\text{out}})）  |

> [!NOTE] RNN と深層ニューラルネットの次元
> RNN ... 3次元テンソル $X \in \mathbb{R}^{N \times T \times d^{\text{in}}}$ 
> 深層ニューラルネット ... 2次元テンソル $X \in \mathbb{R}^{N \times d^{\text{in}}}$


一方、入力は系列データであるが、出力は系列ではなく単一の要素である場合もある。
会話の発生信号やテキストから感情を読み取る **感情分析 (sentiment analysis)** はその一例。この場合、例えば RNN の最後の時刻の出力のみを利用する。

---
## 6.3 ゲート機構

### 6.3.1 RNN と勾配消失問題

RNN は、過去の入力の履歴を最新の出力の計算に反映させることが出来る。
**→ 「どれだけ遠い過去の入力を出力に反映させることが出来るのか？」**

**理論上は、過去の入力全てが関わるはずだが、何も工夫しなければせいぜい過去10時刻分程度であると言われている。**
この限界は、順伝播型ネットワークの勾配消失問題と同様の理由で生じる。

基本的な RNN では、入力を短期的に記憶しておくことはできても、**長期にわたって記憶し、出力の計算に利用することは難しい**と言える。

### 6.3.2 長・短期記憶 (LSTM)

長期にわたる記憶を可能にすべく考案されたのが **長・短期記憶 (LSTM: Long Short-Term Memory)** である。
LSTM は基本的な RNN の中間層のユニットをメモリユニットと呼ぶ要素で置き換えた構造を持つ。入出力層の構造や問題への適用方法は基本的に同じ。

![[PXL_20251107_124805578.jpg]]

**メモリセル** ... 状態 $s_j^t$ を保持し、これを1時刻を隔ててメモリセル自身に帰還することで記憶を実現する。
帰還路には途中に忘却ゲートが挿入されており、ユニット f の出力がゲートの値 $g_j^{F,t}$ となる。メモリセルには、 $s_j^t$ に $g_j^{F,t}$ を掛けたものが伝えられ、それが1に近ければ現状態がそのまま記憶され、0に近ければリセットされる。

このような仕組みで、これまでに扱った **RNN の限界「短期間の記憶しか実現できない」の緩和を図る。**
単純な場合、忘却ゲートを1にし、入力ゲートを0にし続ければ、メモリセルの状態は永遠に記憶される。
タイミングよくこれらのゲートを開閉することで、長い文脈を捉えたより高度な推定が可能になる。

＜数式で書く＞
$j$ 番目のメモリユニット内部のメモリセルは変数 $s_j^t$ を保持する。メモリセルの帰還路は変数 $s_j^t$ の中身を1時刻分後に引き継ぐ。
$$
	s_j^t = g_j^{F,t} s_j^{t-1} + g_j^{I,t} f(u_j^t)
$$
第1項 ... 引き継いだ前時刻の状態に関する項
第2項 ... このメモリユニット $j$ が受け取る入力に関する項

第2項の $u_j^t$ について、
$$
	u_j^t = \sum_i w_{ji}^{in}x_i^t + \sum_j' w_{jj'} z_{j'}^{t-1}
$$
