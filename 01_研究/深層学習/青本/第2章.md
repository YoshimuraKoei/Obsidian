---
title: 深層学習_第2章
AI: "false"
published:
created: " 2025-10-31"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 2

## 2.1 ユニットと活性化関数

**ユニット** ... ニューラルネットワークを構成する最小の要素がユニット

ユニットは、複数の入力を受け取り、1つの入力を計算する。
例：
$$
	u = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + b
$$
各入力に異なる重みを掛けて加算したものにバイアスと呼ばれる値 $b$ を足す。

ユニットは、総入力 $u$ を**活性化関数 (activation function)** と呼ばれる関数 $f$ に入れて得られる値
$$
	z = f(u)
$$
を出力する。

複数のユニットを考え、入力とユニットの数を一般化し、 入力を $i = 1, \cdots, I$、ユニットを $j = 1, \cdots , J$ で表すｒと、各ユニット $j$ の出力は次のように計算される。

$$
	u_j = \sum_{i=1}^I w_{ji} x_i + b_j
$$
$$
	z_j = f(u_j)
$$

ベクトルと行列を用いて表記すると、

$$
	\mathbf{u} = \mathbf{Wx + b} 
$$
$$
	\mathbf{z} = \mathbf{f(u)}
$$
と表せる。

ユニットの総入力に適用される活性化関数にはさまざまな種類があり、目的に応じて使い分けられる。最も基本となるのが、**ReLU (rectifiled linear unit)** 。
$$
	f(u) = \max\{u, 0\}
$$

> [!NOTE] 典型的な活性化関数とその形
> このグラフ探した方が良いかも


ReLU は歴史的に新しく、それ以前は**シグモイド関数 (sigmoid function)** と総称される活性化関数の一群がメジャーな存在だった。

その一つが、**ロジスティック関数 (logistic function)** あるいは **ロジスティックシグモイド関数 (logistic sigmoid function)** と呼ばれる関数。
$$
	f(u) = \frac{1}{1 + e^{-u}}
$$
これは実数全体を定義域に持ち、 $(0, 1)$ を値域とする。

もう一つが、**双曲線正接関数 (hyperbolic tangent function)** 。
$$
	f(u) = \tanh(u) = \frac{e^u - e^{-u}}{e^u + e^{-u}}
$$
これは実数全体を定義域とし、$(-1, 1)$ を値域とする。

いずれのシグモイド関数も、
- **入力の絶対値が大きくなると出力が飽和し一定値となる**
- **その間の出力に対して出力が徐々にかつ滑らかに変化する**
ことが特徴。

ReLU で特筆すべきは**出力が変化する入力値の範囲がずっと広いこと**。

ReLU の拡張例：PReLU
$$
	f(u) = 
		\begin{cases}
			u, \quad if \; u \geq 0 \\
			au, \quad \text{otherwise}
		\end{cases}
$$
ここで、$a$ は学習によって定める。
他にも LeakyReLU、マックスアウト(maxout)、ELU (exponential linear unit)、トランスフォーマーでよく使われる **GELU (Gaussian error linear unit)** など多数の活性化関数が提案されている。

---
## 2.2 順伝播型ネットワーク

ネットワーク層を増やして、**入力層 (input layer) 、中間層 (internal layer) あるいは隠れ層 (hidden layer) 、出力層 (output layer)** の概念を導入。

2層目の中間層のユニットの出力

$$
	\mathbf{u}^{(2)} = \mathbf{W}^{(2)} \mathbf{x} + \mathbf{b}^{(2)}
$$
$$
	\mathbf{z}^{(2)} = \mathbf{f} (\mathbf{u}^{(2)})
$$

任意の層数 $L$ のネットワークに一般化して、

$$
	\mathbf{u}^{(l+1)} = \mathbf{W}^{(l+1)} \mathbf{z}^{(l)} + \mathbf{b}^{(l+1)}
$$
$$
	\mathbf{z}^{(l+1)} = \mathbf{f}(\mathbf{u}^{(l+1)})
$$
ネットワークの最終的な出力は、
$$
	\mathbf{y} \equiv \mathbf{z}^{(L)}
$$
と表記。
このように入力 $\mathbf{x}$ を皮切りに各層の計算を順番に実行し、最後に出力 $\mathbf{y}$ を得るネットワークのことを、 **順伝播型 (ニューラル) ネットワーク (feed-forward neural network)** という。

各層間の結合の重み $\mathbf{W}^{(l)} (l = 2,...,L)$ とユニットのバイアス $\mathbf{b}^{(l)}$ を**ネットワークのパラメータ**と呼ぶ。このことを明確にするため、 $\mathbf{y}(\mathbf{x}:\mathbf{w})$ と書く。
 
---
## 2.3 学習の概要


---
## 2.4 問題の定式化：出力層と損失関数の設計

### 2.4.1 回帰

### 2.4.2 2値分類

### 2.4.3 多クラス分類

活性化関数の一つ：**ソフトマックス関数 (softmax function)**
$$
	y_k \equiv z_k^{(l)} = \frac{\exp(u_k^{(L)})}{\sum_{j=1}^K \exp(u_j^{(L)})}
$$

- こうして決まる出力 $y_1, \cdots, y_K$ は、総和がいつも1になる $(\sum_{k=1}^K y_k = 1)$ 
- 他の活性化関数と異なり、ユニット $k$ の出力はこの層の全ユニットの総入力 $u_1, \cdots, u_K$ を元に決まる

入力 $\mathbf{x}_n$ に対するネトワークの目標出力 $\mathbf{d}_n$ を、 $K$個の要素を持つベクトル $\mathbf{d}_n = [d_{n1} \cdots d_{nK}] \top$ によって表現するとし、各成分 $d_{nk}$ は対応するクラスが真のクラスであったときのみ1をとり、それ以外は0を取るとする (1-of-K 符号化や 1-hot ベクトルなどと表現される)。

**交差エントロピー** と呼ばれる下記の損失関数が採用される。
$$
	E(\mathbf{w}) = - \sum_{n=1}^N \sum_{k=1}^K d_{nk} \log y_k (x_n ; \mathbf{w})
$$

### 2.4.4 マルチラベル分類

一つの入力 $\mathbf{x}$ に対し、複数のラベルの有無を予測する問題。この問題は、複数のラベル (属性) それぞれについて、その有無を予測する2値分類に帰着される１。
つまり、 $k(=1, \cdots, K)$ 番目のラベルの有無を2値の変数 $s_k \in \{ 0,1 \}$ で表すこととし、ネットワークは $s_1, \cdots, s_K$ を予測する $K$ 個の出力を与える $K$ 個のユニットを出力層に持つようにデザインする。
各ユニットの活性化関数は、ロジスティック関数を採用し、ユニット $k$ は $s_k=1$ である確率 $y_k(\mathbf{x}) = p(s_k = 1 | \mathbf{x})$ を出力するものと見なす。

訓練データの正解ラベル ($n$ 番目のサンプルの $k$ 番目のラベル) を $d_{nk} \in \{ 0, 1 \}$ で表すと、 $N$ 個のサンプルに対する損失関数は下記で定義される。
$$
	E(\mathbf{w}) = - \sum_{n=1}^N \sum_{k=1}^K [d_{nk} \log y_k(\mathbf{x}_n ; \mathbf{w}) + (1 - d_{nk}) \log \{ 1 - y_k(\mathbf{x}_n ; \mathbf{w}) \}]
$$
これは2値分類の損失関数に対し、ユニットが増えて総和を取っただけと見なせる。

### 2.4.5 順序回帰

**順序回帰 (ordinal regression)** とは、順序が決まった複数のクラス (ランク) があるとき、その中の一つを選んで答えとする問題。(ex. 5段階評価)
多クラス分類に似ているが、この問題では「惜しい」誤答、つまり正解と順序が近い誤答が存在する。通常の回帰は、予測と正解の「近さ」を直接的に扱う一方で、離散的なクラスラベルを扱うことが出来ない。
**この意味で順序回帰は、回帰と多クラス分類の中間に位置している。**

**順序回帰のための出力層と損失関数の設計方法は少なくとも2つある。**

1. クラス数を $K$ とするとき、問題を $K-1$ 個の独立な2値分類に変換する方法
2. a

＜1.について＞

$K$ 個の順序付きのクラスをインデックス $k=1, \cdots, K$ で表したとき、**それぞれのクラスについて入力 $\mathbf{x}$ の正解クラス $\bar{k}$ がどれより大きいか、そうでないかを判定する2値分類を考える。**
最後のクラス $K$ との比較は意味がないので、 $k=1,\cdots,K-1$ について2値分類を考える。その正解 $d_k(k=1,\cdots,K-1)$ は、
$$
	\begin{cases}
		d_k = 1 \quad k < \bar{k} \\
		d_k = 0 \quad \text{otherelse}
	\end{cases}
$$
となる。これに合わせてネットワークの入力層に $K-1$ 個のユニットを並べ、ロジスティック関数を活性化関数にして、その出力 $y_k$ で $d_k$ を予測する。
理想的には、
$$
	[y_1, \cdots, y_{K-1}] = [1, \cdots, 1, 0, \cdots, 0]
$$
のような形になる。が、このように並ぶ保証はないので、
$$
	(\text{予測するクラスのインデックス}) = 1 + \sum_{k=1}^{K-1} 1\{y_k\}
$$
とする。

＜2. について＞

### 2.4.6 信号の陰的表現

何らかの信号をニューラルネットワークを用いて表現することを主な目的とする、**陰的表現 (implicit neural representation)** と呼ばれる手法がある。
