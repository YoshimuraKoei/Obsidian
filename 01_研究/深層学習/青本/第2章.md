---
title: 深層学習_第2章
AI: "false"
published:
created: " 2025-10-31"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 2

## 2.1 ユニットと活性化関数

**ユニット** ... ニューラルネットワークを構成する最小の要素がユニット

ユニットは、複数の入力を受け取り、1つの入力を計算する。
例：
$$
	u = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + b
$$
各入力に異なる重みを掛けて加算したものにバイアスと呼ばれる値 $b$ を足す。

ユニットは、総入力 $u$ を**活性化関数 (activation function)** と呼ばれる関数 $f$ に入れて得られる値
$$
	z = f(u)
$$
を出力する。

複数のユニットを考え、入力とユニットの数を一般化し、 入力を $i = 1, \cdots, I$、ユニットを $j = 1, \cdots , J$ で表すｒと、各ユニット $j$ の出力は次のように計算される。

$$
	u_j = \sum_{i=1}^I w_{ji} x_i + b_j
$$
$$
	z_j = f(u_j)
$$

ベクトルと行列を用いて表記すると、

$$
	\mathbf{u} = \mathbf{Wx + b} 
$$
$$
	\mathbf{z} = \mathbf{f(u)}
$$
と表せる。

ユニットの総入力に適用される活性化関数にはさまざまな種類があり、目的に応じて使い分けられる。最も基本となるのが、**ReLU (rectifiled linear unit)** 。
$$
	f(u) = \max\{u, 0\}
$$

> [!NOTE] 典型的な活性化関数とその形
> このグラフ探した方が良いかも


ReLU は歴史的に新しく、それ以前は**シグモイド関数 (sigmoid function)** と総称される活性化関数の一群がメジャーな存在だった。

その一つが、**ロジスティック関数 (logistic function)** あるいは **ロジスティックシグモイド関数 (logistic sigmoid function)** と呼ばれる関数。
$$
	f(u) = \frac{1}{1 + e^{-u}}
$$
これは実数全体を定義域に持ち、 $(0, 1)$ を値域とする。

もう一つが、**双曲線正接関数 (hyperbolic tangent function)** 。
$$
	f(u) = \tanh(u) = \frac{e^u - e^{-u}}{e^u + e^{-u}}
$$
これは実数全体を定義域とし、$(-1, 1)$ を値域とする。

いずれのシグモイド関数も、
- **入力の絶対値が大きくなると出力が飽和し一定値となる**
- **その間の出力に対して出力が徐々にかつ滑らかに変化する**
ことが特徴。

ReLU で特筆すべきは**出力が変化する入力値の範囲がずっと広いこと**。

ReLU の拡張例：PReLU
$$
	f(u) = 
		\begin{cases}
			u, \quad if \; u \geq 0 \\
			au, \quad \text{otherwise}
		\end{cases}
$$
ここで、$a$ は学習によって定める。
他にも LeakyReLU、マックスアウト(maxout)、ELU (exponential linear unit)、トランスフォーマーでよく使われる **GELU (Gaussian error linear unit)** など多数の活性化関数が提案されている。

---
## 2.2 順伝播型ネットワーク

ネットワーク層を増やして、**入力層 (input layer) 、中間層 (internal layer) あるいは隠れ層 (hidden layer) 、出力層 (output layer)** の概念を導入。

2層目の中間層のユニットの出力

$$
	\mathbf{u}^{(2)} = \mathbf{W}^{(2)} \mathbf{x} + \mathbf{b}^{(2)}
$$
$$
	\mathbf{z}^{(2)} = \mathbf{f} (\mathbf{u}^{(2)})
$$

任意の層数 $L$ のネットワークに一般化して、

$$
	\mathbf{u}^{(l+1)} = \mathbf{W}^{(l+1)} \mathbf{z}^{(l)} + \mathbf{b}^{(l+1)}
$$
$$
	\mathbf{z}^{(l+1)} = \mathbf{f}(\mathbf{u}^{(l+1)})
$$
ネットワークの最終的な出力は、
$$
	\mathbf{y} \equiv \mathbf{z}^{(L)}
$$
と表記。
このように入力 $\mathbf{x}$ を皮切りに各層の計算を順番に実行し、最後に出力 $\mathbf{y}$ を得るネットワークのことを、 **順伝播型 (ニューラル) ネットワーク (feed-forward neural network)** という。

各層間の結合の重み $\mathbf{W}^{(l)} (l = 2,...,L)$ とユニットのバイアス $\mathbf{b}^{(l)}$ を**ネットワークのパラメータ**と呼ぶ。このことを明確にするため、 $\mathbf{y}(\mathbf{x}:\mathbf{w})$ と書く。
 
---
## 2.3