---
title:
AI: "false"
published:
created: " 2025-11-07"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 5

## 5.1 単純型細胞と複雑型細胞

**畳み込みニューラルネットワーク (convolutional neural network)** は、画像分類など画像を入力とするさまざまな問題に適用可能な、順伝播型ネットワークのこと。
CNN は、畳み込みという基本的な画像処理の演算を実行する **畳み込み層 (convolution layer)** を持つことが特徴。

これまでの章で扱ったネットワークは、隣接層のユニットすべてが **全結合 (fully-connected)** されたものだった。
畳み込み層では、隣接層間の特定のユニットのみが結合を持つ。

![[PXL_20251106_152603810 1.jpg]]

**LeNet** ... CNN と同様の構造を持つネットワークを用いて、学習を誤差逆伝播法 + 勾配降下法で行うようにしたもの。
LeNet は現在の CNN の直接のルーツであり、当時、文字認識に応用されて高い認識性能を挙げることが示された。

**CNN は、画像と画像に類似した構造を持つデータを扱う問題に対して最もよく使われている。**

---
## 5.2 畳み込み


### 5.2.1 定義

画像サイズを $W \times H$ 画素とし、画素をインデックス $(i, j) (i = 0,\cdots,W-1, j = 0, \cdots, H-1)$ で表す。
画素 $(i, j)$ の画素値を $x_{ij}$ と書き、 $x_{ij}$ は負の値を含む実数値を取るとする。

**フィルタ (filter)** と呼ぶサイズの小さい画像を考え、そのサイズを $W_f \times H_f$ 画素とする。
フィルタの画素をインデックス $(p, q) (p=0,\cdots,W_f-1, q=0,\cdots,H_f-1)$ で表し、画素値を $h_{pq}$ とする。

画像の畳み込みとは、画像とフィルタ間で定義される次の積和計算
$$
	u_{ij} = \sum_{p=0}^{W_f-1} \sum_{q=0}^{H_f-1} x_{i+p, j+q} h_{pq}
$$
だがこの計算は相関と呼ぶべきで、正確には、**畳み込み (convolution)** は本来
$$
	u_{ij} = \sum_{p=0}^{W_f-1} \sum_{q=0}^{H_f-1} x_{i-p, j-q} h_{pq}
$$
という計算のこと。

### 5.2.2 畳み込みの働き

画像の畳み込みには、フィルタの濃淡パターンと類似したパターンが入力画像のどこにあるかを検出する働きがある。

### 5.2.3 パディング

畳み込みは、画像にフィルタを重ねた時、画像とフィルタの重なり合う画素どうしの積を求めて、それらの和を求める計算。

画像からフィルタがはみ出すような位置に重ねることはできないので、畳み込み結果のサイズ ($u_{ij}$ のインデックスの範囲)  は入力画像より小さくなり、
$$
	(W - 2 \lfloor W_f/2 \rfloor) \times (H - 2 \lfloor H_f/2 \rfloor)
$$
となる。ただし、 $\lfloor \cdot \rfloor$ は小数点以下を切り下げて整数化する演算子を表す。
ex. 8×8 に 3×3 のフィルタを通す → 6×6

畳み込み結果の画像が入力画像と同じサイズになるようにしたい場合は、入力画像の外側に横方向 $\lfloor W_f/2 \rfloor$ 、縦方向 $\lfloor H_f/2 \rfloor$ の縁をつけて大きくする。
この処理のことを **パディング (padding)** と呼ぶ。この縁の部分の画素値は未定なので、何らかの方法で決める必要がある。

最も一般的なのは、画素値を0にする方法で、 **ゼロパディング (zero-padding)** と呼ばれる。

### 5.2.4 ストライド

画像上のフィルタの適用位置を1画素ではなく、数画素ずつ縦横方向にズラして計算することもできる。このようなフィルタの適用位置の間隔を **ストライド (stride)** と呼ぶ。
ストライドを $s$ とするとき、出力画像の画素値は
$$
	u_{ij} = \sum_{p=0}^{W_f-1} \sum_{q=0}^{H_f-1} x_{si+p, sj+q} h_{pq}
$$
のように計算されて、出力画像サイズは約 $1/s$ 倍となる。


## 5.3 畳み込み層

一般的な埋め込み層では、3次元の配列に対し、同じく3次元の配列となるフィルタを複数、並行して畳み込む演算を行う。
そして、畳み込み層の出力も3次元の配列となる。

この3次元配列は、同じサイズ $(W \times H)$ の画像 $C$ 枚を層状に重ねたものと考えることが出来る。この層のことを **チャネル $(C=1,\cdots,C)$**  と呼ぶ。
内部の畳み込み層では、一般にもっと多くのチャネル数 $(C=16やC=256)$ などを持つ3次元配列を扱う。

![[PXL_20251107_064334528.jpg]]

$$
	u_{ijk} = \sum_{c=0}^{C-1} \sum_{p=0}^{W_f-1} \sum_{q=0}^{H_f-1} z_{i+p, j+q, c}^{(l-1)} h_{pqck} + b_k
$$

| 記号         | 形状                                                    | 内容            |
| ---------- | ----------------------------------------------------- | ------------- |
| 入力         | $( X \in \mathbb{R}^{H \times W \times C} )$          | 画像（縦×横×チャネル）  |
| フィルタ（カーネル） | $( W^{(k)} \in \mathbb{R}^{H_f \times W_f \times C})$ | 各出力チャネル用のフィルタ |
| 出力         | $( U \in \mathbb{R}^{H \times W \times C_{out}} )$    | 特徴マップ群        |

1つのフィルタの適用結果は出力の1チャネルを与えるため、出力のチャネル数はフィルタ数に一致する $(C_{\text{out}})$

上記のように得られた $u_{ijk}$ には一般に活性化関数が適用され、
$$
	z_{ijk} = f(u_{ijk})
$$

---
## 5.4 プーリング層

プーリング層は、**プーリングと呼ばれる演算を行う1つの独立した層**のこと。
プーリング層の各ユニットは、複雑型細胞の簡素なモデルで、畳み込み層の後ろにプーリング層を配置することで、画像内の特徴量の位置変化に対する不変性を生み出す。
従来はプーリング層は CNN に不可欠な存在だったが、ストライド $s>1$ の畳み込みで代用されることが多くなっている。

### 5.4.1 プーリングの役割

2つの役割
- 入力の各位置で、その局所領域内の値を要約する
- 入力の空間解像度を下げる **ダウンサンプリング (downsampling)** の実行

### 5.4.2 いろいろなプーリング

画素 $(i,j)$ を中心とする $W_f \times H_f$ の領域をとり、この中に含まれる画素の集合を $P_{ij}$ で表す。
この $P_{ij}$ 内の画素について、チャネル $c$ ごとに独立に、 $W_f H_f$ 個の要素値から1つの要素値 $u_{ijc}$ を求める。

**①：最大プーリング：**
$$
	u_{ijc} = \underset{(p,q) \in P_{ij}} \max z_{pqc}
$$
**②：平均プーリング：**
$$
	u_{ijc} = \frac{1}{W_f H_f} \sum_{(p,q) \in P_{ij}} z_{pqc}
$$

プーリングの計算は入力画像の各チャネルで独立に行われる。

**③：①②を共に含んだ $L_p$ プーリング**
$$
	u_{ijc} = \left( \frac{1}{W_f H_f} \sum_{(p,q) \in P_{ij}} z_{pqc}^P \right)^{\frac{1}{P}}
$$
$P=1$ で平均プーリングを、 $P=\infty$ で最大プーリングを表現できる。

プーリング層でも畳み込み層と同じくストライドを設定でき、解像度を下げるために $s>1$ のストライドを設定するのが普通。

入力サイズ $W \times H$ とプーリングの対象領域のサイズ $W_f \times H_f$ を一致させた平均プーリングを **大域平均プーリング (global average pooling)** と呼ぶ。
入力のチャネル数を $C$ とすると、出力のサイズは $1 \times 1 \times C$ となる。クラス分類を行う CNN の出力層付近で良く用いられる。

プーリング層では、通常活性化関数は適用しない ($z_{ijc} = u_{ijc}$)
結合重みについても畳み込み層のフィルタのように調節可能なものでもなく、固定されている。
→ **プーリング層には学習によって変化するパラメータはない。**

---
## 5.5 畳み込み層の入力の正規化

### 5.5.1 バッチ正規化

畳み込み層の場合、入力のすべての空間位置で同一のフィルタを適用するため、その結果である出力のチャネルごとに独立に正規化を行う。
ただし、すべての空間位置で共通の正規化である。

---
## 5.6 推論のための CNN 構造

### 5.6.1 基本的な構造

画像を入力に受け取り、クラス分類や回帰などの推論を行う CNN は、**一般に多数の畳み込み層を積み重ね、数層のプーリング層をその途中にはさむ構造を持つ。**
(より最近の CNN では、プーリング層を置かずにストライドを2以上に設定した畳み込み層をプーリング層の代わりとすることが多くなっているが)

