---
title:
AI: "false"
published:
created: " 2025-11-06"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 4

## 4.1 勾配計算の煩わしさ


## 4.2 誤差逆伝播法

https://zenn.dev/yuto_mo/articles/b56e1fc2215c8b

$$
	\frac{\partial E_n}{\partial w_{ji}^{(l)}} = \frac{\partial E_n}{\partial u_{j}^{(l)}} \frac{\partial u_l^{(l)}}{\partial w_{ji}^{(l)}}
$$
$$
	\frac{\partial E_n}{\partial u_j^{(l)}} = \sum_{k} \frac{\partial E_n}{\partial u_k^{(l+1)}} \frac{\partial u_k^{(l+1)}}{\partial u_j^{(l)}}
$$
$$
	\delta_j^{(l)} \equiv \frac{\partial E_n}{\partial u_j^{(l)}}
$$
$$
	\delta_j^{(l)} = \sum_{k} \delta_k^{(l+1)} (w_{kj}^{(l+1)} f'(u_j^{(l)}))
$$

したがって、**上位の $l+1$ 層のユニットのデルタが与えられれば、 $l$ 層のデルタは計算できる**ということ。(デルタは各層 $l$ の各ユニット $j$ に対して定義されることに注意)
つまり、最初に出力層の各ユニットのデルタが求まっていれば、任意の層のデルタが求まるということ。
このとき、デルタは出力層から入力層の向き、つまり順伝播と逆の向きに伝播される。これが誤差逆伝播法の名前の由来。

よって、目的の微分は

$$
	\frac{\partial E_n}{\partial w_{ji}^{(l)}} = \sigma_j^{(l)} z_i^{(l-1)}
$$
のように表される。
**すなわち、第 $l-1$ 層のユニット $i$ と第 $l$ 層のユニット $j$ をつなぐ結合重み $w_{ji}^{(l)}$ に関する微分は、ユニット $j$ に関する $\delta_j^{(l)}$ と、ユニット $i$ からの出力 $z_i^{(l-1)}$ のただの積で与えられる。**



> [!NOTE] 図4.4 誤差逆伝播による勾配 (損失の重みによる微分) の計算手順
> 入力：訓練サンプル $\mathbf{x}_n$ および目標出力 $\mathbf{d}_n$ のペア1つ
> 出力：損失関数 $E_n(\mathbf{w})$ の各層 $l$ のパラメータについての微分 $\partial E_n / \partial w_{ji}^{(l)}, \; (l=2,\cdots,L)$
> 1. $\mathbf{z}^{(1)} = \mathbf{x}_n$ とし、各層 $l$ のユニット入出力 $\mathbf{u}^{(l)}$ および $\mathbf{z}^{(l)}$ を順に計算する (**順伝播**)
> 2. 出力層でのデルタ $\delta_j^{(L)}$ を求める (通常は $\delta_j^{(L)} = z_j - d_j$ となる.)
> 3. 中間層 $l (=L-1, L-2,\cdots,2)$ での $\delta_j^{(l)}$ を、この順に計算する (**逆伝播**)
> 4. 各層 $l (=2,\cdots,L)$ のパラメータ $w_{ji}^{(l)}$ に関する微分を計算する

ミニバッチなどの複数の訓練サンプルに対する損失の総和 $E = \sum_n E_n$ の勾配は、上記の手順を訓練サンプル $(\mathbf{x}_n, \mathbf{d}_n)$ ごとに並行に繰り返し、得られる勾配の和を
$$
	\frac{\partial E}{\partial w_{ji}^{(l)}} = \sum_n \frac{\partial E_n}{\partial w_{ji}^{(l)}}
$$
として求める。


---
## 4.3 自動微分

**自動微分 (automatic differentiation)** とは、ある関数 $\mathbf{y} = \mathbf{f}(\mathbf{x})$ を計算する手順を元に、この関数の微分 $d \mathbf{f} / d \mathbf{x}$ を計算する手順を自動的に生成する方法。
誤差逆伝播法をニューラルネットワーク以外に一般化したものと言える。

**誤差逆伝播法**
→ ネットワークの構造が表す計算手順に対し、出力の重みに関する微分を自動的に計算するアルゴリズム
手順例：
$$
	z = wx + b
$$
$$
	y = \sigma (z)
$$
$$
	E = \frac{1}{2} (y - d)^2
$$
この手順を、**有向非巡回グラフ (directed acyclic graph)** で表す。その際、計算手順を必要に応じて分割し、「**ただちに微分が計算可能**」な原始的な (primitive) 演算の列で表す。
$$
	t_1 = wx, \quad z = t_1 + b, \quad y = \sigma(z), \quad t_2 = y - d, \quad t_3 = t^2, \quad E = t_3/2
$$


![[PXL_20251106_105510287.jpg]]


---
## 4.4 勾配消失問題

順伝播と逆伝播の計算はいずれも層ごとの行列計算であり、互いによく似ている。違うのは、**順伝播は非線形計算であるのに対し、逆伝播は線形計算であ**ること。
順伝播の計算では、各層の出力は活性化関数の出力であり、その非線形性が層の入出力関係全体を非線形なものにする。
一方、デルタの逆伝播計算は出力層から入力層に至るまで線形。
→ 各層の重みが大きいと勾配は各層を伝播するうちに急速に大きくなり発散し、また逆に重みが小さいと急速に消失し 0 になってしまう。
この問題を、 **勾配消失問題 (vanishing gradient problem)** という。

今では、
- ReLUを筆頭とする新しい活性化関数
- バッチ正規化
- 重みの初期化
- 残差接続
等の方法を組み合わせることで、この問題を避けて学習が可能になっている。

---
## 4.5 残差接続

**残差接続 (residual connection)** とは、1つあるいは複数の層を迂回する近道を設け、そこを通る信号を、迂回せず伝播してきた信号に加算することで合流させる構造のこと。
迂回路のことを **スキップ接続 (skip connection)** と呼ぶ。スキップ接続の直前から合流後のひとまとまりを **残差ブロック (residual block)** と呼ぶ。

残差接続を持つネットワークでは、多層であっても学習の難しさが大きく緩和される！
→ ReLU やバッチ正規化と並ぶ、深層ニューラルネットワークの要である。

**残差接続の効用の解釈**
- 上の層からの勾配情報を下の層に確実に伝えてくれる
- 層の中で出力が恒等的にゼロとなるものが存在しても、スキップ接続が信号を上に伝えるのでネットワーク全体に影響しない
