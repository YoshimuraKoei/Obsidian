---
title:
AI: "false"
published:
created: " 2025-11-06"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 4

## 4.1 勾配計算の煩わしさ


## 4.2 誤差逆伝播法

https://zenn.dev/yuto_mo/articles/b56e1fc2215c8b

$$
	\frac{\partial E_n}{\partial w_{ji}^{(l)}} = \frac{\partial E_n}{\partial u_{j}^{(l)}} \frac{\partial u_l^{(l)}}{\partial w_{ji}^{(l)}}
$$
$$
	\frac{\partial E_n}{\partial u_j^{(l)}} = \sum_{k} \frac{\partial E_n}{\partial u_k^{(l+1)}} \frac{\partial u_k^{(l+1)}}{\partial u_j^{(l)}}
$$
$$
	\delta_j^{(l)} \equiv \frac{\partial E_n}{\partial u_j^{(l)}}
$$
$$
	\delta_j^{(l)} = \sum_{k} \delta_k^{(l+1)} (w_{kj}^{(l+1)} f'(u_j^{(l)}))
$$

したがって、**上位の $l+1$ 層のユニットのデルタが与えられれば、 $l$ 層のデルタは計算できる**ということ。(デルタは各層 $l$ の各ユニット $j$ に対して定義されることに注意)
つまり、最初に出力層の各ユニットのデルタが求まっていれば、任意の層のデルタが求まるということ。
このとき、デルタは出力層から入力層の向き、つまり順伝播と逆の向きに伝播される。これが誤差逆伝播法の名前の由来。

よって、目的の微分は

$$
	\frac{\partial E_n}{\partial w_{ji}^{(l)}} = \sigma_j^{(l)} z_i^{(l-1)}
$$
のように表される。
**すなわち、第 $l-1$ 層のユニット $i$ と第 $l$ 層のユニット $j$ をつなぐ結合重み $w_{ji}^{(l)}$ に関する微分は、ユニット $j$ に関する $\delta_j^{(l)}$ と、ユニット $i$ からの出力 $z_i^{(l-1)}$ のただの積で与えられる。**



> [!NOTE] 図4.4 誤差逆伝播による勾配 (損失の重みによる微分) の計算手順
> 入力：訓練サンプル $\mathbf{x}_n$ および目標出力 $\mathbf{d}_n$ のペア1つ
> 出力：損失関数 $E_n(\mathbf{w})$ の各層 $l$ のパラメータについての微分 $\partial E_n / \partial w_{ji}^{(l)}, \; (l=2,\cdots,L)$
> 1. $\mathbf{z}^{(1)} = \mathbf{x}_n$ とし、各層 $l$ のユニット入出力 $\mathbf{u}^{(l)}$ および $\mathbf{z}^{(l)}$ を順に計算する (**順伝播**)
> 2. 出力層でのデルタ $\delta_j^{(L)}$ を求める (通常は $\delta_j^{(L)} = z_j - d_j$ となる.)
> 3. 中間層 $l (=L-1, L-2,\cdots,2)$ での $\delta_j^{(l)}$ を、この順に計算する (**逆伝播**)
> 4. 各層 $l (=2,\cdots,L)$ のパラメータ $w_{ji}^{(l)}$ に関する微分を計算する

ミニバッチなどの複数の訓練サンプルに対する損失の総和 $E = \sum_n E_n$ の勾配は、上記の手順を訓練サンプル $(\mathbf{x}_n, \mathbf{d}_n)$ ごとに並行に繰り返し、得られる勾配の和を
$$
	\frac{\partial E}{\partial w_{ji}^{(l)}} = \sum_n \frac{\partial E_n}{\partial w_{ji}^{(l)}}
$$
として求める。