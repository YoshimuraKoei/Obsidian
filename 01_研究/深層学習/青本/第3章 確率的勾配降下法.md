---
title:
AI: "false"
published:
created: " 2025-11-02"
description:
tags:
  - permanent-note
  - knowledge
  - sgd
---

# Chapter 3

## 3.1 確率的勾配降下法 (SGD)

### 3.1.1 勾配降下法の基礎

訓練データ $\mathcal{D}$ に対して計算される損失関数 $E(\mathbf{w})$ を $\mathbf{w}$ について最小化することが順伝播型ネットワークの学習である。
可能ながば大域的最小点 $\mathbf{w} = \arg \min_\mathbf{w} E(\mathbf{w})$ を求めたいところだが、これの複雑さからそれは一般的に不可能で、局所的な極小点 $\mathbf{w}$ を得ることしかできない。
それでもその極小点での損失の値がある程度小さければ、目的とする推論を上手く実行できる可能性がある。

非線形関数の最小化のための方法は多くあるが、ニューラルネットワークの学習では **勾配降下法 (gradient descent method)**  を用いる。
**勾配 (gradient)** は損失関数の1次微分であり、
$$
	\nabla E \equiv \frac{\partial E}{\partial \mathbf{w}} = [\frac{\partial E}{\partial w_1}m, \cdots, \frac{\partial E}{\partial w_M}]^\top
$$

というベクトルになる。 $\mathbf{w}$ の更新については、
$$
	\mathbf{w}_{t+1} = \mathbf{w} - \epsilon \nabla E
$$
である。ここで、 $\epsilon$ は $\mathbf{w}$ の更新量の大きさを定める定数で、**学習率 (learning rate)** と呼ばれる。


> [!NOTE] 学習率
> - 小さすぎると
> 	- 極小点に至る反復回数が増える
> - 大きすぎると
> 	- $E(\mathbf{w})$ の形状によって値が増大してしまうことがある
> 
> → 学習率 $\epsilon$ をどう選ぶかはかなり重要で、学習の成否を左右する。


### 3.1.2 バッチ学習から SGD へ

訓練データ $\mathcal{D}$ のすべてのサンプルに対する損失関数 $E(\mathbf{w})$ を考えた。
$$
	E(\mathbf{w}) = \sum_{n=1}^N E_n(\mathbf{w})
$$
この $E(\mathbf{w})$ の勾配を用いて $\mathbf{w}$ を更新する手法を **バッチ学習** と呼ぶ。


> [!NOTE] バッチ学習
> すべてのデータを一度に使ってモデルを更新する方法で、学習時に「データ全部が手元にある」前提。
> - 特徴
> 	- 精度が安定している
> 	- だが計算コストが重い
> 	- 大量データだと一回の更新に時間がかかる

これに対し、サンプル1つの損失 $E_n$ を使ってパラメータを更新する方法を **確率的勾配降下法 (stochastic gradient descent)** と呼ぶ。
1つのサンプル $n$ について計算される損失 $E_n(\mathbf{w})$ の勾配 $\nabla E_n$ を計算し、
$$
	\mathbf{w}_{t+1} = \mathbf{w} - \epsilon \nabla E_n
$$
のように $\mathbf{w}$ を更新していく。


> [!NOTE] 確率的勾配降下法
> 利点
> - 反復計算が望まない局所的な極小解にトラップされてしまうリスクを低減できる
> 	- $\mathbf{w}$ の更新のたびに異なる目的関数 $E_n(\mathbf{w})$ を考える
> 
> → 反復のたびにランダムにサンプルを選ぶことで、この効果を最大化できると考えられており、「確率的」という表現はここに由来。


### 3.1.3 ミニバッチの利用




