---
title:
AI: "false"
published:
created: " 2025-11-02"
description:
tags:
  - permanent-note
  - knowledge
  - sgd
---

# Chapter 3

## 3.1 確率的勾配降下法 (SGD)

訓練データ $\mathcal{D}$ に対して計算される損失関数 $E(\mathbf{w})$ を $\mathbf{w}$ について最小化することが順伝播型ネットワークの学習である。
可能ながば大域的最小点 $\mathbf{w} = \arg \min_\mathbf{w} E(\mathbf{w})$ を求めたいところだが、これの複雑さからそれは一般的に不可能で、局所的な極小点 $\mathbf{w}$ を得ることしかできない。
それでもその極小点での損失の値がある程度小さければ、目的とする推論を上手く実行できる可能性がある。

非線形関数の最小化のための方法は多くあるが、ニューラルネットワークの学習では **勾配降下法 (gradient descent method)**  を用いる。
**勾配 (gradient)** は損失関数の1次微分であり、
$$
	\nabla E \equiv \frac{\partial E}{\partial \mathbf{w}} = [\far]
$$