---
title:
AI: "false"
published:
created: " 2025-11-02"
description:
tags:
  - permanent-note
  - knowledge
  - sgd
---

# Chapter 3

## 3.1 確率的勾配降下法 (SGD)

### 3.1.1 勾配降下法の基礎

訓練データ $\mathcal{D}$ に対して計算される損失関数 $E(\mathbf{w})$ を $\mathbf{w}$ について最小化することが順伝播型ネットワークの学習である。
可能ながば大域的最小点 $\mathbf{w} = \arg \min_\mathbf{w} E(\mathbf{w})$ を求めたいところだが、これの複雑さからそれは一般的に不可能で、局所的な極小点 $\mathbf{w}$ を得ることしかできない。
それでもその極小点での損失の値がある程度小さければ、目的とする推論を上手く実行できる可能性がある。

非線形関数の最小化のための方法は多くあるが、ニューラルネットワークの学習では **勾配降下法 (gradient descent method)**  を用いる。
**勾配 (gradient)** は損失関数の1次微分であり、
$$
	\nabla E \equiv \frac{\partial E}{\partial \mathbf{w}} = [\frac{\partial E}{\partial w_1}m, \cdots, \frac{\partial E}{\partial w_M}]^\top
$$

というベクトルになる。 $\mathbf{w}$ の更新については、
$$
	\mathbf{w}_{t+1} = \mathbf{w} - \epsilon \nabla E
$$
である。ここで、 $\epsilon$ は $\mathbf{w}$ の更新量の大きさを定める定数で、**学習率 (learning rate)** と呼ばれる。


> [!NOTE] 学習率
> - 小さすぎると
> 	- 極小点に至る反復回数が増える
> - 大きすぎると
> 	- $E(\mathbf{w})$ の形状によって値が増大してしまうことがある
> 
> → 学習率 $\epsilon$ をどう選ぶかはかなり重要で、学習の成否を左右する。


### 3.1.2 バッチ学習から SGD へ

訓練データ $\mathcal{D}$ のすべてのサンプルに対する損失関数 $E(\mathbf{w})$ を考えた。
$$
	E(\mathbf{w}) = \sum_{n=1}^N E_n(\mathbf{w})
$$
この $E(\mathbf{w})$ の勾配を用いて $\mathbf{w}$ を更新する手法を **バッチ学習** と呼ぶ。


> [!NOTE] バッチ学習
> すべてのデータを一度に使ってモデルを更新する方法で、学習時に「データ全部が手元にある」前提。
> - 特徴
> 	- 精度が安定している
> 	- だが計算コストが重い
> 	- 大量データだと一回の更新に時間がかかる

これに対し、サンプル1つの損失 $E_n$ を使ってパラメータを更新する方法を **確率的勾配降下法 (stochastic gradient descent)** と呼ぶ。
1つのサンプル $n$ について計算される損失 $E_n(\mathbf{w})$ の勾配 $\nabla E_n$ を計算し、
$$
	\mathbf{w}_{t+1} = \mathbf{w} - \epsilon \nabla E_n
$$
のように $\mathbf{w}$ を更新していく。


> [!NOTE] 確率的勾配降下法
> 利点
> - 反復計算が望まない局所的な極小解にトラップされてしまうリスクを低減できる
> 	- $\mathbf{w}$ の更新のたびに異なる目的関数 $E_n(\mathbf{w})$ を考える
> 
> → 反復のたびにランダムにサンプルを選ぶことで、この効果を最大化できると考えられており、「確率的」という表現はここに由来。


### 3.1.3 ミニバッチの利用


> [!NOTE] ミニバッチ学習
> バッチサイズのサンプルで損失を計算し、重みを1回更新する。そして、他のミニバッチで損失を計算し、重みを再び更新。
> 1エポックの場合、$\text{更新回数} = \text{バッチ数}$
> Eエポックの場合、$\text{更新回数} = \text{バッチ数} × E$
> 


ニューラルネットワークの学習には大きな計算コストがかかり、効率化には並列計算資源の利用が不可欠。
**サンプル1つごとに重みの更新を行うのでは計算の並列化が難しい** ので、一定数のサンプルの集合単位で重みを更新するのが現実的な選択になる。
このサンプルの集合のことを**ミニバッチ (minibatch)** と呼ぶ。

|手法|更新単位|勾配の安定性|局所解からの脱出性|並列化(GPU適性)|メリット|デメリット|典型用途|
|---|---|---|---|---|---|---|---|
|**バッチ学習 (Full Batch)**|全データ|**◎**（非常に滑らか）|**×**（揺らぎがなくハマりやすい）|**◎**（行列演算で最大効率）|・収束先が安定・理論解析しやすい|・計算が重い・大規模データで非現実的|小〜中規模データでの厳密最適化|
|**SGD (Stochastic, 1サンプル)**|1サンプル|**×**（非常にノイジー）|**◎**（揺らぎで抜けやすい）|**×**（並列化ほぼ不可）|・即時更新でオンライン学習に強い|・精度がブレやすい・GPUが全く生きない|ストリーム処理・リアルタイム学習|
|**SGD (Mini-Batch)**|少数サンプル (例: 32〜1024)|**○**（ほどよく安定）|**○**（適度な揺らぎ）|**◎**（GPU最適）|・収束の安定性と探索性のバランス最良・実装が標準化されている|・バッチサイズの調整は必要|**ディープラーニングでの事実上の標準**|

