---
title:
AI: "false"
published:
created: " 2025-11-08"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 7

## 7.1 集合データを扱うネットワーク

新たに入力が集合 $\mathbf{X} = \{ \mathbf{x}_1, \cdots, \mathbf{x}_N \}$ となる場合を考える。ただし、 $\mathbf{x}_i \in \mathbb{R}^D \; (i=1,\cdots,N)$ とする。
このような条件で、 $\mathbf{X}$ を入力に受け取り、1つのベクトル $\mathbf{y}/ \in \mathbb{R}^K$ を出力するネットワーク
$$
	\mathbf{y} = \mathbf{f}(\mathbf{X};\mathbf{w})
$$
を考える。**集合として表されるデータは、その要素に順序がないことが特徴** → $\mathbf{X}$ の要素の **並び替え (permutation)** が結果に影響しないことが求められる。
これに対する1つの答えは、集合の各要素の情報を平等に統合するプーリングを実行すること。例として、
$$
	\mathbf{y} = \mathbf{f(X)} = \mathbf{f'}\left( \sum_{i=1}^N \mathbf{g'}(\mathbf{x}_i) \right)
$$
この方法は PointNet と DeepSets で採用されている。これに加え、入力 $\mathbf{X}$ から別の集合 $\mathbf{Y} = \{ \mathbf{y}_1, \cdots, \mathbf{y}_N \}$ を出力する問題を考えることもある。
$$
	\mathbf{Y} = \mathbf{g(X;w)}
$$
トランスフォーマーは元々は集合データへの適用を想定して作られておらず、 $\mathbf{X}$ の要素数が大きくなると効率が悪化する。
そのたまえ、集合データ向けの改良を施した **集合トランスフォーマー (set transformer)** が考案されている。

---
## 7.2 注意機構

### 7.2.1 基本的な考え方

**注意機構 (attention mechanism)** はニューラルネットワークの構造要素の1つ。深層学習の中核をなす技術の1つに位置づけられる。
**注意 (attention)** とは、複数の要素からなる集合に対し、今持っている関心に応じた重要度に従って、各要素を重みづけすることを指す。

画像を小領域に分割し、それぞれをインデックス $i(=1,\cdots,N)$ で表したとき、各小領域 $i$ から画像特徴 $\mathbf{z}_i \in \mathbb{R}^D$ を取り出すとする。また、1つの問いをベクトル $\mathbf{q} \in \mathbb{R}^D$  (クエリ) で表現できるとする。
このとき、各領域の画像特徴 $\mathbf{z}_i$ と、問い $\mathbf{q}$ との関連性を、ある関数 $r$ によって
$$
	r_i = r(\mathbf{z}_i, \mathbf{q})
$$
と計算する。そして、各領域に対して得られた $r_1, \cdots, r_N$ を総和が1になるようにソフトマックス関数で正規化する。
$$
	a_i = \text{softmax}_i (r_1,\cdots,r_N) = \frac{\exp (r_i)}{\sum_{l=1}^N \exp (r_l)}
$$
この $a_i$ は、領域 $i$ (の特徴 $\mathbf{z}_i$ ) と $\mathbf{q}$ との関連性の高さを表すと言える。そこで、 $a_i$ を重みとする特徴ベクトル $\mathbf{z}_i$ の加重平均
$$
	\mathbf{z} = \sum_{i=1}^N a_i\mathbf{z}_i
$$
を求める。この $\mathbf{z}$ は、$\mathbf{q}$ と関連の深い画像の中身を、1つのベクトルでコンパクトに表現したものと考えることが出来る。
上記の計算において、
- 特徴の集合 $\{ \mathbf{z}_1,\cdots,\mathbf{z}_N \}$ ： **ソース(source)**
- $\mathbf{q}$ ：**ターゲット (target)**
- 上記3つの式計算：**ソース・ターゲット注意 (source-to-target attention)**
と呼ぶ。また、ソースとターゲットを同じくする場合があり、そのときは **自己注意 (self-attention)** と呼ぶ。

### 7.2.2 Seq2Seq と注意

- エンコーダのRNN
	- $\mathbf{x}_1,\cdots,\mathbf{x}_N$ から内部状態の系列 $\mathbf{z}_1,\cdots,\mathbf{z}_N$ を計算
	- → $i=1,\cdots,N$ の順に $\mathbf{z}_i = \mathbf{f} (\mathbf{x}_i, \mathbf{z}_{i-1})$ を実行
- デコーダのRNN
	- 最初 ($j=1$) の内部状態 $\mathbf{h}_1$ をエンコーダが出力する最後の内部状態で初期化 ($\mathbf{h}_1 \leftarrow \mathbf{z}_N$) した後、自己回帰によって $\mathbf{y}_1,\cdots,\mathbf{y}_M$ を計算する。
	- $j=1,\cdots,M$ の順に $\mathbf{h}_j = \mathbf{g} (\mathbf{y}_{j-1}, \mathbf{h}_{j-1})$  を実行する

上記の基本的な Seq2Seq では、**デコーダはエンコーダの最後の内部状態 $\mathbf{z}_N$ だけしか入力に取り込めない**。
そこで、デコータの計算の各ステップ $j$ で、内部状態の系列 $\mathbf{z}_1,\cdots,\mathbf{z}_N$ を要約したベクトル $\mathbf{c}_j$ (文脈ベクトル) を作り、
$$
	\mathbf{h}_j = \mathbf{g}(\mathbf{y}_{j-1}, \mathbf{h}_{j-1}, \mathbf{c}_j)
$$
と内部状態を計算する拡張を行う。

この $\mathbf{c}_j$ をソース・ターゲット注意の考え方で計算する。
ソースをエンコーダの内部状態 $\mathbf{z}_1, \cdots,\mathbf{z}_N$ とし、ターゲットをデコータの最新の内部状態 $\mathbf{h}_{j-1}$ とする。そして、 $\{ \mathbf{z}_i \}$ の要素の加重線形和
$$
	\mathbf{c}_j = \sum_{i=1}^N a_{ij} \mathbf{z}_i
$$
によって作り、先ほどの式で用いる。ここで、 $a_{ij}$ は、
$$
	e_{ij} = r(\mathbf{z}_i, \mathbf{h}_{j-1})
$$
$$
	a_{ij} = \text{softmax}_i (e_{1j}, \cdots, e_{ij})
$$
と正規化して得る。これは出力系列の $j$ ステップ目の計算における $\mathbf{z}_i$ の重要度を表すと考えることが出来る。

### 7.2.3 関連性の計算

ソース $\mathbf{z_1}, \cdots, \mathbf{z}_N$ に対するターゲット $\mathbf{q}$ 間からの注意の計算では、要素間の関連性 $r(\mathbf{z}_i, \mathbf{q})$ をどのように計算するかが重要。
$r$ にはさまざまな関数を用いることが出来るが、最も一般的なのは $\mathbf{z}$ と $\mathbf{q}$ の内積
$$
	r(\mathbf{z},\mathbf{q}) = \frac{\mathbf{z}^\top \mathbf{q}}{\sqrt{D}}
$$
である。**$\mathbf{z}$ および $\mathbf{q}$ の次元数の平方根 $\sqrt{D}$ で割るのは、ソフトマックスの出力が0と1に二極化する傾向を緩和するため。**
個の内積に重み $\mathbf{W} \in \mathbb{R}^{D \times D}$ を挿入し、学習可能なパラメータを導入した
$$
	r(\mathbf{z}, \mathbf{q}) = \frac{\mathbf{z}^\top \mathbf{W} \mathbf{q}}{\sqrt D}
$$
が良く使われている **(乗算的注意：multiplicative attention)**。このときの $\mathbf{W}$ はネットワークの他の重みと一緒に学習で決定する。
他には、1層以上の順伝播型ネットワークを用いた **加算的注意 (additive attention)**
$$
	r(\mathbf{z}, \mathbf{q}) = \text{ReLU} \left( \mathbf{w}^\top \begin{bmatrix} \mathbf{z} \\ \mathbf{q} \end{bmatrix} \right)
$$
