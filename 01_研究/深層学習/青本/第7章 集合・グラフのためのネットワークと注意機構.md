---
title:
AI: "false"
published:
created: " 2025-11-08"
description:
tags:
  - permanent-note
  - knowledge
---

# Chapter 7

## 7.1 集合データを扱うネットワーク

新たに入力が集合 $\mathbf{X} = \{ \mathbf{x}_1, \cdots, \mathbf{x}_N \}$ となる場合を考える。ただし、 $\mathbf{x}_i \in \mathbb{R}^D \; (i=1,\cdots,N)$ とする。
このような条件で、 $\mathbf{X}$ を入力に受け取り、1つのベクトル $\mathbf{y}/ \in \mathbb{R}^K$ を出力するネットワーク
$$
	\mathbf{y} = \mathbf{f}(\mathbf{X};\mathbf{w})
$$
を考える。**集合として表されるデータは、その要素に順序がないことが特徴** → $\mathbf{X}$ の要素の **並び替え (permutation)** が結果に影響しないことが求められる。
これに対する1つの答えは、集合の各要素の情報を平等に統合するプーリングを実行すること。例として、
$$
	\mathbf{y} = \mathbf{f(X)} = \mathbf{f'}\left( \sum_{i=1}^N \mathbf{g'}(\mathbf{x}_i) \right)
$$
この方法は PointNet と DeepSets で採用されている。これに加え、入力 $\mathbf{X}$ から別の集合 $\mathbf{Y} = \{ \mathbf{y}_1, \cdots, \mathbf{y}_N \}$ を出力する問題を考えることもある。
$$
	\mathbf{Y} = \mathbf{g(X;w)}
$$
トランスフォーマーは元々は集合データへの適用を想定して作られておらず、 $\mathbf{X}$ の要素数が大きくなると効率が悪化する。
そのたまえ、集合データ向けの改良を施した **集合トランスフォーマー (set transformer)** が考案されている。

---
## 7.2 注意機構

### 7.2.1 基本的な考え方

**注意機構 (attention mechanism)** はニューラルネットワークの構造要素の1つ。深層学習の中核をなす技術の1つに位置づけられる。
**注意 (attention)** とは、複数の要素からなる集合に対し、今持っている関心に応じた重要度に従って、各要素を重みづけすることを指す。

画像を小領域に分割し、それぞれをインデックス $i(=1,\cdots,N)$ で表したとき、各小領域 $i$ から画像特徴 $\mathbf{z}_i \in \mathbb{R}^D$ を取り出すとする。また、1つの問いをベクトル $\mathbf{q} \in \mathbb{R}^D$  (クエリ) で表現できるとする。
このとき、各領域の画像特徴 $\mathbf{z}_i$ と、問い $\mathbf{q}$ との関連性を、ある関数 $r$ によって
$$
	r_i = r(\mathbf{z}_i, \mathbf{q})
$$
と計算する。そして、各領域に対して得られた $r_1, \cdots, r_N$ を総和が1になるようにソフトマックス関数で正規化する。
$$
	a_i = \text{softmax}_i (r_1,\cdots,r_N) = \frac{\exp (r_i)}{\sum_{l=1}^N \exp (r_l)}
$$
この $a_i$ は、領域 $i$ (の特徴 $\mathbf{z}_i$ ) と $\mathbf{q}$ との関連性の高さを表すと言える。
