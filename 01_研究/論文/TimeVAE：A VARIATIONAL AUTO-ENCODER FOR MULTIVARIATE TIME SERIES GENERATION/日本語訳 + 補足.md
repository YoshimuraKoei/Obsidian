---
title:
AI: "false"
published:
created: " 2025-11-01"
description:
tags:
  - permanent-note
  - knowledge
  - paper
---

## 論文詳細

##### 📌 KEYWORDS
- Machine Learning

##### 📌 著者
- Abhyuday Desai
- Cynthia Freeman
- Ian Beaver
- Zuhui Wang

##### 📌 ジャーナル・出版情報
- **会議・ジャーナル**: Arxiv / ICLR 2022 審査中
- **日**: 2021/12/07
- **DOI**: 
- **ページ数**: 10
- **GitHub URL**: 
	- TimeVAE: https://github.com/abudesai/timeVAE
	- RCGAN: https://github.com/ratschlab/RGAN
	- TimeGAN: https://github.com/jsyoon0823/TimeGAN

---
## Abstract

この論文の要約を以下に示します。

時系列データの合成データ生成に関する最近の研究は、主に**敵対的生成ネットワーク（GAN）の利用**に焦点を当てています。しかし、本論文では、変分オートエンコーダ（VAE）を用いて時系列データを合成生成する新しいアーキテクチャを提案しています。この提案されたアーキテクチャは、解釈可能性、ドメイン知識のエンコード能力、およびトレーニング時間の短縮という、いくつかの異なる特性を持っています。

研究チームは、類似性と予測性によってデータ生成の品質を評価するために、4つの多変量データセットで評価を行いました。彼らは、VAEメソッドといくつかの最先端のデータ生成メソッドの両方について、データ可用性が生成品質に与える影響を測定するために、さまざまなサイズのトレーニングデータで実験を行っています。

類似性テストの結果は、**VAEアプローチが元のデータの時間的属性を正確に表現できる**ことを示しています。生成されたデータを用いた次ステップ予測タスクでは、提案されたVAEアーキテクチャが最先端のデータ生成メソッドのパフォーマンスと同等またはそれ以上であることが一貫して示されています。

生成されたデータからノイズを低減すると、元のデータから逸脱する可能性があるものの、その結果として得られるノイズ除去されたデータが、生成されたデータを用いた次ステップ予測のパフォーマンスを大幅に向上させることが実証されています。最後に、提案されたアーキテクチャは、**多項式トレンドや季節性などのドメイン固有の時系列パターンを組み込むことができ、解釈可能な出力を提供**します。このような解釈可能性は、モデル出力の透明性が必要なアプリケーションや、ユーザーが時系列パターンの事前知識を生成モデルに注入したい場合に非常に有利です。


---
## 1. Introduction

**生成モデルへの関心は近年、深層学習の産業界と学術界での利用が爆発的に増加するにつれて、かなり高まっています。** データジェネレーターは、十分な量の実データが利用できないシナリオ、プライバシー上の理由によるデータ利用制限、まだ現実で遭遇していない状況のシミュレーションの必要性、例外的なケースのシミュレーション、外れ値や変化点が存在するなどの特定のテストシナリオ用のデータセット作成の必要性において役立ちます。また、データハングリーな深層学習モデルの限界を軽減するのにも役立ちます。

生成モデルは大きく2つのカテゴリに分類されます。1つは実世界データから学習する**訓練済みジェネレーター**、もう1つはユーザー定義分布を持つ**モンテカルロサンプリングメソッドを使用するジェネレーター**です。**敵対的生成ネットワーク（GAN）、変分オートエンコーダ（VAE）、GPT（Radford et al., 2019）** などの言語モデルは最初のカテゴリに分類されます。モンテカルロ生成の簡単な例には、古典的なドーナツ問題（Beiden et al., 2003）やX-OR問題データ（Gomes et al., 2006）の生成に使用されるメソッドが含まれます。訓練済みジェネレーターの主な利点は、手動分析を必要とせずに実世界データで観測されたパターンを忠実に表現できることですが、その主な欠点は、実データをシミュレートすることを学習するために、大規模なトレーニングデータセットと長いトレーニング時間が必要になることです。一方、モンテカルロメソッドは、設定は手動ですが、よりシンプルで使いやすいです。それらから生成されたデータは解釈可能であり、ユーザーは生成されたデータを制御し、主題の専門知識を注入し、特定の状況をシミュレートできます。限界は、生成されたデータが実世界データと大きく異なる可能性があることです。したがって、両方のアプローチを組み合わせたハイブリッドメソッドは、予測、予測、分類、テストなど、多くの下流タスクにとって価値があるでしょう。


> [!NOTE] 生成モデルの説明
> - 実世界データから学習する訓練済みジェネレーター
> - ユーザー定義分布を持つモンテカルロサンプリングメソッド


時系列データの領域では、データ内の時間的パターンがあるため、合成データ生成は困難なタスクです。生成プロセスは、**特徴の分布と時間的関係の両方を捉える**必要があります。深層学習メソッドは、このような複雑な関係をモデル化するのに特に適しています。しかし、時系列データを含む多くの実世界のケースでは、利用可能なデータの量がサンプル数または履歴の長さの点で制限される可能性があります。例としては、新規株式公開後まもなくの企業に関わる株式市場予測や、新規開店した小売組織のスタッフニーズの予測などが挙げられます。このような状況では、データ量が少ないにもかかわらずうまく機能し、ユーザーが特定のユースケースに存在する既知の時系列パターンの特定の構造を導入できるデータ生成メソッドが必要です。

合成データ生成に関する最近の研究は、**GAN（Yoon et al., 2019; Esteban et al., 2017）の利用に大きく焦点を当てており**、主に生成と識別の両方に**リカレントニューラルネットワーク**を使用しています。しかし、時間的関係によって導入される複雑さのために、実データと合成データの二項識別という標準的なアプローチでは、時間的依存性を捉えるには不十分です。その結果、この課題を克服するために、GANネットワーク内で特別なメカニズムが必要とされます。そのような特別なメカニズムの例としては、自己回帰モデルで伝統的に使用されている教師あり学習とGANの教師なし学習を組み合わせたものが、Yoon et al. (2019) で提案されています。

両方の一般的なデータ生成アプローチの強みを活用するために、我々は**ユーザー定義分布を可能にするデコーダ設計を持つ変分オートエンコーダ（VAE）に基づく新しいモデル**を提案します。これを**TimeVAE**と呼びます。TimeVAEが実世界データの時間的成分を正確にモデル化できることを実証します。さらに、VAEに固有のエンコーダとデコーダ間のボトルネックメカニズムが、予測などの下流タスクに役立つノイズ除去の役割を果たすことを示します。我々のメソッドは、レベル、トレンド、季節性などのカスタム時間的構造を注入して、解釈可能な信号を生成することを可能にします。我々の実験は、TimeVAEが元のデータとの類似性を測定する際に最高の生成モデルのパフォーマンスを満たし、トレーニングに計算効率が優れ、次ステップ予測タスクにおいて利用可能なメソッドを上回ることを示しています。最後に、利用可能な実世界トレーニングデータのサイズが減少するにつれて、我々の提案するメソッドが既存のメソッドよりも優れていることを示します。


> [!NOTE] 流れ
> -   **生成モデルの重要性と活用シーンの紹介**: データ不足、プライバシー、シミュレーション、データセット作成の必要性から、深層学習モデルのデータ要求を満たす生成モデルへの関心が高まっていることを説明しています。
> - **生成モデルの2つの主要カテゴリ**:
> 	*   実世界データから学習する訓練済みジェネレーター（GANs, VAEs, GPTなど
> 	*   モンテカルロサンプリングメソッド
> * **各カテゴリの利点と課題**:
> 	*   訓練済みジェネレーター: 実世界データを忠実に表現できるが、大規模なデータと長い訓練時間が必要。
> 	*   モンテカルロメソッド: 設定は手動だが、シンプルで使いやすく、解釈可能で制御しやすい。ただし、生成データが実世界データと大きく異なる可能性がある。
> *   **時系列データ生成の課題**: データ内の時間的パターンを捉える必要があり、深層学習が適しているが、利用可能なデータ量が限られる場合に問題となる点を指摘。
> *   **GANsを中心とした先行研究の紹介と限界**: GANsが主流だが、時間的関係の複雑さから時間的依存性を捉えるのが難しいという課題があり、特別なメカニズムが必要とされる。
> *   **本研究の提案モデル「TimeVAE」の紹介**: 上記の課題を解決するため、VAEに基づいた新しいモデルであるTimeVAEを提案。
> *   **TimeVAEの主な特徴と貢献**:
> 	*   実世界データの時間的成分を正確にモデル化できる。
> 	*   ノイズ除去効果がある。
> 	*   レベル、トレンド、季節性などのカスタム時間的構造を注入でき、解釈可能な信号を生成する。
> 	*   既存の生成モデルと同等以上のパフォーマンスを発揮し、計算効率も良い。
> 	*   特にトレーニングデータが少ない場合に、既存のメソッドよりも優れている。


---
## 2. Related Work

関連研究として、Yoon et al. (2019) では、自己回帰モデルの能力と教師なしGANアプローチを組み合わせたTimeGANが提案されています。TimeGANは、自己回帰モデルが持つ決定論的という欠点を補いつつ、GANが時系列データにおける時間的相関を捉えるのが苦手という課題に対処しようとするもので、合成時系列データ生成の現在の最先端技術です。

Recurrent Conditional GAN (RCGAN) は、プライバシー保護のためのeICUデータ生成のためにEsteban et al. (2017) によって開発されました。これは、実数値の時系列データと関連するラベルからなる補助情報で条件付けられたRNNベースのGANアーキテクチャを使用して、実数値のシーケンシャルデータを生成します。

C-RNN-GANは、Mogren (2016) によって古典的な音楽データを生成するために開発されました。このモデルでは、ランダムに生成されたノイズが深層LSTMモジュールに入力され、隠れた表現が生成され、その後、全結合層に送られて最終的なシーケンシャルデータサンプルが得られます。この敵対的学習は、RNNが使用する音の数や強度が異なる音楽を生成するのに役立ちます。

Fabius & Van Amersfoort (2014) は、エンコーダとデコーダにRNNを使用したリカレントVAEを実装しました。このアーキテクチャは、エンコーダRNNの最後にエンコードされた状態を入力シーケンスの表現として使用し、VAEで使用される再パラメータ化トリックに送ります。エンコードされた表現からのサンプルはデコーダに渡されます。しかし、Hyland et al. (2017) は、生成された正弦波データでテストした際に、RCGANメソッドで得られた結果よりも明らかに劣る不整合な結果を報告しています。

提示された生成モデルの一般的な限界には、**訓練が難しいこと**、すなわち、**訓練されたモデルが多様性のない合成データを生成してしまうモード崩壊に陥りやすい**ことが挙げられます（Srivastava et al., 2017）。また、TimeGANやRCGANメソッドは、大規模なデータセットで5,000エポックを訓練するのにV100 GPUを使用しても1日以上かかり、時間がかかるという問題もあります。さらに、GANベースのモデルを訓練するには十分なデータが必要ですが、多くの実世界の予測状況では十分なデータを容易に得ることができません。本論文で提案されているモデルは、これらの限界に対処しようと試みています。

---
## 3. Methods

本研究の目標は、以下の2つの目的を達成することです。まず、**VAEのエンコーダとデコーダのコンポーネントについて、時系列データの現実的なサンプルを生成する能力を最大化するアーキテクチャを選択したい**と考えています。次に、**データ生成プロセスに特定の時間的構造を注入できるアーキテクチャ**を求めます。これらの時間的構造は、解釈可能な生成プロセスを作成し、ジェネレーターモデルを訓練するための実世界データが不十分な場合に、ドメインの専門知識を注入するために使用できます。

### 3.1 Variational Auto-Encoder As a Generative Model

まず、VAE（Variational Auto-Encoders）について簡単に概説します。従来のオートエンコーダとは異なり、VAEのエンコーダは埋め込みの点推定ではなく、その分布を出力します。我々は、連続変数または離散変数 $x$ の $N$ 個の独立同分布（i.i.d.）サンプルからなる入力データセット $X$ を与えられています。我々の主な目標は、元のデータの分布を正確に表現するサンプルを生成することです。具体的には、未知の確率関数$p(x)$をモデル化したいと考えています。

Kingma & Welling (2013) によって記述されたVAEは、観測データが2段階のプロセスによって生成されると仮定しています。まず、ある事前分布$p_\theta(z)$から値 $z$ が生成され、次に、ある条件付き分布 $p_\theta(x|z)$ から値 $x$ が生成されます。事前分布と尤度 $p_\theta(x|z)$ の真の値は不明ですが、我々はそれらが $\theta$ と $z$ の両方に関して微分可能であると仮定します。ここで、入力データと潜在表現の関係は次のように定義できます。事前分布は $p_\theta(z)$ 、尤度は $p_\theta(x|z)$ 、事後分布は $p_\theta(z|x)$ です。


> [!NOTE] WRT
>  we assume they are differentiable **WRT** both $θ$ and $z$. 
>  → WRT = With Respect To = ～に関して


$p_\theta(z)$の計算は非常に高価であり、ごく一般的に扱いにくいものです。これを克服するために、VAEは事後分布の近似を次のように導入します。
$$
q_\Phi(z|x) \approx p_\theta(z|x)
$$
このフレームワークでは、エンコーダは確率的事後分布 $q_\Phi(z|x)$ をモデル化する役割を果たし、デコーダは条件付き尤度 $p_\theta(x|z)$ をモデル化する役割を果たします。

通常、$z$ のこの事前分布はガウス分布、より具体的には**標準正規分布**が選択されます。そして、潜在空間がこの事前分布の周りに収まるように、訓練中に事後分布が正則化されます。これは、事後分布の変分近似と選択された事前分布との間のKLダイバージェンスを損失関数に追加することによって行われます。与えられた入力を選択された事前分布に埋め込むため、事前分布から $z$ を直接サンプリングし、この $z$ をデコーダに通すことができます。これにより、VAEのデコーダは本質的に生成モデルに変換されます。


> [!NOTE] KLダイバージェンス
> 今、事後分布 $p_\theta (z|x)$ を$q_\Phi (z|x)$ で近似しようとしているが、その複雑な元々の分布と近似した分布の類似度合いを評価したい。
> そのために**KLダイバージェンス**は使われる。
> 


### 3.2 VAE Loss Function

VAEの損失関数は、**Evidence Lower Bound（ELBO）損失関数**とも呼ばれ、次のように書くことができます。

$$
L_{\theta, \phi} = -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + D_{KL}(q_\phi(z|x)||p_\theta(z))
$$

右辺の最初の項は、 $q_\phi(z|x)$ からサンプリングされた $z$ が与えられた場合の、データの負の対数尤度です。右辺の2番目の項は、エンコードされた潜在空間分布と事前分布との間のKLダイバージェンス損失です。通常、 $q_\phi(z|x)$ から $z$ をサンプリングするプロセスは微分不可能になります。しかし、VAEは「再パラメータ化トリック」と呼ばれる手法を用いることで、VAEをエンドツーエンドで訓練可能にしています（Kingma & Welling, 2013）。

### 3.3 Base TimeVAE Architecture

ここではまだエンコーディング関数とデコーディング関数については説明していません。上記で説明した損失関数が微分可能である限り、これらの関数には任意のモデルを選択できます。我々の手法は、**全結合層**や**畳み込み層**といった従来の深層学習層と、レベル、多項式トレンド、季節パターンといった時系列固有のコンポーネントをモデル化するためのカスタム層を組み合わせて使用しています。図1は、TimeVAEの基本バージョンのブロック図を示しています。基本バージョンにはカスタム時間的構造は含まれておらず、時系列固有の知識も必要ありません。

エンコーダへの入力信号 $X$ は、 $N \times T \times D$ の3次元配列です。ここで、 $N$ はバッチサイズ、 $T$ はタイムステップの数、 $D$ は特徴次元の数です。与えられたデータが可変長のシーケンスである場合、すべてのシーケンスが同じ長さ $T$ になるように、先頭にゼロを埋め込みます。エンコーダは、一連の畳み込み層をReLU活性化関数とともに通過させます。次に、データは平坦化されてから、全結合（高密度）線形層を通過します。もし $m$ が選択された潜在次元の数であり、多変量ガウス分布の次元を表す場合、この最後の層は $2m$ 個のニューロンを持ちます。この出力を使用して多変量ガウス分布をパラメータ化します。潜在空間のサイズ $m$ は、モデルの重要なハイパーパラメータです。

次に、再パラメータ化トリック (Reparameterization Trick) を使用して、多変量ガウス分布からベクトル $z$ をサンプリングします。デコーダは、長さ $m$ のサンプリングされた潜在ベクトル $z$ を受け取ります。それは全結合線形層を通過します。その後、データは3次元配列に再形成され、一連の転置畳み込み層をReLU活性化関数とともに通過します。最後に、データは、最終的な出力形状が元の信号 $X$ と同じになるような次元を持つ時系列分散全結合層を通過します。


> [!NOTE] メモ
> エンコーダは、従来のオートエンコーダが点推定を出力するのとは異なり、埋め込みの分布を出力する。
> output layer は 2m 個のニューロンを持ち、多変量ガウス分布の平均 $\mu$ と分散 $\sigma^2$ を出力する。
> これらのパラメータを用いて、潜在空間におけるデータの確率分布を表現する。

[[Reparameterization Trickについて]]


> [!NOTE] 学習方法
> この論文の文脈では、エンコーダとデコーダはE2Eで学習する。そのために、**Reparameterization Trick** を用いて微分可能にしている。

![[Pasted image 20251103164703.png]]

### 3.4 Interpretable TimeVAE
![[Pasted image 20251103153851.png]]****

我々は、デコーダにおけるデータ生成プロセスに時間的構造を注入することで、モデル化されたデータ生成プロセスの解釈可能性を達成しています。より具体的には、予測モデルで一般的なアプローチであるレベル、トレンド、季節成分への時系列分解を使用します。たとえば、有名なHolt-Winters指数平滑化手法（Holt, 1957; Winters, 1960）などが挙げられます。

解釈可能なTimeVAE（図2）は、Base TimeVAEと同じエンコーダ構造を使用しています。デコーダは、デコードプロセスに特定の時間的構造を付与するため、より複雑なアーキテクチャを持っています。デコーダは、異なる時間的構造を表す並列ブロックを使用し、それらを合計して最終出力を生成します。構造はトレンドと季節性の2種類です。

次に、トレンドブロックと季節性ブロックを定義します（図3参照）。以下のセクションでは、$N$をバッチサイズ、$D$を特徴の数、$T$をタイムステップの数（すなわちエポックの数）とします。

**トレンドブロック**: 我々は、Oreshkin et al. (2019) によってN-Beats予測モデルのために開発されたトレンドアーキテクチャを再利用しています。トレンドは単調関数としてモデル化されます。 $P$ をユーザーによって指定される多項式の次数とします。トレンド多項式 $p = 0, 1, 2, ..., P$ を2段階のプロセスでモデル化します。まず、潜在空間ベクトル $z$ を使用して、トレンドの基底展開係数 $\theta_{tr}$ を推定します。 $\theta_{tr}$ は $N \times D \times P$ の次元を持ちます。次に、 $\theta_{tr}$ を使用して元の信号中のトレンド $V_{tr}$ を再構築します。信号中のトレンド再構築は行列形式で以下のようになります。

$$
V_{tr} = \theta_{tr}R
$$

ここで、 $R = [1, r, ..., r^P]$ は、 $r = [0, 1, 2, ..., T - 1]/T$ である時間ベクトル $r$ の累乗の行列です。
$R$ は $P \times T$ の次元を持ちます。我々は $\theta_{tr}$ と $R$ の行列乗算を実行し、その後、軸1と軸2を転置して、最終的なトレンド行列を得ます。この行列 $V_{tr}$ は $N \times T \times D$ の次元を持ちます。


> [!NOTE] テンソルと行列のブロードキャスト
> $N \times D \times P$ と $P \times T$ の掛け算で、 $N \times D \times T$ の次元を作る。
> $$
> 	V_{tr}​[n,d,t] = \sum_{p=0}^{P-1} \theta_{tr}​[n,d,p] \cdot R[p,t]
> $$


行列 $\theta_{tr}$ は、トレンドブロックに解釈可能性をもたらします。 $\theta_{tr}$ からの値は、特に各サンプル $N$ と特徴次元 $D$ について、0次、1次、2次、...、 $P$ 次のトレンドを定義します。

$p=0$ の場合、平坦なトレンド（すなわち、上昇または下降トレンドがない）が得られることに注意してください。これは、従来の時系列モデリングの用語でいう「レベル」コンポーネントに相当します。レベルとは、時系列の平均値を指します。


> [!NOTE] トレンド
> トレンドブロックは、**時間方向の変化を「多項式基底＋係数」という明示的・分解可能な形で表す**ことで、  
> 各項（係数）が「何を意味しているのか」が人間に理解できる構造を持っているから、解釈可能になる。
> 通常のNNデコーダ： 「この出力がなぜ得られたか」を追跡できない。
> TimeVAE： 「このトレンドは $\theta_0, \theta_1, \theta_2$ の組み合わせでどのように生成されたか」が説明できる。

|種類|導関数|幾何的意味|
|---|---|---|
|1階導関数 (V'(r))|傾き（上昇 or 下降）|トレンド方向|
|2階導関数 (V''(r))|曲率（凹・凸）|加速 or 減速|
|3階導関数 (V'''(r))|曲率の変化（変曲点）|折れ目・転換点の出現|


**解釈可能なデコーダの出力**: 解釈可能なデコーダからの最終出力は、トレンドブロック出力$V_{tr}$、季節性ブロック出力$V^j_{sn}$（$j = 1, 2, ..., S$ の場合）、そして（使用される場合）残差ベースデコーダからの出力の要素ごとの合計です。

**季節性ブロック**: モデル化する異なる季節性パターンの数を $S$ とします。各季節性パターン（インデックス $j$ で示される）は、季節の数 $m$ と各季節の期間 $d$ という2つのパラメータで定義されます。例えば、日次データにおける曜日の季節性を表現する場合、 $m$ は7、 $d$ は1となります。一方、時間単位のデータにおける曜日の季節性は、 $m$ が7、 $d$ が24でモデル化されます。

各季節性パターン $j$ について、TimeVAEは2つのステップを実行します。まず、潜在空間ベクトル $z$ を使用して、次元が $N \times D \times m$ である基底展開係数行列 $\theta^j_{sn}$ を推定します。次に、 $X$ の各タイムステップに対応する特定の季節の $\theta^j_{sn}$ の要素をインデックス付けして、形状が $N \times T \times D$ である季節パターン値 $V^j_{sn}$ を取得します。TensorFlowでは、季節インデックス配列 $K$ を使用して、このインデックス付けを実行するために `tf.gather` 関数を使用します。最終的な季節性推定値 $V_{sn}$ は、 $j = 1, 2, ..., S$ のすべての $V^j_{sn}$ の要素ごとの合計です。

季節性ブロック$j$において、行列$\theta^j_{sn}$は、各サンプル$N$と特徴次元$D$に対する季節パターンに解釈可能性をもたらします。$\theta^j_{sn}$の要素をインデックス付けすることで、季節サイクル内の$m$個の季節のそれぞれが与える影響を特定できます。

**残差ブロックとしてのベースデコーダ**: 解釈可能なTimeVAEアーキテクチャでは、元のベースデコーダ（図1）をデコーダ内の残差ブランチとして使用することも可能です。実際、図2に示すトレンド、季節性、またはベースデコーダのいずれかのブランチを有効または無効に選択できます。


> [!NOTE] デコーダの最終出力
> $$
> 	V_{\text{decoded}} = V_{\text{tr}} + \sum_{j=1}^S V_{\text{sn}}^j + V_{\text{res}} \; (\text{if used})
> $$
> - $V_{tr}$​：**トレンド成分**（長期的・滑らかな変化）
> - $V^j_{sn}$​：**季節性成分**（周期的な変化、複数周期あり得る）
> - $V_{res}$​：**残差成分**（非構造的なノイズや残り）
> $$
> 	V_{sn}^j ​(t) = \theta_{sn}^j​[:,:,K_t]
> $$
> 
> → 季節インデックス (周期的な時間位置) に応じて、対応する係数を取り出す処理
> $$
> 	K_t = t \mod m
> $$

| 記号                  | 形状                      | 意味                                                     |
| ------------------- | ----------------------- | ------------------------------------------------------ |
| $( \theta^j_{sn} )$ | $(N \times D \times m)$ | 各サンプル $(N)$ × 特徴次元 $(D)$ × 季節の種類 $(m)$ に対する「季節ごとの効果」係数 |
| $( K_t )$           | 長さTの配列                  | 各時間ステップ $t$ が属する季節インデックス（例: $t \mod m$）                |
| $( V^j_{sn}(t) )$   | $(N \times D)$          | 時刻 $t$ における出力値（後で $T$ 軸に展開）                            |


> [!NOTE] 残渣ブロックとしてのベースデコーダ
> トレンドや季節性では説明しきれない「残り（residual）」の時間構造を柔軟に補うための補正項。この部分は解釈不可能。


### 3.5 TimeVAE Objective Function

我々は、以前に定義したELBO損失関数に1つの修正を加えてTimeVAEを訓練します。この修正では、近似された事後分布$q_\phi(z|x)$と事前分布$p_\theta(z)$との間のKLダイバージェンス損失と比較して、再構築損失に与えられる重要度を増減させるために、再構築誤差に重みを使用します。

この再構築誤差の重み係数は、その後の実験では0.5から3.5の範囲で変化させました。この重みは、生成されたサンプルの品質を目視で検査するか、生成されたサンプルが下流の教師あり学習タスクで使用される場合は、ハイパーパラメータチューニングによって選択できます。


> [!NOTE] 復習
> VAE における中核となる **ELBO 損失関数**
> $$
> 	L_{\theta, \phi} = -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + D_{KL}(q_\phi(z|x)||p_\theta(z))
> $$ 


---
## 4. Experiments

### 4.1 Data Sets

本研究の実験では、4つの多変量データセットを検討しました。これらのデータセットは以下の通りです。

1.  **sines（サイン波）**: 周波数、振幅、位相が異なる5次元の正弦波シーケンスを10,000サンプル生成しました。各特徴は互いに相関しています。具体的には、各次元 $i \in \{1, 2, ..., 5\}$ について、$x_i(t) = a \sin(2\pi \eta t + \theta)$ と定義され、ここで $\eta \sim u[0.1, 0.15]$、 $\theta \sim u[0, 2\pi]$、 $a \sim u[1.0, 3.0]$ です。
2.  **stockv（株価）**: Yahoo Financeから取得した10年間の6次元日次株価データを3,919サンプル使用しました。
3.  **energy（エネルギー）**: UCI Machine Learningリポジトリから、連続値測定で構成される28次元の家電エネルギー予測データセットを19,711サンプル使用しました。
4.  **air（大気）**: UCI Machine Learningリポジトリから、空気品質センサーによる15の特徴を持つ時間平均応答データを9,333サンプル使用しました。

### 4.2 Comparison Methodologies

我々は、我々のVAEフレームワークを、RCGAN（教師強制学習（T-Forcing）で訓練されたRNN）およびTimeGANと比較します。これらは、Yoon et al. (2019) の研究においてTimeGANと比較して最高のパフォーマンスを示した手法でした。

T-Forcing、または教師強制学習は、自己回帰型リカレントニューラルネットワークを訓練するための決定論的な手法であり、先行するタイムステップのモデル出力の代わりに、正解データを入力として使用します（Yoon et al., 2019）。Yoon et al. (2019) が彼らのT-Forcingコードを提供していないため、我々は彼らの実装の補足説明に可能な限り従いました。具体的には、隠れ次元が入力特徴量の4倍のサイズの3層GRU、tanh活性化関数、出力層の活性化関数としてのシグモイド、[0,1]範囲にmin-maxスケーリングされた出力、$\lambda = 1$、$\eta = 10$を使用しました。

決定論的な自己回帰アプローチの代わりに、Esteban et al. (2017) で言及されているRCGANは、シーケンシャルデータにGANアーキテクチャを適用しますが、追加の入力に条件付けしつつ、前の出力への依存を排除します。我々は、異なるデータセットの次元とサンプルサイズを考慮して設定を調整し、[https://github.com/ratschlab/RGAN](https://github.com/ratschlab/RGAN) のRCGANコードを使用しています。

TimeGANは、敵対的GAN手法と自己回帰モデル技術を組み合わせて時間的ダイナミクスを考慮する生成時系列モデルです。我々は、著者らのコードを[https://github.com/jsyoon0823/TimeGAN](https://github.com/jsyoon0823/TimeGAN) で直接適用しています。

### 4.3 Comparison Metrics

生成されたデータの品質を評価するために、我々は時間次元を平坦化した元のデータと合成データの**2次元t-SNEプロット**を分析します。また、教師ありタスクとして、元のデータと合成データを区別するための分類モデルも訓練します。**識別スコア**は、ホールドアウトセットでの（**精度 - 0.5**）です。スコアが0に近いほど、生成されたデータが元のデータと区別しにくいことを示しており、より良い結果と判断されます。

最後に、**予測スコア**を考慮します。より具体的には、次ステップの時系列ベクトルを予測するために、事後シーケンスモデルを訓練します。この2層LSTMは合成データで訓練され、元のデータセットに対する平均絶対誤差を使用して評価されます。これらの比較指標はすべてYoon et al. (2019) で検討されており、我々はこれらの指標のために著者らが公開している[https://github.com/jsyoon0823/TimeGAN](https://github.com/jsyoon0823/TimeGAN) のコードを使用しています。スコアが低いほど、より良いと判断されます。

### 4.4 Procedure

セクション4.1で述べられている各データセットについて、各手法で100%、20%、10%、5%、2%を訓練データとして使用します。たとえば、RCGANがairデータセットの最後の20%のみで訓練された場合にどれだけうまく機能するかを検討します。これらの訓練されたモデルによって生成された合成データは、その後、セクション4.3で記述されている識別および予測スコアを得るために、事後シーケンスモデルを訓練するために使用されます。事後シーケンスモデルを訓練するために生成されるデータの量は、ジェネレーターを訓練するために使用される元の訓練データの割合と同等です。


> [!NOTE] 事後シーケンスモデル
> TimeVAE や TimeGAN などは人工的に新しい時系列データを生成するモデル。
> **生成データを使って下流タスクを訓練し、実データでの性能を測る**という方法を取る。
> この下流タスクで使われるモデルが **事後シーケンスモデル** である。


## 5. Results / Discussion

