---
title:
AI: "false"
published:
created: " 2025-11-01"
description:
tags:
  - permanent-note
  - knowledge
  - paper
---

## 論文詳細

##### 📌 KEYWORDS
- Machine Learning

##### 📌 著者
- Abhyuday Desai
- Cynthia Freeman
- Ian Beaver
- Zuhui Wang

##### 📌 ジャーナル・出版情報
- **会議・ジャーナル**: Arxiv / ICLR 2022 審査中
- **日**: 2021/12/07
- **DOI**: 
- **ページ数**: 10
- **GitHub URL**: 

---
## Abstract

この論文の要約を以下に示します。

時系列データの合成データ生成に関する最近の研究は、主に**敵対的生成ネットワーク（GAN）の利用**に焦点を当てています。しかし、本論文では、変分オートエンコーダ（VAE）を用いて時系列データを合成生成する新しいアーキテクチャを提案しています。この提案されたアーキテクチャは、解釈可能性、ドメイン知識のエンコード能力、およびトレーニング時間の短縮という、いくつかの異なる特性を持っています。

研究チームは、類似性と予測性によってデータ生成の品質を評価するために、4つの多変量データセットで評価を行いました。彼らは、VAEメソッドといくつかの最先端のデータ生成メソッドの両方について、データ可用性が生成品質に与える影響を測定するために、さまざまなサイズのトレーニングデータで実験を行っています。

類似性テストの結果は、**VAEアプローチが元のデータの時間的属性を正確に表現できる**ことを示しています。生成されたデータを用いた次ステップ予測タスクでは、提案されたVAEアーキテクチャが最先端のデータ生成メソッドのパフォーマンスと同等またはそれ以上であることが一貫して示されています。

生成されたデータからノイズを低減すると、元のデータから逸脱する可能性があるものの、その結果として得られるノイズ除去されたデータが、生成されたデータを用いた次ステップ予測のパフォーマンスを大幅に向上させることが実証されています。最後に、提案されたアーキテクチャは、**多項式トレンドや季節性などのドメイン固有の時系列パターンを組み込むことができ、解釈可能な出力を提供**します。このような解釈可能性は、モデル出力の透明性が必要なアプリケーションや、ユーザーが時系列パターンの事前知識を生成モデルに注入したい場合に非常に有利です。


---
## 1. Introduction

**生成モデルへの関心は近年、深層学習の産業界と学術界での利用が爆発的に増加するにつれて、かなり高まっています。** データジェネレーターは、十分な量の実データが利用できないシナリオ、プライバシー上の理由によるデータ利用制限、まだ現実で遭遇していない状況のシミュレーションの必要性、例外的なケースのシミュレーション、外れ値や変化点が存在するなどの特定のテストシナリオ用のデータセット作成の必要性において役立ちます。また、データハングリーな深層学習モデルの限界を軽減するのにも役立ちます。

生成モデルは大きく2つのカテゴリに分類されます。1つは実世界データから学習する**訓練済みジェネレーター**、もう1つはユーザー定義分布を持つ**モンテカルロサンプリングメソッドを使用するジェネレーター**です。**敵対的生成ネットワーク（GAN）、変分オートエンコーダ（VAE）、GPT（Radford et al., 2019）** などの言語モデルは最初のカテゴリに分類されます。モンテカルロ生成の簡単な例には、古典的なドーナツ問題（Beiden et al., 2003）やX-OR問題データ（Gomes et al., 2006）の生成に使用されるメソッドが含まれます。訓練済みジェネレーターの主な利点は、手動分析を必要とせずに実世界データで観測されたパターンを忠実に表現できることですが、その主な欠点は、実データをシミュレートすることを学習するために、大規模なトレーニングデータセットと長いトレーニング時間が必要になることです。一方、モンテカルロメソッドは、設定は手動ですが、よりシンプルで使いやすいです。それらから生成されたデータは解釈可能であり、ユーザーは生成されたデータを制御し、主題の専門知識を注入し、特定の状況をシミュレートできます。限界は、生成されたデータが実世界データと大きく異なる可能性があることです。したがって、両方のアプローチを組み合わせたハイブリッドメソッドは、予測、予測、分類、テストなど、多くの下流タスクにとって価値があるでしょう。


> [!NOTE] 生成モデルの説明
> - 実世界データから学習する訓練済みジェネレーター
> - ユーザー定義分布を持つモンテカルロサンプリングメソッド


時系列データの領域では、データ内の時間的パターンがあるため、合成データ生成は困難なタスクです。生成プロセスは、**特徴の分布と時間的関係の両方を捉える**必要があります。深層学習メソッドは、このような複雑な関係をモデル化するのに特に適しています。しかし、時系列データを含む多くの実世界のケースでは、利用可能なデータの量がサンプル数または履歴の長さの点で制限される可能性があります。例としては、新規株式公開後まもなくの企業に関わる株式市場予測や、新規開店した小売組織のスタッフニーズの予測などが挙げられます。このような状況では、データ量が少ないにもかかわらずうまく機能し、ユーザーが特定のユースケースに存在する既知の時系列パターンの特定の構造を導入できるデータ生成メソッドが必要です。

合成データ生成に関する最近の研究は、**GAN（Yoon et al., 2019; Esteban et al., 2017）の利用に大きく焦点を当てており**、主に生成と識別の両方に**リカレントニューラルネットワーク**を使用しています。しかし、時間的関係によって導入される複雑さのために、実データと合成データの二項識別という標準的なアプローチでは、時間的依存性を捉えるには不十分です。その結果、この課題を克服するために、GANネットワーク内で特別なメカニズムが必要とされます。そのような特別なメカニズムの例としては、自己回帰モデルで伝統的に使用されている教師あり学習とGANの教師なし学習を組み合わせたものが、Yoon et al. (2019) で提案されています。

両方の一般的なデータ生成アプローチの強みを活用するために、我々は**ユーザー定義分布を可能にするデコーダ設計を持つ変分オートエンコーダ（VAE）に基づく新しいモデル**を提案します。これを**TimeVAE**と呼びます。TimeVAEが実世界データの時間的成分を正確にモデル化できることを実証します。さらに、VAEに固有のエンコーダとデコーダ間のボトルネックメカニズムが、予測などの下流タスクに役立つノイズ除去の役割を果たすことを示します。我々のメソッドは、レベル、トレンド、季節性などのカスタム時間的構造を注入して、解釈可能な信号を生成することを可能にします。我々の実験は、TimeVAEが元のデータとの類似性を測定する際に最高の生成モデルのパフォーマンスを満たし、トレーニングに計算効率が優れ、次ステップ予測タスクにおいて利用可能なメソッドを上回ることを示しています。最後に、利用可能な実世界トレーニングデータのサイズが減少するにつれて、我々の提案するメソッドが既存のメソッドよりも優れていることを示します。


> [!NOTE] 流れ
> -   **生成モデルの重要性と活用シーンの紹介**: データ不足、プライバシー、シミュレーション、データセット作成の必要性から、深層学習モデルのデータ要求を満たす生成モデルへの関心が高まっていることを説明しています。
> - **生成モデルの2つの主要カテゴリ**:
> 	*   実世界データから学習する訓練済みジェネレーター（GANs, VAEs, GPTなど
> 	*   モンテカルロサンプリングメソッド
> * **各カテゴリの利点と課題**:
> 	*   訓練済みジェネレーター: 実世界データを忠実に表現できるが、大規模なデータと長い訓練時間が必要。
> 	*   モンテカルロメソッド: 設定は手動だが、シンプルで使いやすく、解釈可能で制御しやすい。ただし、生成データが実世界データと大きく異なる可能性がある。
> *   **時系列データ生成の課題**: データ内の時間的パターンを捉える必要があり、深層学習が適しているが、利用可能なデータ量が限られる場合に問題となる点を指摘。
> *   **GANsを中心とした先行研究の紹介と限界**: GANsが主流だが、時間的関係の複雑さから時間的依存性を捉えるのが難しいという課題があり、特別なメカニズムが必要とされる。
> *   **本研究の提案モデル「TimeVAE」の紹介**: 上記の課題を解決するため、VAEに基づいた新しいモデルであるTimeVAEを提案。
> *   **TimeVAEの主な特徴と貢献**:
> 	*   実世界データの時間的成分を正確にモデル化できる。
> 	*   ノイズ除去効果がある。
> 	*   レベル、トレンド、季節性などのカスタム時間的構造を注入でき、解釈可能な信号を生成する。
> 	*   既存の生成モデルと同等以上のパフォーマンスを発揮し、計算効率も良い。
> 	*   特にトレーニングデータが少ない場合に、既存のメソッドよりも優れている。


---
## 2. Related Work

関連研究として、Yoon et al. (2019) では、自己回帰モデルの能力と教師なしGANアプローチを組み合わせたTimeGANが提案されています。TimeGANは、自己回帰モデルが持つ決定論的という欠点を補いつつ、GANが時系列データにおける時間的相関を捉えるのが苦手という課題に対処しようとするもので、合成時系列データ生成の現在の最先端技術です。

Recurrent Conditional GAN (RCGAN) は、プライバシー保護のためのeICUデータ生成のためにEsteban et al. (2017) によって開発されました。これは、実数値の時系列データと関連するラベルからなる補助情報で条件付けられたRNNベースのGANアーキテクチャを使用して、実数値のシーケンシャルデータを生成します。

C-RNN-GANは、Mogren (2016) によって古典的な音楽データを生成するために開発されました。このモデルでは、ランダムに生成されたノイズが深層LSTMモジュールに入力され、隠れた表現が生成され、その後、全結合層に送られて最終的なシーケンシャルデータサンプルが得られます。この敵対的学習は、RNNが使用する音の数や強度が異なる音楽を生成するのに役立ちます。

Fabius & Van Amersfoort (2014) は、エンコーダとデコーダにRNNを使用したリカレントVAEを実装しました。このアーキテクチャは、エンコーダRNNの最後にエンコードされた状態を入力シーケンスの表現として使用し、VAEで使用される再パラメータ化トリックに送ります。エンコードされた表現からのサンプルはデコーダに渡されます。しかし、Hyland et al. (2017) は、生成された正弦波データでテストした際に、RCGANメソッドで得られた結果よりも明らかに劣る不整合な結果を報告しています。

提示された生成モデルの一般的な限界には、**訓練が難しいこと**、すなわち、**訓練されたモデルが多様性のない合成データを生成してしまうモード崩壊に陥りやすい**ことが挙げられます（Srivastava et al., 2017）。また、TimeGANやRCGANメソッドは、大規模なデータセットで5,000エポックを訓練するのにV100 GPUを使用しても1日以上かかり、時間がかかるという問題もあります。さらに、GANベースのモデルを訓練するには十分なデータが必要ですが、多くの実世界の予測状況では十分なデータを容易に得ることができません。本論文で提案されているモデルは、これらの限界に対処しようと試みています。

---
## 3. Methods

本研究の目標は、以下の2つの目的を達成することです。まず、**VAEのエンコーダとデコーダのコンポーネントについて、時系列データの現実的なサンプルを生成する能力を最大化するアーキテクチャを選択したい**と考えています。次に、**データ生成プロセスに特定の時間的構造を注入できるアーキテクチャ**を求めます。これらの時間的構造は、解釈可能な生成プロセスを作成し、ジェネレーターモデルを訓練するための実世界データが不十分な場合に、ドメインの専門知識を注入するために使用できます。

### 3.1 Variational Auto-Encoder As a Generative Model

まず、VAE（Variational Auto-Encoders）について簡単に概説します。従来のオートエンコーダとは異なり、VAEのエンコーダは埋め込みの点推定ではなく、その分布を出力します。我々は、連続変数または離散変数 $x$ の $N$ 個の独立同分布（i.i.d.）サンプルからなる入力データセット $X$ を与えられています。我々の主な目標は、元のデータの分布を正確に表現するサンプルを生成することです。具体的には、未知の確率関数$p(x)$をモデル化したいと考えています。

Kingma & Welling (2013) によって記述されたVAEは、観測データが2段階のプロセスによって生成されると仮定しています。まず、ある事前分布$p_\theta(z)$から値 $z$ が生成され、次に、ある条件付き分布 $p_\theta(x|z)$ から値 $x$ が生成されます。事前分布と尤度 $p_\theta(x|z)$ の真の値は不明ですが、我々はそれらが $\theta$ と $z$ の両方に関して微分可能であると仮定します。ここで、入力データと潜在表現の関係は次のように定義できます。事前分布は $p_\theta(z)$ 、尤度は $p_\theta(x|z)$ 、事後分布は $p_\theta(z|x)$ です。


> [!NOTE] WRT
>  we assume they are differentiable **WRT** both $θ$ and $z$. 
>  → WRT = With Respect To = ～に関して


$p_\theta(z)$の計算は非常に高価であり、ごく一般的に扱いにくいものです。これを克服するために、VAEは事後分布の近似を次のように導入します。
$$
q_\Phi(z|x) \approx p_\theta(z|x)
$$
このフレームワークでは、エンコーダは確率的事後分布 $q_\Phi(z|x)$ をモデル化する役割を果たし、デコーダは条件付き尤度 $p_\theta(x|z)$ をモデル化する役割を果たします。

通常、$z$ のこの事前分布はガウス分布、より具体的には**標準正規分布**が選択されます。そして、潜在空間がこの事前分布の周りに収まるように、訓練中に事後分布が正則化されます。これは、事後分布の変分近似と選択された事前分布との間のKLダイバージェンスを損失関数に追加することによって行われます。与えられた入力を選択された事前分布に埋め込むため、事前分布から $z$ を直接サンプリングし、この $z$ をデコーダに通すことができます。これにより、VAEのデコーダは本質的に生成モデルに変換されます。


> [!NOTE] KLダイバージェンス
> 今、事後分布 $p_\theta (z|x)$ を$q_\Phi (z|x)$ で近似しようとしているが、その複雑な元々の分布と近似した分布の類似度合いを評価したい。
> そのために**KLダイバージェンス**は使われる。
> 


### 3.2 VAE Loss Function

VAEの損失関数は、**Evidence Lower Bound（ELBO）損失関数**とも呼ばれ、次のように書くことができます。

$$
L_{\theta, \phi} = -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + D_{KL}(q_\phi(z|x)||p_\theta(z))
$$

右辺の最初の項は、 $q_\phi(z|x)$ からサンプリングされた $z$ が与えられた場合の、データの負の対数尤度です。右辺の2番目の項は、エンコードされた潜在空間分布と事前分布との間のKLダイバージェンス損失です。通常、 $q_\phi(z|x)$ から $z$ をサンプリングするプロセスは微分不可能になります。しかし、VAEは「再パラメータ化トリック」と呼ばれる手法を用いることで、VAEをエンドツーエンドで訓練可能にしています（Kingma & Welling, 2013）。

### 3.3 Base TimeVAE Architecture

ここではまだエンコーディング関数とデコーディング関数については説明していません。上記で説明した損失関数が微分可能である限り、これらの関数には任意のモデルを選択できます。我々の手法は、**全結合層**や**畳み込み層**といった従来の深層学習層と、レベル、多項式トレンド、季節パターンといった時系列固有のコンポーネントをモデル化するためのカスタム層を組み合わせて使用しています。図1は、TimeVAEの基本バージョンのブロック図を示しています。基本バージョンにはカスタム時間的構造は含まれておらず、時系列固有の知識も必要ありません。

エンコーダへの入力信号Xは、$N \times T \times D$ の3次元配列です。ここで、 $N$ はバッチサイズ、 $T$ はタイムステップの数、 $D$ は特徴次元の数です。与えられたデータが可変長のシーケンスである場合、すべてのシーケンスが同じ長さ $T$ になるように、先頭にゼロを埋め込みます。エンコーダは、一連の畳み込み層をReLU活性化関数とともに通過させます。次に、データは平坦化されてから、全結合（高密度）線形層を通過します。もし$m$が選択された潜在次元の数であり、多変量ガウス分布の次元を表す場合、この最後の層は$2m$個のニューロンを持ちます。この出力を使用して多変量ガウス分布をパラメータ化します。潜在空間のサイズ$m$は、モデルの重要なハイパーパラメータです。

次に、再パラメータ化トリックを使用して、多変量ガウス分布からベクトル$z$をサンプリングします。デコーダは、長さ$m$のサンプリングされた潜在ベクトル$z$を受け取ります。それは全結合線形層を通過します。その後、データは3次元配列に再形成され、一連の転置畳み込み層をReLU活性化関数とともに通過します。最後に、データは、最終的な出力形状が元の信号Xと同じになるような次元を持つ時系列分散全結合層を通過します。

![[Pasted image 20251103153851.png]]


### 3.4 Interpretable TimeVAE

