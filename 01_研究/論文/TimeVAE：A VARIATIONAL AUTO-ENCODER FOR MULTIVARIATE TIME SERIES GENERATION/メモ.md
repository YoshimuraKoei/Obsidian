

> [!NOTE] LINE Yahoo 問4
> 長時間の稼働で徐々に増えるメモリ使用量の原因究明と対策の進め方を記述する。
> 一定時間ごとに高い負荷がかかるWebアプリが、稼働から数日後に不安定になる。 
> あなたは、どのようなメトリクスを集めると原因が推測できるか、どのように条件を絞って再現しやすくするかを説明してください。
> 記録の取り方、保たれ続けている参照の見つけ方、使わなくなった情報を回収できるようにする考え方、必要であれば最大量の上限を設定する手段などを、順序立てて説明してください。
> 最後に、完全な解決までの間に停止を避けるための暫定案を短く添える。

![[Pasted image 20251103150142.png]]

![[Pasted image 20251103150229.png]]

1. ユーザー → Webサーバにリクエスト送信
2. Webサーバがアプリケーションロジックを実行  
    → 一時的なオブジェクトを生成（RAM使用）
3. Webサーバ → DBサーバへSQL送信
4. DBサーバがデータをRAM上のバッファから取得（RAM使用）
5. 結果を返してWebサーバがレスポンスを組み立てる
6. 不要になったオブジェクトはGCまたは明示的に解放
7. OSが空いたメモリを再利用

この間ずっと、OSは

> 「このプロセスは200MB使ってる」「空きが10GBある」「キャッシュをどれだけ残そうか」  
> と監視・管理しています。

---
### 構造別にみる「長時間稼働でメモリが増える」主な原因とメトリクス

#### Webサーバ

| カテゴリ             | 内容                           | 典型例                                                                    |
| ---------------- | ---------------------------- | ---------------------------------------------------------------------- |
| メモリリーク           | オブジェクト・配列・セッション・クロージャが解放されない | Python: list/dictにappendしっぱなしNode.js: クロージャ参照保持Java: static変数が大きな参照を保持 |
| キャッシュ肥大化         | キャッシュ上限がなく増え続ける              | Redis/ローカルキャッシュ/Flask cacheなど                                          |
| リクエスト単位の一時データ未解放 | 大きなレスポンス生成時にオブジェクトを保持        | JSONシリアライズ、画像変換、AI推論など                                                 |
| GC設定不備           | ガーベジコレクタのしきい値が不適切            | JVMヒープ領域が肥大、Pythonの循環参照残留                                              |
| 外部接続の未解放         | DB・ファイル・ソケット・スレッドが閉じない       | `connection.close()`しない／`with`未使用                                      |

| 種別      | メトリクス名                     | 観察目的          | 代表ツール                                 |
| ------- | -------------------------- | ------------- | ------------------------------------- |
| プロセスメモリ | RSS / VSS                  | 使用メモリの増加傾向    | `top`, `ps`, `htop`, Prometheus       |
| ヒープ領域   | heap size / old gen / GC回数 | リーク傾向やGC遅延を検出 | GCログ, `jstat`, `tracemalloc`          |
| オブジェクト数 | type別インスタンス数               | どの型が増え続けているか  | `objgraph`, `memory_profiler`         |
| FD数     | open file descriptors      | リソースリークの兆候    | `lsof`, `ulimit`, Prometheus exporter |

#### DBサーバ


#### OS



### 1. 想定される原因

- **アプリ層**：オブジェクトやキャッシュの参照が解放されない、外部接続の未close、GC設定不備。
- **DB層**：コネクションプールやクエリキャッシュの肥大化。
- **OS層**：ページキャッシュやスワップによる実メモリ圧迫。
    

---

### 2. 集めるべきメトリクス

- **プロセス単位の使用量**：RSS / ヒープサイズ / GC回数
    
- **リソース指標**：スレッド数・FD数・DB接続数・キャッシュサイズ
    
- **OS全体**：`free`, `cached`, `swap` の推移  
    → Prometheus, top, jstat, memory_profiler等で時系列収集し、負荷周期と増加傾向の相関を確認。
    

---

### 3. 再現性を高める条件

- 高負荷周期を短縮してシミュレーション（負荷ツール：wrk, locust）
    
- 同一リクエストを反復送信し、特定操作で増加するか確認
    
- コンテナのメモリ上限を小さく設定して早期に兆候を出す
    

---

### 4. 記録と分析の進め方

1. 定期的にメモリスナップショット取得（heapdump, tracemalloc）
2. 初期・中期・異常直前の差分比較で増え続けるオブジェクトを特定
3. GCルートから参照をたどり、保持元を特定

---

### 5. 解放・抑制の設計

- 不要参照を切る（リスト・キャッシュ・セッションの明示的削除）
- 弱参照・TTLキャッシュの導入
- 外部リソースを確実にclose()
- メモリ上限設定（JVMの`-Xmx`、Nodeの`--max-old-space-size`など）

---

### 6. 暫定的対策

完全修正までの間は、

- 定期的な再起動（cron/k8s）
- メモリ閾値監視による自動リスタートで停止を回避する。



長時間稼働でメモリ使用量が増える主な原因は、アプリケーションが不要になったデータを解放できていないことにある。
Webサーバでは、オブジェクトやキャッシュの参照が残るメモリリーク、外部接続の未解放、ガーベジコレクタ設定の不備が多い。
DBサーバでは、コネクションプールやキャッシュの肥大化、OSではページキャッシュやスワップによる実メモリ圧迫が起きやすい。

原因を推測するには、プロセス単位のRSSやヒープ使用量、GC回数、スレッド数、DB接続数、free、cached、swap などのメトリクスを時系列で収集し、負荷ピークと増加傾向の相関を確認する。
PrometheusやGrafanaなどを用いて、どの層が増えているかを可視化するのが有効である。

再現性を高めるには、負荷テストツールで短い周期の高負荷を再現し、同一リクエストを繰り返す。コンテナのメモリ上限を低く設定すれば、問題を早期に顕在化できる。

記録の取り方は、定期的にメモリスナップショットを取得し、初期と異常直前の状態を比較して増え続けるオブジェクトを特定する。GCルートをたどって参照を保持している箇所を見つけ、不要な参照を切る、キャッシュに期限を設ける、弱参照を使うなど、回収可能な設計に改める。外部リソースは明示的にcloseし、ライフサイクルを管理する。

必要に応じて、Node.jsの--max-old-space-sizeなどでメモリ上限を設定し、異常な増加を防ぐ。
完全な修正までの暫定策としては、定期再起動やメモリ閾値監視による自動リスタートを導入し、停止を避けながら安定稼働を維持する。