---
title: 日本語訳 + 翻訳
AI: "false"
published:
created: " 2025-10-30"
description:
tags:
  - permanent-note
  - paper
  - thesis
  - memo
source:
---
github URL: https://github.com/KurochkinAlexey/SOM-VAE

## 論文詳細

##### 📌 CCS CONCEPTS

- **Computing methodologies → Representation learning; Dimensionality reduction; Time series analysis**

##### 📌 KEYWORDS

- Self-Organizing Map (SOM)
- Variational Autoencoder (VAE)
- Discrete Representation Learning
- Time Series
- Interpretability
- Probabilistic Modeling

##### 📌 著者

- **Vincent Fortuin**（ETH Zürich, Department of Computer Science）
- **Matthias Hüser**（ETH Zürich, Department of Computer Science）
- **Francesco Locatello**（ETH Zürich, Department of Computer Science）
- **Heiko Strathmann**（University College London, Gatsby Unit）
- **Gunnar Rätsch**（ETH Zürich, Department of Computer Science）

##### 📌 ジャーナル・出版情報

- **会議**: International Conference on Learning Representations (ICLR 2019)
- **開催**: 2019年5月6–9日, New Orleans, USA
- **出版社**: International Conference on Learning Representations (ICLR) / OpenReview
- **URL**: https://openreview.net/forum?id=HktopHAcKQ
- **ページ数**: 約18 pages

---
## Abstract

高次元の時系列データは多くの分野で一般的に見られる。しかし、人間の認知は高次元空間での作業に最適化されていないため、こうした分野では**解釈可能な低次元表現**が有用である。ところが、既存の多くの時系列データに対する表現学習アルゴリズムは、**解釈が難しい**という問題を抱えている。これは、データ特徴量から表現の重要な性質への対応が直感的でなく、時間的に滑らかでないことが主な原因である。


> [!NOTE] メモ
> 表現学習では、高次元データを低次元の潜在空間に圧縮する。
> **この圧縮により、データの本質的特徴は保持されるが、時間的な変化や要素間の関係が直接的に見えなくなる。**
> 特に時系列データの場合、
> - 各時点の意味 (何が原因で何が結果か)
> - 変化の方向性 (増加・減少、トレンド)
> 
> が潜在ベクトルに埋め込まれてしまい、人間が直感的に解釈しにくくなる。


この問題に対処するために、本研究では**解釈可能な離散次元削減**と**深層生成モデル**の考え方を組み合わせた新しい表現学習フレームワークを提案する。このフレームワークにより、時系列データの**離散的な表現**を学習し、それによって**滑らかで解釈可能な埋め込み表現**を得ることができ、**クラスタリング性能も優れている**ことが示される。

また、離散表現学習における**非微分可能性の問題を克服する新たな方法**を導入し、従来の**自己組織化マップ（Self-Organizing Map; SOM）** を**勾配ベースで最適化できる新しいバージョン**として改良した。この手法は元のアルゴリズムよりも高性能である。

さらに、提案手法に**確率的な解釈**を与えるために、表現空間内に**マルコフモデル**を統合する。このモデルにより、時系列データの**遷移構造**を明らかにし、クラスタリング性能をさらに改善するとともに、**不確実性の自然な表現**や**追加的な説明的知見**を提供できる。


> [!NOTE] やったこと
> - 解釈可能な次元削減 → 表現空間内にマルコフモデルを統合
> - 離散表現学習における非微分可能性を克服 → SOM を勾配ベースで最適化できるように改良


> [!NOTE] マルコフモデル
> マルコフモデルは、時系列データの遷移構造を明らかにするために使われている。
> マルコフモデルの勉強もしなければ


我々は、本手法をクラスタリング性能と解釈可能性の観点から評価し、静的な(Fashion-)MNISTデータ、線形補間した(Fashion-)MNIST画像の時系列、2つのマクロ状態を持つ**ローレンツアトラクタ系**、そして実世界の医療時系列データである**eICUデータセット**に適用した。結果として、提案手法は競合手法と比較して優れた性能を示し、実データ上での下流タスクの実施を容易にすることが確認された。

---
## 1. Introduction

時系列データにおける解釈可能な表現学習は、カオス的な動的システムや医療時系列など、複雑なシステムにおける潜在構造を明らかにするための画期的な問題である。

人間が大量のデータに基づいて意思決定を行わなければならない分野においては、**解釈可能性**は人間の作業を容易にするための基本的要素である。  
特に、金融や医療のように、時間をかけずに意思決定を下さなければならず、外部のカオス的な過程を時間的に観測することに依存しているような場合、**直感的に理解できる解釈**の必要性はいっそう強くなる。

しかし、多くの教師なし学習手法、たとえばクラスタリングなどは、データが独立同分布（i.i.d.）であるという**誤った仮定**を置いており、データがもつ豊かな**時間的構造**や**時間に沿った滑らかな変化**を無視してしまう。  
したがって、時系列の表現がその滑らかさを保持しながら、低次元空間内で**位相的構造（topological structure）** を持つようなクラスタリング手法が求められている。


> [!NOTE] クラスタリングの仮定
> クラスタリングはデータが独立同分布であるという仮定のもとで使える。
> 
> 時系列では、
> - $x_t$​ と $x_{t+1}$ は強く依存している（連続性・滑らかさ）
> - データの分布が時間とともに変化する（非定常性）
> 
> なのにクラスタリングがi.i.d.を仮定すると：
> - 時間的に近いサンプルがバラバラなクラスタに割り当てられる
> - “動きの連続性”が失われる
> - 結果として、**「時間的にあり得ないクラスタ遷移」**が生じる


本研究では、このような性質を備えた手法を提案する。  
私たちは、表現学習において非常に成功してきた実績のある**深層ニューラルネットワーク（Bengioら, 2013）** を採用することにした。  
近年、これらのネットワークは**生成モデリング**と組み合わされるようになり、**敵対的生成ネットワーク（GAN）**（Goodfellowら, 2014）や**変分オートエンコーダ（VAE）**（Kingma & Welling, 2013）の登場をもたらした。


> [!NOTE] GAN、VAE
> - 敵対的生成ネットワーク (GAN)
> - 変分オートエンコーダ (VAE)
> 
> 調べなければ


しかし、これらのモデルが学習する表現はしばしば**不可解（cryptic）** であり、必要とされる解釈可能性を提供しない（Chenら, 2016）。  
この点を改善するための研究は多く行われており、GAN（Chenら, 2016）およびVAE（Higginsら, 2017; Esmaeiliら, 2018）の両方で進展が見られる。  
とはいえ、これらの研究は**連続的表現（continuous representations）** のみに焦点を当てており、**離散的表現（discrete representations）** については依然として十分に探究されていない。


> [!NOTE] 課題
> 解釈可能な表現学習においては連続的表現のみに焦点が当てられていたが、離散的表現については十分に探究されていない。


離散表現空間において時間的滑らかさを定義するためには、その空間に**位相的な近傍関係（topological neighborhood relationship）** を与える必要がある。  
このような構造を持つ表現空間の一例として、**自己組織化マップ（Self-Organizing Map, SOM）**（Kohonen, 1990）によって誘導される空間が挙げられる。  
SOMは、解釈困難な連続空間の状態を、あらかじめ定義された**位相的に解釈可能な低次元空間**（たとえば可視化が容易な2次元グリッドなど）へ写像することを可能にする。


> [!NOTE] 自己組織化マップ (SOM)
> SOMにより、解釈困難な連続空間の状態を、あらかじめ定義された位相的に解釈可能な低次元空間へ写像できる


しかし、SOMは**静的な状態空間**（たとえば患者の静的な状態など; Tirunagariら, 2015）の可視化には有望な結果を示す一方で、**時間の概念を含まない**という問題がある。  

時間的要素を取り込むには、**確率的遷移モデル（probabilistic transition model）**、たとえば**マルコフモデル**を導入することで、  
時系列内の各時点の表現をその前後の時点からの情報で拡張することができる。


> [!NOTE] 課題
> SOMは時間の概念を含まない問題がある → 確率的遷移モデルの一つのマルコフモデルを導入する
> 
> ちなみに確率的遷移モデルでマルコフモデルを選んだ理由は何なのだろう？


したがって、**確率的モデリング**のアプローチを、表現学習および離散的次元削減とともに**エンドツーエンドのモデル**として適用することは有望である。

本研究では、**確率的手法に基づく、位相的に解釈可能な離散表現を学習する新しい深層アーキテクチャ**を提案する。  
さらに、離散表現学習アーキテクチャにおける**非微分可能性の問題**を克服するための新しい手法を導入し、  
古典的な自己組織化マップアルゴリズムを**勾配ベースで改良したバージョン**を開発する。

私たちは、提案モデルの性能を、合成および実世界の時系列データで広範に検証した。  
その対象には、ベンチマークデータセット、カオス的挙動を示す人工動的システム、および実際の医療データが含まれる。


> [!NOTE] 流れ
> やりたいこと
> - 時系列データにおいて、**時間的滑らかさを保ちながら解釈可能な低次元表現**を獲得したい
> 
> 課題
> 1. 解釈可能な表現学習において、離散的表現は十分に探究されていない
> 2. 1 を行うために SOM を使おうとするが、時間的要素を取り込めていない
> 
> 施策
> - SOMに**確率的遷移モデル（Markovモデル）**を導入し、**時間的依存関係**を表現可能にする
> - さらに、**非微分可能なSOMを勾配ベースで最適化できるように改良**
> 
> 効果
> - 時系列データの滑らかさを保持したまま離散的表現を学習可能に
> - 各クラスタ（状態）の関係が**位相的に可視化**でき、**人間が直感的に解釈しやすい表現空間**を実現
---
## 2. Probabilistic SOM-VAE

提案されたモデルは、自己組織化マップ（Kohonen, 1990）、変分オートエンコーダー（Kingma and Welling, 2013）、および確率モデルのアイデアを組み合わせたものです。以下に、モデルのさまざまなコンポーネントとその相互作用を説明します。

### 2.1 Introducing Topological Structure In The Latent Space

![[Pasted image 20251101192808.png]]
図1：本モデルのアーキテクチャの概略図です。データ空間からの時系列データ（緑色）は、ニューラルネットワーク（黒色）によって時間点ごとに潜在空間にエンコードされます。潜在データ多様体は自己組織化マップ（SOM）（赤色）によって近似されます。離散表現を得るために、各潜在データ点（$z_e$）はSOM内で最も近いノード（$z_q$）にマッピングされます。マルコフ遷移モデル（青色）は、現在の離散表現（$z_q^t$）が与えられた場合に次の離散表現（$z_{q}^{t+1}$）を予測するように学習されます。離散表現は、別のニューラルネットワークによって元のデータ空間にデコードし直すことができます。


提案されたモデルの概略は図1に示されています。入力 $x \in R^d$ は、エンコーダーニューラルネットワークによってパラメータ化された $f_\theta(\cdot)$ を用いて $z_e = f_\theta(x)$ を計算することにより、潜在エンコーディング $z_e \in R^m$ (通常 $m < d$) にマッピングされます。その後、エンコーディングは、サンプリング $z_q \sim p(z_q|z_e)$ によって埋め込み辞書 $E = \{e_1, \ldots, e_k | e_i \in R^m\}$ 内の埋め込み $z_q \in R^m$ に割り当てられます。この分布の形式は柔軟で、設計の選択肢となり得ます。モデルが元のSOMアルゴリズム（下記参照）と同様に動作するように、実験では、分布を $z_e$ に最も近い埋め込みに対して確率質量1を持つカテゴリカル分布として選択します。つまり、$p(z_q|z_e) = 1[z_q = \text{arg min}_{e \in E} \|z_e - e\|^2]$ であり、ここで $1[\cdot]$ は指示関数です。入力の再構築 $\hat{x}$ は、デコーダーニューラルネットワークによってパラメータ化された $g_\phi(\cdot)$ を用いて $\hat{x} = g_\phi(z)$ として計算できます。エンコーディングと埋め込みは同じ空間にあるため、$\hat{x}_e = g_\phi(z_e)$ と $\hat{x}_q = g_\phi(z_q)$ という2つの異なる再構築を計算できます。


> [!NOTE] 数式の流れ
> 1. 入力 $\mathbf{x}$ はエンコーダーによって潜在エンコーディングベクトル $z_e$ に変換される
> 2. サンプリング $z_q \sim p(z_q|z_e)$により連続的表現である $z_e$ を埋め込み辞書 $E$ の中のどれか一つの離散状態 $z_q$ に割り当てる
> 	- 割り当て方法：潜在エンコーディング $z_e$ に最も近い辞書内の埋め込み $e$ を探し、その $e$ を $z_q$ として選択する
> 
> **何のために行われているのか？**
> - 離散的な表現の実現
> - SOM の位相的構造の導入


位相的に解釈可能な近傍構造を実現するために、埋め込みは自己組織化マップを形成するように接続されています。自己組織化マップは、$k$ 個のノード $V = \{v_1, \ldots, v_k\}$ で構成され、各ノードはデータ空間 $e_v \in R^d$ 内の埋め込みと、低次元離散空間 $m_v \in M$ (通常 $M \subset N^2$) 内の表現に対応します。データセット $D = \{x_1, \ldots, x_n\}$ での訓練中、各点 $x_i$ に対して、$\tilde{v} = \text{arg min}_{v \in V} \|e_v - x_i\|^2$ に従って勝者ノード $\tilde{v}$ が選択されます。

各ノード $u \in V$ の埋め込みベクトルは、学習率 $\eta$ と表現空間 $M$ で定義されるノード間の近傍関数 $N(m_u, m_{\tilde{v}})$ を用いて $e_u \leftarrow e_u + N(m_u, m_{\tilde{v}}) \eta (x_i - e_u)$ に従って更新されます。$N(m_u, m_{\tilde{v}})$ にはさまざまな設計の選択肢があります。自己組織化マップアルゴリズムの詳細なレビューは付録（セクションA）に譲ります。Tirunagariら（2015）と同様に視覚化を容易にするため、二次元SOMを使用することを選択しました。アーキテクチャをエンドツーエンドで訓練可能にしたいので、上記の標準的なSOM訓練アルゴリズムは使用できません。代わりに、元のSOM更新ルール（下記参照）の重み付けされたバージョンに対応する勾配を持つ損失関数項を考案します。マップ内の位置 $(i, j)$ にある埋め込み $e_{i,j}$ が更新されるたびに、その直近の近傍 $N(e_{i,j})$ 内のすべての埋め込みも更新されるように実装しています。近傍は二次元マップの場合 $N(e_{i,j}) = \{e_{i-1,j}, e_{i+1,j}, e_{i,j-1}, e_{i,j+1}\}$ として定義されます。


> [!NOTE] 学習されるパラメータ
> 1. 埋め込み辞書 (SOVノード) $E = \{ e_1, \cdots, e_k \}$ のベクトル
> 	- 各ノード $v \in V$ に対応する埋め込みベクトル $e_v \in \mathbb{R}^d$ が学習の主要なターゲット
> 	- 具体的には、データセット $D$ からの各入力 $x_i$ に対して勝者ノード $\bar{v}$ が見つけられ、その $e_{\bar{v}}$ は $x_i$ に近づくように更新される
> 	- 各 $e_v$ がデータ空間内の特定のクラスターの中心を表すことになる
> 2. ノード間の位相的 (トポロジカル) 近傍関係
> 	- 勝者ノード $e_{\bar{v}}$ だけでなく、その低次元空間 $M$ 上での近傍ノード $u$ の埋め込み $e_u$ も更新される
> 	- この更新は近傍関数 $N(m_u, m_{\bar{v}})$によって重みづけされる
> 	- これにより、データ空間で類似しているデータ点が、低次元の SOM マップ上でも近くに配置されるようになる → SOM は入力データの位相構造を保存するようになる
> 3. エンコーダー $f_\theta(\cdot)$ とデコーダー $g_\phi(\cdot)$ のパラメータ
> 	- SOM-VAEモデルを E2E で訓練するために、エンコーダとデコーダのパラメータも学習する
> 	- エンコーダ：高次元の入力 $\mathbf{x}$ をより低次元で意味のある潜在エンコーディング $z_e$ にマッピングする
> 	- デコーダ：離散表現 $z_q$ から元のデータ空間の再構築 $\mathbf{\hat{x}}$ を行う


> [!NOTE] ハイパーパラメータ
> 4. SOM のノード数 $k$
> 5. 低次元離散空間 $M$ の構造
> 6. 学習率 $\eta$
> 7. 近傍関数 $N(m_u, m_{\bar{v}})$
> 8. 損失関数の重みハイパーパラメータ
> 9. 潜在空間の次元 $m$
