---
title:
AI:
published:
created: " 2025-11-01"
description:
tags:
  - permanent-note
  - knowledge
  - paper
---

> [!NOTE] TS2Vec を踏襲した論文の発見
> https://qiita.com/anyai_corp/items/124e443781deba726cf0


## 論文詳細

##### 📌 KEYWORDS
- Machine Learning (ML)

##### 📌 著者
- Zhihan Yue (Peking University Microsoft)
- Yujing Wang (Microsoft Peking University)
- Juanyong Duan (Microsoft)
- Tianmeng Yang (Peking University Microsoft)
- Congrui Huang (Microsoft)
- Yunhai Tong (Peking University)
- Bixiong Xu (Microsoft)

##### 📌 周辺情報
- **ジャーナル**: Proceedings of the AAAI Conference on Artificial Intelligence, 36(8), 8980-8987.
- **出版日**: 2022-06-28
- **DOI**: [https://doi.org/10.1609/aaai.v36i8.20881](https://doi.org/10.1609/aaai.v36i8.20881)
- **ページ数**: 8
- **GitHub URL**: https://github.com/yuezhihan/ts2vec

---
## Abstract 

この論文では、任意の意味レベルで時系列の表現を学習するための普遍的なフレームワークであるTS2Vecを提案しています。既存の手法とは異なり、TS2Vecは拡張されたコンテキストビューに対して階層的に対照学習を実行し、各タイムスタンプの堅牢なコンテキスト表現を可能にします。さらに、時系列内の任意のサブシーケンスの表現を取得するために、対応するタイムスタンプの表現に対して単純な集約を適用できます。時系列表現の品質を評価するために、時系列分類タスクで広範な実験を実施しました。その結果、TS2Vecは、125のUCRデータセットと29のUEAデータセットにおいて、教師なし時系列表現の既存のSOTA（最先端）手法に対して大幅な改善を達成しました。学習されたタイムスタンプレベルの表現は、時系列予測および異常検出タスクでも優れた結果を達成しています。学習された表現に基づいて訓練された線形回帰は、以前の時系列予測のSOTAを上回っています。さらに、学習された表現を教師なし異常検出に適用する単純な方法を提示し、これにより文献でSOTAの結果を確立しています。ソースコードは[https://github.com/yuezhihan/ts2vec](https://github.com/yuezhihan/ts2vec)で公開されています。

---
## 1 Introduction

時系列データは、金融市場、需要予測、気候モデリングなど、さまざまな産業で重要な役割を果たしています。時系列の汎用的な表現を学習することは根本的でありながらも困難な問題です。多くの研究（Tonekaboni, Eytan, and Goldenberg 2021; Franceschi, Dieuleveut, and Jaggi 2019; Wu et al. 2018）では、入力時系列全体のセグメントを記述するインスタンスレベルの表現の学習に焦点を当てており、クラスタリングや分類などのタスクで大きな成功を収めています。さらに、最近の研究（Eldele et al. 2021; Franceschi, Dieuleveut, and Jaggi 2019）では、コントラスト損失を用いて時系列の固有の構造を学習しています。しかし、既存の方法には依然として顕著な限界があります。

第一に、インスタンスレベルの表現は、時系列予測や異常検出など、きめ細かい表現が必要なタスクには適さない場合があります。このようなタスクでは、特定のタイムスタンプまたはサブシリーズでターゲットを推論する必要がありますが、時系列全体の粗い表現では十分な性能を達成できません。

第二に、既存の方法でマルチスケールのコンテキスト情報を異なる粒度で区別しているものはほとんどありません。たとえば、TNC（Tonekaboni, Eytan, and Goldenberg 2021）は、一定の長さのセグメントを区別しています。T-Loss（Franceschi, Dieuleveut, and Jaggi 2019）は、元の時系列からランダムなサブシリーズを正のサンプルとして使用しています。しかし、どちらの方法も、時系列のタスクの成功に不可欠なスケール不変情報を捉えるために、異なるスケールで時系列を特徴付けていません。直感的には、マルチスケール特徴は、異なるレベルのセマンティクスを提供し、学習された表現の汎化能力を向上させる可能性があります。

第三に、教師なし時系列表現の既存のほとんどの方法は、CVおよびNLPドメインの経験に触発されており、変換不変性やクロッピング不変性などの強い帰納的バイアスを持っています。しかし、これらの仮定は、時系列のモデリングには常に適用できるとは限りません。たとえば、クロッピングは、画像のデータ拡張戦略として頻繁に使用されます。しかし、時系列の分布やセマンティクスは時間とともに変化する可能性があり、クロッピングされたサブシーケンスは、元の時系列とは異なる分布を持つ可能性があります。